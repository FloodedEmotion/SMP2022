{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990464db-bc35-4eeb-915c-1c0504fee50b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-02 12:26:53.143157: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-02 12:26:54.830300: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-02 12:26:54.830613: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-02 12:26:54.830648: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apex AMP Installed :: False\n",
      "SWA Available :: True\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import os\n",
    "import gc\n",
    "gc.enable()\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn import model_selection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader,\n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    APEX_INSTALLED = True\n",
    "except ImportError:\n",
    "    APEX_INSTALLED = False\n",
    "\n",
    "from madgrad import MADGRAD\n",
    "\n",
    "try:\n",
    "    from torch.optim.swa_utils import (\n",
    "        AveragedModel, update_bn, SWALR\n",
    "    )\n",
    "    SWA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SWA_AVAILABLE = False\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    logging,\n",
    "    MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING,\n",
    ")\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def optimal_num_of_loader_workers():\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
    "    return optimal_value\n",
    "\n",
    "print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n",
    "print(f\"SWA Available :: {SWA_AVAILABLE}\")\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f039d22-b8dd-4662-aae0-d67aa29bf129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('../data/forauto.csv')\n",
    "alltags = all_data[['Title','label']]\n",
    "train = alltags[:-180581]\n",
    "test = alltags[-180581:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e5e630a-6dcd-4780-9ede-ac9b60ddbbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Luis Drayton - Edinburgh shoot #6</td>\n",
       "      <td>11.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arena da Barra - Arena HSBC - Arena do PAN   #...</td>\n",
       "      <td>15.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MARILYN 2015</td>\n",
       "      <td>10.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Knikkertijd - 1959</td>\n",
       "      <td>8.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAMELS01</td>\n",
       "      <td>11.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305608</th>\n",
       "      <td>Mushrooms</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305609</th>\n",
       "      <td>Evie</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305610</th>\n",
       "      <td>BSSLS+R8GT!!</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305611</th>\n",
       "      <td>C to A1</td>\n",
       "      <td>6.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305612</th>\n",
       "      <td>Empire State Building</td>\n",
       "      <td>1.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>305613 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  label\n",
       "0                       Luis Drayton - Edinburgh shoot #6  11.18\n",
       "1       Arena da Barra - Arena HSBC - Arena do PAN   #...  15.15\n",
       "2                                            MARILYN 2015  10.99\n",
       "3                                      Knikkertijd - 1959   8.63\n",
       "4                                                CAMELS01  11.16\n",
       "...                                                   ...    ...\n",
       "305608                                          Mushrooms   4.00\n",
       "305609                                               Evie   1.00\n",
       "305610                                       BSSLS+R8GT!!   2.00\n",
       "305611                                            C to A1   6.89\n",
       "305612                              Empire State Building   1.58\n",
       "\n",
       "[305613 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6224bb5b-27c5-43a0-826b-d43ed769335f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_770637/1693006389.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"kfold\"] = -1\n"
     ]
    }
   ],
   "source": [
    "# train = pd.read_csv('../input/commonlitreadabilityprize/train.csv', low_memory=False)\n",
    "def create_folds(data, num_splits):\n",
    "    data[\"kfold\"] = -1\n",
    "    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=2021)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data)):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    return data\n",
    "train = create_folds(train, num_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd24902-6428-4e03-9a06-cee5c5b6124d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # model\n",
    "    num_labels = 1\n",
    "    model_type = 'roberta'\n",
    "    model_name_or_path = 'roberta-base'\n",
    "    config_name = 'roberta-base'\n",
    "    fp16 = True if APEX_INSTALLED else False\n",
    "    fp16_opt_level = \"O1\"\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer_name = 'roberta-base'\n",
    "    max_seq_length = 250\n",
    "\n",
    "    # train\n",
    "    epochs = 6\n",
    "    train_batch_size = 64\n",
    "    eval_batch_size = 32\n",
    "\n",
    "    # optimizer\n",
    "    optimizer_type = 'MADGRAD'\n",
    "    learning_rate = 2e-5\n",
    "    weight_decay = 1e-5\n",
    "    epsilon = 1e-6\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # stochastic weight averaging\n",
    "    swa = True\n",
    "    swa_start = 4\n",
    "    swa_learning_rate = 1e-4\n",
    "    anneal_epochs=2\n",
    "    anneal_strategy='cos'\n",
    "\n",
    "    # scheduler\n",
    "    decay_name = 'cosine-warmup'\n",
    "    warmup_ratio = 0.03\n",
    "\n",
    "    # logging\n",
    "    logging_steps = 10\n",
    "\n",
    "    # evaluate\n",
    "    output_dir = 'output'\n",
    "    seed = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1e7a6ba-06b2-41ae-821b-d948170426ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.max = 0\n",
    "        self.min = 1e5\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        if val > self.max:\n",
    "            self.max = val\n",
    "        if val < self.min:\n",
    "            self.min = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12b6aeee-4dd0-4ee0-98eb-66582fb4661a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
    "        super(DatasetRetriever, self).__init__()\n",
    "        self.data = data\n",
    "        self.is_test = is_test\n",
    "        self.excerpts = self.data.Title.values.tolist()\n",
    "        if not self.is_test:\n",
    "            self.targets = self.data.label.values.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        excerpt = self.excerpts[item]\n",
    "        features = self.convert_examples_to_features(\n",
    "            excerpt, self.tokenizer, \n",
    "            self.max_len\n",
    "        )\n",
    "        features = {key : torch.tensor(value, dtype=torch.long) for key, value in features.items()}\n",
    "        if not self.is_test:\n",
    "            label = self.targets[item]\n",
    "            features['labels'] = torch.tensor(label, dtype=torch.double)\n",
    "        return features\n",
    "    \n",
    "    def convert_examples_to_features(self, example, tokenizer, max_len):\n",
    "        features = tokenizer.encode_plus(\n",
    "            example.replace('\\n', ''), \n",
    "            max_length=max_len, \n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f9850b1-59df-481d-82c0-db637e6a31be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name, \n",
    "        config\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        self.roberta = AutoModel.from_pretrained(\n",
    "            model_name, \n",
    "            config=config\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=0.0)\n",
    "        self.high_dropout = nn.Dropout(p=0.0)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-5)\n",
    "        self._init_weights(self.layer_norm)\n",
    "        self.regressor = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self._init_weights(self.regressor)\n",
    "        \n",
    "        weights_init = torch.zeros(config.num_hidden_layers + 1).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    " \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    " \n",
    "    def forward(\n",
    "        self, input_ids=None,\n",
    "        attention_mask=None, labels=None\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        all_hidden_states = outputs[2]\n",
    "        \n",
    "        # weighted layer pooling\n",
    "        cls_embeddings = torch.stack(\n",
    "            [self.dropout(layer[:, 0]) for layer in all_hidden_states], \n",
    "            dim=2\n",
    "        )\n",
    "        cls_output = (\n",
    "            torch.softmax(self.layer_weights, dim=0) * cls_embeddings\n",
    "        ).sum(-1)\n",
    "        cls_output = self.layer_norm(cls_output)\n",
    "        \n",
    "        # multi-sample dropout\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [self.regressor(self.high_dropout(cls_output)) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    " \n",
    "        # calculate loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # regression task\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            logits = logits.view(-1).to(labels.dtype)\n",
    "            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n",
    "        output = (logits,) + outputs[2:]\n",
    "        \n",
    "        del all_hidden_states, cls_embeddings\n",
    "        del cls_output, logits\n",
    "        gc.collect();\n",
    "        \n",
    "        return ((loss,) + output) if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df42ab4b-86c9-47b7-9e32-fccb0499b87a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimizer_grouped_parameters(args, model):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
    "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
    "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b894d0a7-fcc4-4cfd-9002-8ce3f5f1d33e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model(args, output_attentions=False):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "    config.update({'num_labels':args.num_labels})\n",
    "    config.update({\"output_hidden_states\":True})\n",
    "    if output_attentions:\n",
    "        config.update({\"output_attentions\":True})\n",
    "    model = Model(args.model_name_or_path, config=config)\n",
    "    return model, config, tokenizer\n",
    "\n",
    "def make_optimizer(args, model):\n",
    "    optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n",
    "    if args.optimizer_type == \"AdamW\":\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            eps=args.epsilon,\n",
    "            correct_bias=not args.use_bertadam\n",
    "        )\n",
    "    else:\n",
    "        optimizer = MADGRAD(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            eps=args.epsilon,\n",
    "            weight_decay=args.weight_decay\n",
    "        )\n",
    "    return optimizer\n",
    "\n",
    "def make_scheduler(\n",
    "    args, optimizer, \n",
    "    num_warmup_steps, \n",
    "    num_training_steps\n",
    "):\n",
    "    if args.decay_name == \"cosine-warmup\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    else:\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    return scheduler    \n",
    "\n",
    "def make_loader(\n",
    "    args, data, \n",
    "    tokenizer, fold\n",
    "):\n",
    "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
    "\n",
    "    train_dataset = DatasetRetriever(train_set, tokenizer, args.max_seq_length)\n",
    "    valid_dataset = DatasetRetriever(valid_set, tokenizer, args.max_seq_length)\n",
    "    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n",
    "    \n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    valid_sampler = SequentialSampler(valid_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=optimal_num_of_loader_workers(),\n",
    "        pin_memory=True,\n",
    "        drop_last=False \n",
    "    )\n",
    "\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=args.eval_batch_size, \n",
    "        sampler=valid_sampler,\n",
    "        num_workers=optimal_num_of_loader_workers(),\n",
    "        pin_memory=True, \n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "146cf3eb-66ff-455f-bc95-3f6f913246ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, model, tokenizer, \n",
    "        optimizer, scheduler, \n",
    "        swa_model=None, swa_scheduler=None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.swa_model = swa_model\n",
    "        self.swa_scheduler = swa_scheduler\n",
    "\n",
    "    def train(\n",
    "        self, args, \n",
    "        train_dataloader, \n",
    "        epoch, result_dict\n",
    "    ):\n",
    "        count = 0\n",
    "        losses = AverageMeter()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        self.model.train()\n",
    "        \n",
    "        fix_all_seeds(args.seed)\n",
    "        for batch_idx, batch_data in enumerate(train_dataloader):\n",
    "            input_ids, attention_mask, labels = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], batch_data['labels']\n",
    "            input_ids, attention_mask, labels = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n",
    "\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss, logits = outputs[:2]\n",
    "            \n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            count += labels.size(0)\n",
    "            losses.update(loss.item(), input_ids.size(0))\n",
    "\n",
    "            if args.fp16:\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            if not args.swa:\n",
    "                self.scheduler.step()\n",
    "            else:\n",
    "                if (epoch+1) < args.swa_start:\n",
    "                    self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n",
    "                _s = str(len(str(len(train_dataloader.sampler))))\n",
    "                ret = [\n",
    "                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n",
    "                    'Train Loss: {: >4.5f}'.format(losses.avg),\n",
    "                ]\n",
    "                print(', '.join(ret))\n",
    "            \n",
    "        if args.swa and (epoch+1) >= args.swa_start:\n",
    "            self.swa_model.update_parameters(self.model)\n",
    "            self.swa_scheduler.step()\n",
    "\n",
    "        result_dict['train_loss'].append(losses.avg)\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4f9df6a-0d9f-431d-8ad8-4319e99b9637",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model, swa_model):\n",
    "        self.model = model\n",
    "        self.swa_model = swa_model\n",
    "    \n",
    "    def save(self, result, output_dir):\n",
    "        with open(f'{output_dir}/result_dict.json', 'w') as f:\n",
    "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
    "\n",
    "    def evaluate(self, valid_dataloader, epoch, result_dict):\n",
    "        losses = AverageMeter()\n",
    "        for batch_idx, batch_data in enumerate(valid_dataloader):\n",
    "            self.model = self.model.eval()\n",
    "            input_ids, attention_mask, labels = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], batch_data['labels']\n",
    "            input_ids, attention_mask, labels = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n",
    "            with torch.no_grad():            \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss, logits = outputs[:2]\n",
    "                losses.update(loss.item(), input_ids.size(0))\n",
    "        print('----Validation Results Summary----')\n",
    "        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n",
    "        result_dict['val_loss'].append(losses.avg)        \n",
    "        return result_dict\n",
    "    \n",
    "    def swa_evaluate(self, valid_dataloader, epoch, result_dict):\n",
    "        losses = AverageMeter()\n",
    "        for batch_idx, batch_data in enumerate(valid_dataloader):\n",
    "            self.swa_model = self.swa_model.eval()\n",
    "            input_ids, attention_mask, labels = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], batch_data['labels']\n",
    "            input_ids, attention_mask, labels = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n",
    "            with torch.no_grad():            \n",
    "                outputs = self.swa_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss, logits = outputs[:2]\n",
    "                losses.update(loss.item(), input_ids.size(0))\n",
    "        print('----SWA Validation Results Summary----')\n",
    "        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n",
    "        result_dict['swa_loss'].append(losses.avg)        \n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de0604d3-53a7-44ca-8f93-f3aa8bebffa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_training(args, data, fold):\n",
    "    fix_all_seeds(args.seed)\n",
    "    \n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    \n",
    "    # model\n",
    "    model, model_config, tokenizer = make_model(args)\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "    \n",
    "    # data loaders for training and evaluation\n",
    "    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = make_optimizer(args, model)\n",
    "\n",
    "    # scheduler\n",
    "    num_training_steps = len(train_dataloader) * args.epochs\n",
    "    if args.warmup_ratio > 0:\n",
    "        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n",
    "    else:\n",
    "        num_warmup_steps = 0\n",
    "    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n",
    "    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "    # stochastic weight averaging\n",
    "    swa_model = AveragedModel(model)\n",
    "    swa_scheduler = SWALR(\n",
    "        optimizer, swa_lr=args.swa_learning_rate, \n",
    "        anneal_epochs=args.anneal_epochs, \n",
    "        anneal_strategy=args.anneal_strategy\n",
    "    )\n",
    "\n",
    "    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}, SWA Start Step: {args.swa_start}\")\n",
    "\n",
    "    # mixed precision training with NVIDIA Apex\n",
    "    if args.fp16:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "    \n",
    "    result_dict = {\n",
    "        'epoch':[], \n",
    "        'train_loss': [], \n",
    "        'val_loss' : [], \n",
    "        'swa_loss': [],\n",
    "        'best_val_loss': np.inf\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        model, model_config, tokenizer, optimizer, scheduler, \n",
    "        train_dataloader, valid_dataloader, result_dict,\n",
    "        swa_model, swa_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eef5ed5b-9eb1-4560-a946-87e40921cb38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(data, fold):\n",
    "    args = Config()\n",
    "    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n",
    "        valid_dataloader, result_dict, swa_model, swa_scheduler = init_training(args, data, fold)\n",
    "    \n",
    "    trainer = Trainer(model, tokenizer, optimizer, scheduler, swa_model, swa_scheduler)\n",
    "    evaluator = Evaluator(model, swa_model)\n",
    "\n",
    "    train_time_list = []\n",
    "    valid_time_list = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        result_dict['epoch'].append(epoch)\n",
    "\n",
    "        # Train\n",
    "        torch.cuda.synchronize()\n",
    "        tic1 = time.time()\n",
    "        result_dict = trainer.train(\n",
    "            args, train_dataloader, \n",
    "            epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "        tic2 = time.time() \n",
    "        train_time_list.append(tic2 - tic1)\n",
    "        \n",
    "        # Evaluate\n",
    "        torch.cuda.synchronize()\n",
    "        tic3 = time.time()\n",
    "        result_dict = evaluator.evaluate(\n",
    "            valid_dataloader, epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "        tic4 = time.time() \n",
    "        valid_time_list.append(tic4 - tic3)\n",
    "            \n",
    "        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n",
    "        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
    "            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
    "            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n",
    "            \n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
    "            model_config.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            print(f\"Saving model checkpoint to {output_dir}.\")\n",
    "    \n",
    "            #torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "            #torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "            #print(f\"Saving optimizer and scheduler states to {output_dir}.\")\n",
    "        print()\n",
    "        \n",
    "    if args.swa:\n",
    "        update_bn(train_dataloader, swa_model, device=torch.device('cuda'))\n",
    "    result_dict = evaluator.swa_evaluate(valid_dataloader, epoch, result_dict)\n",
    "    \n",
    "    evaluator.save(result_dict, output_dir)\n",
    "    torch.save(swa_model.state_dict(), f\"{output_dir}/swa_pytorch_model.bin\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n",
    "    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    del trainer, evaluator\n",
    "    del model, model_config, tokenizer\n",
    "    del optimizer, scheduler\n",
    "    del train_dataloader, valid_dataloader, result_dict\n",
    "    del swa_model, swa_scheduler\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1badfc0-a518-4f23-8d20-43a6cdb21a39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 0\n",
      "--------------------------------------------------\n",
      "Model pushed to 2 GPU(s), type NVIDIA GeForce RTX 3090.\n",
      "Num examples Train= 244490, Num examples Valid=61123\n",
      "Total Training Steps: 22926, Total Warmup Steps: 687\n",
      "Total Training Steps: 22926, Total Warmup Steps: 687, SWA Start Step: 4\n",
      "Epoch: 00 [    64/244490 (  0%)], Train Loss: 5.50869\n",
      "Epoch: 00 [   704/244490 (  0%)], Train Loss: 5.75456\n",
      "Epoch: 00 [  1344/244490 (  1%)], Train Loss: 5.67657\n",
      "Epoch: 00 [  1984/244490 (  1%)], Train Loss: 5.51343\n",
      "Epoch: 00 [  2624/244490 (  1%)], Train Loss: 5.17195\n",
      "Epoch: 00 [  3264/244490 (  1%)], Train Loss: 4.68487\n",
      "Epoch: 00 [  3904/244490 (  2%)], Train Loss: 4.31099\n",
      "Epoch: 00 [  4544/244490 (  2%)], Train Loss: 4.04528\n",
      "Epoch: 00 [  5184/244490 (  2%)], Train Loss: 3.84860\n",
      "Epoch: 00 [  5824/244490 (  2%)], Train Loss: 3.70538\n",
      "Epoch: 00 [  6464/244490 (  3%)], Train Loss: 3.57666\n",
      "Epoch: 00 [  7104/244490 (  3%)], Train Loss: 3.47476\n",
      "Epoch: 00 [  7744/244490 (  3%)], Train Loss: 3.39036\n",
      "Epoch: 00 [  8384/244490 (  3%)], Train Loss: 3.31042\n",
      "Epoch: 00 [  9024/244490 (  4%)], Train Loss: 3.25241\n",
      "Epoch: 00 [  9664/244490 (  4%)], Train Loss: 3.19922\n",
      "Epoch: 00 [ 10304/244490 (  4%)], Train Loss: 3.14544\n",
      "Epoch: 00 [ 10944/244490 (  4%)], Train Loss: 3.09786\n",
      "Epoch: 00 [ 11584/244490 (  5%)], Train Loss: 3.05922\n",
      "Epoch: 00 [ 12224/244490 (  5%)], Train Loss: 3.02254\n",
      "Epoch: 00 [ 12864/244490 (  5%)], Train Loss: 2.99505\n",
      "Epoch: 00 [ 13504/244490 (  6%)], Train Loss: 2.96826\n",
      "Epoch: 00 [ 14144/244490 (  6%)], Train Loss: 2.94180\n",
      "Epoch: 00 [ 14784/244490 (  6%)], Train Loss: 2.91382\n",
      "Epoch: 00 [ 15424/244490 (  6%)], Train Loss: 2.89057\n",
      "Epoch: 00 [ 16064/244490 (  7%)], Train Loss: 2.86881\n",
      "Epoch: 00 [ 16704/244490 (  7%)], Train Loss: 2.84937\n",
      "Epoch: 00 [ 17344/244490 (  7%)], Train Loss: 2.82899\n",
      "Epoch: 00 [ 17984/244490 (  7%)], Train Loss: 2.80918\n",
      "Epoch: 00 [ 18624/244490 (  8%)], Train Loss: 2.79363\n",
      "Epoch: 00 [ 19264/244490 (  8%)], Train Loss: 2.78353\n",
      "Epoch: 00 [ 19904/244490 (  8%)], Train Loss: 2.77127\n",
      "Epoch: 00 [ 20544/244490 (  8%)], Train Loss: 2.76047\n",
      "Epoch: 00 [ 21184/244490 (  9%)], Train Loss: 2.74880\n",
      "Epoch: 00 [ 21824/244490 (  9%)], Train Loss: 2.73729\n",
      "Epoch: 00 [ 22464/244490 (  9%)], Train Loss: 2.72681\n",
      "Epoch: 00 [ 23104/244490 (  9%)], Train Loss: 2.71464\n",
      "Epoch: 00 [ 23744/244490 ( 10%)], Train Loss: 2.70497\n",
      "Epoch: 00 [ 24384/244490 ( 10%)], Train Loss: 2.69652\n",
      "Epoch: 00 [ 25024/244490 ( 10%)], Train Loss: 2.68723\n",
      "Epoch: 00 [ 25664/244490 ( 10%)], Train Loss: 2.67723\n",
      "Epoch: 00 [ 26304/244490 ( 11%)], Train Loss: 2.66834\n",
      "Epoch: 00 [ 26944/244490 ( 11%)], Train Loss: 2.65660\n",
      "Epoch: 00 [ 27584/244490 ( 11%)], Train Loss: 2.64685\n",
      "Epoch: 00 [ 28224/244490 ( 12%)], Train Loss: 2.63672\n",
      "Epoch: 00 [ 28864/244490 ( 12%)], Train Loss: 2.62687\n",
      "Epoch: 00 [ 29504/244490 ( 12%)], Train Loss: 2.61932\n",
      "Epoch: 00 [ 30144/244490 ( 12%)], Train Loss: 2.61279\n",
      "Epoch: 00 [ 30784/244490 ( 13%)], Train Loss: 2.60575\n",
      "Epoch: 00 [ 31424/244490 ( 13%)], Train Loss: 2.59687\n",
      "Epoch: 00 [ 32064/244490 ( 13%)], Train Loss: 2.58911\n",
      "Epoch: 00 [ 32704/244490 ( 13%)], Train Loss: 2.58098\n",
      "Epoch: 00 [ 33344/244490 ( 14%)], Train Loss: 2.57548\n",
      "Epoch: 00 [ 33984/244490 ( 14%)], Train Loss: 2.56887\n",
      "Epoch: 00 [ 34624/244490 ( 14%)], Train Loss: 2.56409\n",
      "Epoch: 00 [ 35264/244490 ( 14%)], Train Loss: 2.55868\n",
      "Epoch: 00 [ 35904/244490 ( 15%)], Train Loss: 2.55146\n",
      "Epoch: 00 [ 36544/244490 ( 15%)], Train Loss: 2.54565\n",
      "Epoch: 00 [ 37184/244490 ( 15%)], Train Loss: 2.53907\n",
      "Epoch: 00 [ 37824/244490 ( 15%)], Train Loss: 2.53330\n",
      "Epoch: 00 [ 38464/244490 ( 16%)], Train Loss: 2.52808\n",
      "Epoch: 00 [ 39104/244490 ( 16%)], Train Loss: 2.52399\n",
      "Epoch: 00 [ 39744/244490 ( 16%)], Train Loss: 2.51880\n",
      "Epoch: 00 [ 40384/244490 ( 17%)], Train Loss: 2.51470\n",
      "Epoch: 00 [ 41024/244490 ( 17%)], Train Loss: 2.51071\n",
      "Epoch: 00 [ 41664/244490 ( 17%)], Train Loss: 2.50645\n",
      "Epoch: 00 [ 42304/244490 ( 17%)], Train Loss: 2.50435\n",
      "Epoch: 00 [ 42944/244490 ( 18%)], Train Loss: 2.49965\n",
      "Epoch: 00 [ 43584/244490 ( 18%)], Train Loss: 2.49528\n",
      "Epoch: 00 [ 44224/244490 ( 18%)], Train Loss: 2.49026\n",
      "Epoch: 00 [ 44864/244490 ( 18%)], Train Loss: 2.48630\n",
      "Epoch: 00 [ 45504/244490 ( 19%)], Train Loss: 2.48408\n",
      "Epoch: 00 [ 46144/244490 ( 19%)], Train Loss: 2.47957\n",
      "Epoch: 00 [ 46784/244490 ( 19%)], Train Loss: 2.47692\n",
      "Epoch: 00 [ 47424/244490 ( 19%)], Train Loss: 2.47534\n",
      "Epoch: 00 [ 48064/244490 ( 20%)], Train Loss: 2.47211\n",
      "Epoch: 00 [ 48704/244490 ( 20%)], Train Loss: 2.47036\n",
      "Epoch: 00 [ 49344/244490 ( 20%)], Train Loss: 2.46686\n",
      "Epoch: 00 [ 49984/244490 ( 20%)], Train Loss: 2.46441\n",
      "Epoch: 00 [ 50624/244490 ( 21%)], Train Loss: 2.46020\n",
      "Epoch: 00 [ 51264/244490 ( 21%)], Train Loss: 2.45657\n",
      "Epoch: 00 [ 51904/244490 ( 21%)], Train Loss: 2.45301\n",
      "Epoch: 00 [ 52544/244490 ( 21%)], Train Loss: 2.45000\n",
      "Epoch: 00 [ 53184/244490 ( 22%)], Train Loss: 2.44742\n",
      "Epoch: 00 [ 53824/244490 ( 22%)], Train Loss: 2.44402\n",
      "Epoch: 00 [ 54464/244490 ( 22%)], Train Loss: 2.43953\n",
      "Epoch: 00 [ 55104/244490 ( 23%)], Train Loss: 2.43768\n",
      "Epoch: 00 [ 55744/244490 ( 23%)], Train Loss: 2.43645\n",
      "Epoch: 00 [ 56384/244490 ( 23%)], Train Loss: 2.43254\n",
      "Epoch: 00 [ 57024/244490 ( 23%)], Train Loss: 2.42979\n",
      "Epoch: 00 [ 57664/244490 ( 24%)], Train Loss: 2.42673\n",
      "Epoch: 00 [ 58304/244490 ( 24%)], Train Loss: 2.42391\n",
      "Epoch: 00 [ 58944/244490 ( 24%)], Train Loss: 2.42033\n",
      "Epoch: 00 [ 59584/244490 ( 24%)], Train Loss: 2.41846\n",
      "Epoch: 00 [ 60224/244490 ( 25%)], Train Loss: 2.41472\n",
      "Epoch: 00 [ 60864/244490 ( 25%)], Train Loss: 2.41344\n",
      "Epoch: 00 [ 61504/244490 ( 25%)], Train Loss: 2.41113\n",
      "Epoch: 00 [ 62144/244490 ( 25%)], Train Loss: 2.40985\n",
      "Epoch: 00 [ 62784/244490 ( 26%)], Train Loss: 2.40605\n",
      "Epoch: 00 [ 63424/244490 ( 26%)], Train Loss: 2.40381\n",
      "Epoch: 00 [ 64064/244490 ( 26%)], Train Loss: 2.40026\n",
      "Epoch: 00 [ 64704/244490 ( 26%)], Train Loss: 2.39898\n",
      "Epoch: 00 [ 65344/244490 ( 27%)], Train Loss: 2.39748\n",
      "Epoch: 00 [ 65984/244490 ( 27%)], Train Loss: 2.39662\n",
      "Epoch: 00 [ 66624/244490 ( 27%)], Train Loss: 2.39566\n",
      "Epoch: 00 [ 67264/244490 ( 28%)], Train Loss: 2.39335\n",
      "Epoch: 00 [ 67904/244490 ( 28%)], Train Loss: 2.39124\n",
      "Epoch: 00 [ 68544/244490 ( 28%)], Train Loss: 2.39013\n",
      "Epoch: 00 [ 69184/244490 ( 28%)], Train Loss: 2.38697\n",
      "Epoch: 00 [ 69824/244490 ( 29%)], Train Loss: 2.38429\n",
      "Epoch: 00 [ 70464/244490 ( 29%)], Train Loss: 2.38235\n",
      "Epoch: 00 [ 71104/244490 ( 29%)], Train Loss: 2.38032\n",
      "Epoch: 00 [ 71744/244490 ( 29%)], Train Loss: 2.37774\n",
      "Epoch: 00 [ 72384/244490 ( 30%)], Train Loss: 2.37501\n",
      "Epoch: 00 [ 73024/244490 ( 30%)], Train Loss: 2.37200\n",
      "Epoch: 00 [ 73664/244490 ( 30%)], Train Loss: 2.36959\n",
      "Epoch: 00 [ 74304/244490 ( 30%)], Train Loss: 2.36809\n",
      "Epoch: 00 [ 74944/244490 ( 31%)], Train Loss: 2.36545\n",
      "Epoch: 00 [ 75584/244490 ( 31%)], Train Loss: 2.36239\n",
      "Epoch: 00 [ 76224/244490 ( 31%)], Train Loss: 2.36010\n",
      "Epoch: 00 [ 76864/244490 ( 31%)], Train Loss: 2.35695\n",
      "Epoch: 00 [ 77504/244490 ( 32%)], Train Loss: 2.35533\n",
      "Epoch: 00 [ 78144/244490 ( 32%)], Train Loss: 2.35387\n",
      "Epoch: 00 [ 78784/244490 ( 32%)], Train Loss: 2.35372\n",
      "Epoch: 00 [ 79424/244490 ( 32%)], Train Loss: 2.35381\n",
      "Epoch: 00 [ 80064/244490 ( 33%)], Train Loss: 2.35080\n",
      "Epoch: 00 [ 80704/244490 ( 33%)], Train Loss: 2.34894\n",
      "Epoch: 00 [ 81344/244490 ( 33%)], Train Loss: 2.34756\n",
      "Epoch: 00 [ 81984/244490 ( 34%)], Train Loss: 2.34520\n",
      "Epoch: 00 [ 82624/244490 ( 34%)], Train Loss: 2.34329\n",
      "Epoch: 00 [ 83264/244490 ( 34%)], Train Loss: 2.34179\n",
      "Epoch: 00 [ 83904/244490 ( 34%)], Train Loss: 2.33923\n",
      "Epoch: 00 [ 84544/244490 ( 35%)], Train Loss: 2.33668\n",
      "Epoch: 00 [ 85184/244490 ( 35%)], Train Loss: 2.33445\n",
      "Epoch: 00 [ 85824/244490 ( 35%)], Train Loss: 2.33150\n",
      "Epoch: 00 [ 86464/244490 ( 35%)], Train Loss: 2.32959\n",
      "Epoch: 00 [ 87104/244490 ( 36%)], Train Loss: 2.32744\n",
      "Epoch: 00 [ 87744/244490 ( 36%)], Train Loss: 2.32566\n",
      "Epoch: 00 [ 88384/244490 ( 36%)], Train Loss: 2.32482\n",
      "Epoch: 00 [ 89024/244490 ( 36%)], Train Loss: 2.32379\n",
      "Epoch: 00 [ 89664/244490 ( 37%)], Train Loss: 2.32263\n",
      "Epoch: 00 [ 90304/244490 ( 37%)], Train Loss: 2.32136\n",
      "Epoch: 00 [ 90944/244490 ( 37%)], Train Loss: 2.31953\n",
      "Epoch: 00 [ 91584/244490 ( 37%)], Train Loss: 2.31777\n",
      "Epoch: 00 [ 92224/244490 ( 38%)], Train Loss: 2.31706\n",
      "Epoch: 00 [ 92864/244490 ( 38%)], Train Loss: 2.31546\n",
      "Epoch: 00 [ 93504/244490 ( 38%)], Train Loss: 2.31383\n",
      "Epoch: 00 [ 94144/244490 ( 39%)], Train Loss: 2.31215\n",
      "Epoch: 00 [ 94784/244490 ( 39%)], Train Loss: 2.31053\n",
      "Epoch: 00 [ 95424/244490 ( 39%)], Train Loss: 2.30848\n",
      "Epoch: 00 [ 96064/244490 ( 39%)], Train Loss: 2.30709\n",
      "Epoch: 00 [ 96704/244490 ( 40%)], Train Loss: 2.30514\n",
      "Epoch: 00 [ 97344/244490 ( 40%)], Train Loss: 2.30414\n",
      "Epoch: 00 [ 97984/244490 ( 40%)], Train Loss: 2.30270\n",
      "Epoch: 00 [ 98624/244490 ( 40%)], Train Loss: 2.30102\n",
      "Epoch: 00 [ 99264/244490 ( 41%)], Train Loss: 2.29854\n",
      "Epoch: 00 [ 99904/244490 ( 41%)], Train Loss: 2.29696\n",
      "Epoch: 00 [100544/244490 ( 41%)], Train Loss: 2.29561\n",
      "Epoch: 00 [101184/244490 ( 41%)], Train Loss: 2.29519\n",
      "Epoch: 00 [101824/244490 ( 42%)], Train Loss: 2.29415\n",
      "Epoch: 00 [102464/244490 ( 42%)], Train Loss: 2.29326\n",
      "Epoch: 00 [103104/244490 ( 42%)], Train Loss: 2.29155\n",
      "Epoch: 00 [103744/244490 ( 42%)], Train Loss: 2.29025\n",
      "Epoch: 00 [104384/244490 ( 43%)], Train Loss: 2.28946\n",
      "Epoch: 00 [105024/244490 ( 43%)], Train Loss: 2.28822\n",
      "Epoch: 00 [105664/244490 ( 43%)], Train Loss: 2.28762\n",
      "Epoch: 00 [106304/244490 ( 43%)], Train Loss: 2.28650\n",
      "Epoch: 00 [106944/244490 ( 44%)], Train Loss: 2.28515\n",
      "Epoch: 00 [107584/244490 ( 44%)], Train Loss: 2.28390\n",
      "Epoch: 00 [108224/244490 ( 44%)], Train Loss: 2.28258\n",
      "Epoch: 00 [108864/244490 ( 45%)], Train Loss: 2.28183\n",
      "Epoch: 00 [109504/244490 ( 45%)], Train Loss: 2.28064\n",
      "Epoch: 00 [110144/244490 ( 45%)], Train Loss: 2.27868\n",
      "Epoch: 00 [110784/244490 ( 45%)], Train Loss: 2.27662\n",
      "Epoch: 00 [111424/244490 ( 46%)], Train Loss: 2.27507\n",
      "Epoch: 00 [112064/244490 ( 46%)], Train Loss: 2.27375\n",
      "Epoch: 00 [112704/244490 ( 46%)], Train Loss: 2.27189\n",
      "Epoch: 00 [113344/244490 ( 46%)], Train Loss: 2.27063\n",
      "Epoch: 00 [113984/244490 ( 47%)], Train Loss: 2.26944\n",
      "Epoch: 00 [114624/244490 ( 47%)], Train Loss: 2.26899\n",
      "Epoch: 00 [115264/244490 ( 47%)], Train Loss: 2.26716\n",
      "Epoch: 00 [115904/244490 ( 47%)], Train Loss: 2.26587\n",
      "Epoch: 00 [116544/244490 ( 48%)], Train Loss: 2.26484\n",
      "Epoch: 00 [117184/244490 ( 48%)], Train Loss: 2.26316\n",
      "Epoch: 00 [117824/244490 ( 48%)], Train Loss: 2.26222\n",
      "Epoch: 00 [118464/244490 ( 48%)], Train Loss: 2.26064\n",
      "Epoch: 00 [119104/244490 ( 49%)], Train Loss: 2.26002\n",
      "Epoch: 00 [119744/244490 ( 49%)], Train Loss: 2.25869\n",
      "Epoch: 00 [120384/244490 ( 49%)], Train Loss: 2.25733\n",
      "Epoch: 00 [121024/244490 ( 50%)], Train Loss: 2.25634\n",
      "Epoch: 00 [121664/244490 ( 50%)], Train Loss: 2.25544\n",
      "Epoch: 00 [122304/244490 ( 50%)], Train Loss: 2.25462\n",
      "Epoch: 00 [122944/244490 ( 50%)], Train Loss: 2.25349\n",
      "Epoch: 00 [123584/244490 ( 51%)], Train Loss: 2.25213\n",
      "Epoch: 00 [124224/244490 ( 51%)], Train Loss: 2.25140\n",
      "Epoch: 00 [124864/244490 ( 51%)], Train Loss: 2.25128\n",
      "Epoch: 00 [125504/244490 ( 51%)], Train Loss: 2.25060\n",
      "Epoch: 00 [126144/244490 ( 52%)], Train Loss: 2.24921\n",
      "Epoch: 00 [126784/244490 ( 52%)], Train Loss: 2.24821\n",
      "Epoch: 00 [127424/244490 ( 52%)], Train Loss: 2.24745\n",
      "Epoch: 00 [128064/244490 ( 52%)], Train Loss: 2.24779\n",
      "Epoch: 00 [128704/244490 ( 53%)], Train Loss: 2.24700\n",
      "Epoch: 00 [129344/244490 ( 53%)], Train Loss: 2.24694\n",
      "Epoch: 00 [129984/244490 ( 53%)], Train Loss: 2.24567\n",
      "Epoch: 00 [130624/244490 ( 53%)], Train Loss: 2.24480\n",
      "Epoch: 00 [131264/244490 ( 54%)], Train Loss: 2.24442\n",
      "Epoch: 00 [131904/244490 ( 54%)], Train Loss: 2.24370\n",
      "Epoch: 00 [132544/244490 ( 54%)], Train Loss: 2.24289\n",
      "Epoch: 00 [133184/244490 ( 54%)], Train Loss: 2.24149\n",
      "Epoch: 00 [133824/244490 ( 55%)], Train Loss: 2.24093\n",
      "Epoch: 00 [134464/244490 ( 55%)], Train Loss: 2.23971\n",
      "Epoch: 00 [135104/244490 ( 55%)], Train Loss: 2.23890\n",
      "Epoch: 00 [135744/244490 ( 56%)], Train Loss: 2.23860\n",
      "Epoch: 00 [136384/244490 ( 56%)], Train Loss: 2.23801\n",
      "Epoch: 00 [137024/244490 ( 56%)], Train Loss: 2.23718\n",
      "Epoch: 00 [137664/244490 ( 56%)], Train Loss: 2.23633\n",
      "Epoch: 00 [138304/244490 ( 57%)], Train Loss: 2.23564\n",
      "Epoch: 00 [138944/244490 ( 57%)], Train Loss: 2.23438\n",
      "Epoch: 00 [139584/244490 ( 57%)], Train Loss: 2.23299\n",
      "Epoch: 00 [140224/244490 ( 57%)], Train Loss: 2.23198\n",
      "Epoch: 00 [140864/244490 ( 58%)], Train Loss: 2.23110\n",
      "Epoch: 00 [141504/244490 ( 58%)], Train Loss: 2.22976\n",
      "Epoch: 00 [142144/244490 ( 58%)], Train Loss: 2.22882\n",
      "Epoch: 00 [142784/244490 ( 58%)], Train Loss: 2.22819\n",
      "Epoch: 00 [143424/244490 ( 59%)], Train Loss: 2.22698\n",
      "Epoch: 00 [144064/244490 ( 59%)], Train Loss: 2.22581\n",
      "Epoch: 00 [144704/244490 ( 59%)], Train Loss: 2.22494\n",
      "Epoch: 00 [145344/244490 ( 59%)], Train Loss: 2.22377\n",
      "Epoch: 00 [145984/244490 ( 60%)], Train Loss: 2.22306\n",
      "Epoch: 00 [146624/244490 ( 60%)], Train Loss: 2.22226\n",
      "Epoch: 00 [147264/244490 ( 60%)], Train Loss: 2.22173\n",
      "Epoch: 00 [147904/244490 ( 60%)], Train Loss: 2.22106\n",
      "Epoch: 00 [148544/244490 ( 61%)], Train Loss: 2.22012\n",
      "Epoch: 00 [149184/244490 ( 61%)], Train Loss: 2.22003\n",
      "Epoch: 00 [149824/244490 ( 61%)], Train Loss: 2.21935\n",
      "Epoch: 00 [150464/244490 ( 62%)], Train Loss: 2.21847\n",
      "Epoch: 00 [151104/244490 ( 62%)], Train Loss: 2.21844\n",
      "Epoch: 00 [151744/244490 ( 62%)], Train Loss: 2.21784\n",
      "Epoch: 00 [152384/244490 ( 62%)], Train Loss: 2.21733\n",
      "Epoch: 00 [153024/244490 ( 63%)], Train Loss: 2.21606\n",
      "Epoch: 00 [153664/244490 ( 63%)], Train Loss: 2.21547\n",
      "Epoch: 00 [154304/244490 ( 63%)], Train Loss: 2.21475\n",
      "Epoch: 00 [154944/244490 ( 63%)], Train Loss: 2.21418\n",
      "Epoch: 00 [155584/244490 ( 64%)], Train Loss: 2.21331\n",
      "Epoch: 00 [156224/244490 ( 64%)], Train Loss: 2.21339\n",
      "Epoch: 00 [156864/244490 ( 64%)], Train Loss: 2.21238\n",
      "Epoch: 00 [157504/244490 ( 64%)], Train Loss: 2.21123\n",
      "Epoch: 00 [158144/244490 ( 65%)], Train Loss: 2.21074\n",
      "Epoch: 00 [158784/244490 ( 65%)], Train Loss: 2.21017\n",
      "Epoch: 00 [159424/244490 ( 65%)], Train Loss: 2.21001\n",
      "Epoch: 00 [160064/244490 ( 65%)], Train Loss: 2.20915\n",
      "Epoch: 00 [160704/244490 ( 66%)], Train Loss: 2.20820\n",
      "Epoch: 00 [161344/244490 ( 66%)], Train Loss: 2.20776\n",
      "Epoch: 00 [161984/244490 ( 66%)], Train Loss: 2.20679\n",
      "Epoch: 00 [162624/244490 ( 67%)], Train Loss: 2.20566\n",
      "Epoch: 00 [163264/244490 ( 67%)], Train Loss: 2.20478\n",
      "Epoch: 00 [163904/244490 ( 67%)], Train Loss: 2.20408\n",
      "Epoch: 00 [164544/244490 ( 67%)], Train Loss: 2.20305\n",
      "Epoch: 00 [165184/244490 ( 68%)], Train Loss: 2.20225\n",
      "Epoch: 00 [165824/244490 ( 68%)], Train Loss: 2.20126\n",
      "Epoch: 00 [166464/244490 ( 68%)], Train Loss: 2.20096\n",
      "Epoch: 00 [167104/244490 ( 68%)], Train Loss: 2.20065\n",
      "Epoch: 00 [167744/244490 ( 69%)], Train Loss: 2.19992\n",
      "Epoch: 00 [168384/244490 ( 69%)], Train Loss: 2.19899\n",
      "Epoch: 00 [169024/244490 ( 69%)], Train Loss: 2.19842\n",
      "Epoch: 00 [169664/244490 ( 69%)], Train Loss: 2.19785\n",
      "Epoch: 00 [170304/244490 ( 70%)], Train Loss: 2.19685\n",
      "Epoch: 00 [170944/244490 ( 70%)], Train Loss: 2.19600\n",
      "Epoch: 00 [171584/244490 ( 70%)], Train Loss: 2.19537\n",
      "Epoch: 00 [172224/244490 ( 70%)], Train Loss: 2.19491\n",
      "Epoch: 00 [172864/244490 ( 71%)], Train Loss: 2.19394\n",
      "Epoch: 00 [173504/244490 ( 71%)], Train Loss: 2.19344\n",
      "Epoch: 00 [174144/244490 ( 71%)], Train Loss: 2.19282\n",
      "Epoch: 00 [174784/244490 ( 71%)], Train Loss: 2.19224\n",
      "Epoch: 00 [175424/244490 ( 72%)], Train Loss: 2.19118\n",
      "Epoch: 00 [176064/244490 ( 72%)], Train Loss: 2.19080\n",
      "Epoch: 00 [176704/244490 ( 72%)], Train Loss: 2.19040\n",
      "Epoch: 00 [177344/244490 ( 73%)], Train Loss: 2.18974\n",
      "Epoch: 00 [177984/244490 ( 73%)], Train Loss: 2.18919\n",
      "Epoch: 00 [178624/244490 ( 73%)], Train Loss: 2.18861\n",
      "Epoch: 00 [179264/244490 ( 73%)], Train Loss: 2.18786\n",
      "Epoch: 00 [179904/244490 ( 74%)], Train Loss: 2.18752\n",
      "Epoch: 00 [180544/244490 ( 74%)], Train Loss: 2.18699\n",
      "Epoch: 00 [181184/244490 ( 74%)], Train Loss: 2.18595\n",
      "Epoch: 00 [181824/244490 ( 74%)], Train Loss: 2.18480\n",
      "Epoch: 00 [182464/244490 ( 75%)], Train Loss: 2.18391\n",
      "Epoch: 00 [183104/244490 ( 75%)], Train Loss: 2.18339\n",
      "Epoch: 00 [183744/244490 ( 75%)], Train Loss: 2.18248\n",
      "Epoch: 00 [184384/244490 ( 75%)], Train Loss: 2.18172\n",
      "Epoch: 00 [185024/244490 ( 76%)], Train Loss: 2.18090\n",
      "Epoch: 00 [185664/244490 ( 76%)], Train Loss: 2.17972\n",
      "Epoch: 00 [186304/244490 ( 76%)], Train Loss: 2.17923\n",
      "Epoch: 00 [186944/244490 ( 76%)], Train Loss: 2.17842\n",
      "Epoch: 00 [187584/244490 ( 77%)], Train Loss: 2.17756\n",
      "Epoch: 00 [188224/244490 ( 77%)], Train Loss: 2.17694\n",
      "Epoch: 00 [188864/244490 ( 77%)], Train Loss: 2.17612\n",
      "Epoch: 00 [189504/244490 ( 78%)], Train Loss: 2.17556\n",
      "Epoch: 00 [190144/244490 ( 78%)], Train Loss: 2.17492\n",
      "Epoch: 00 [190784/244490 ( 78%)], Train Loss: 2.17434\n",
      "Epoch: 00 [191424/244490 ( 78%)], Train Loss: 2.17378\n",
      "Epoch: 00 [192064/244490 ( 79%)], Train Loss: 2.17304\n",
      "Epoch: 00 [192704/244490 ( 79%)], Train Loss: 2.17230\n",
      "Epoch: 00 [193344/244490 ( 79%)], Train Loss: 2.17164\n",
      "Epoch: 00 [193984/244490 ( 79%)], Train Loss: 2.17108\n",
      "Epoch: 00 [194624/244490 ( 80%)], Train Loss: 2.17036\n",
      "Epoch: 00 [195264/244490 ( 80%)], Train Loss: 2.17008\n",
      "Epoch: 00 [195904/244490 ( 80%)], Train Loss: 2.16909\n",
      "Epoch: 00 [196544/244490 ( 80%)], Train Loss: 2.16834\n",
      "Epoch: 00 [197184/244490 ( 81%)], Train Loss: 2.16776\n",
      "Epoch: 00 [197824/244490 ( 81%)], Train Loss: 2.16745\n",
      "Epoch: 00 [198464/244490 ( 81%)], Train Loss: 2.16668\n",
      "Epoch: 00 [199104/244490 ( 81%)], Train Loss: 2.16597\n",
      "Epoch: 00 [199744/244490 ( 82%)], Train Loss: 2.16549\n",
      "Epoch: 00 [200384/244490 ( 82%)], Train Loss: 2.16472\n",
      "Epoch: 00 [201024/244490 ( 82%)], Train Loss: 2.16386\n",
      "Epoch: 00 [201664/244490 ( 82%)], Train Loss: 2.16318\n",
      "Epoch: 00 [202304/244490 ( 83%)], Train Loss: 2.16269\n",
      "Epoch: 00 [202944/244490 ( 83%)], Train Loss: 2.16188\n",
      "Epoch: 00 [203584/244490 ( 83%)], Train Loss: 2.16095\n",
      "Epoch: 00 [204224/244490 ( 84%)], Train Loss: 2.16070\n",
      "Epoch: 00 [204864/244490 ( 84%)], Train Loss: 2.16009\n",
      "Epoch: 00 [205504/244490 ( 84%)], Train Loss: 2.15961\n",
      "Epoch: 00 [206144/244490 ( 84%)], Train Loss: 2.15919\n",
      "Epoch: 00 [206784/244490 ( 85%)], Train Loss: 2.15864\n",
      "Epoch: 00 [207424/244490 ( 85%)], Train Loss: 2.15797\n",
      "Epoch: 00 [208064/244490 ( 85%)], Train Loss: 2.15695\n",
      "Epoch: 00 [208704/244490 ( 85%)], Train Loss: 2.15672\n",
      "Epoch: 00 [209344/244490 ( 86%)], Train Loss: 2.15602\n",
      "Epoch: 00 [209984/244490 ( 86%)], Train Loss: 2.15558\n",
      "Epoch: 00 [210624/244490 ( 86%)], Train Loss: 2.15494\n",
      "Epoch: 00 [211264/244490 ( 86%)], Train Loss: 2.15433\n",
      "Epoch: 00 [211904/244490 ( 87%)], Train Loss: 2.15367\n",
      "Epoch: 00 [212544/244490 ( 87%)], Train Loss: 2.15307\n",
      "Epoch: 00 [213184/244490 ( 87%)], Train Loss: 2.15284\n",
      "Epoch: 00 [213824/244490 ( 87%)], Train Loss: 2.15237\n",
      "Epoch: 00 [214464/244490 ( 88%)], Train Loss: 2.15182\n",
      "Epoch: 00 [215104/244490 ( 88%)], Train Loss: 2.15126\n",
      "Epoch: 00 [215744/244490 ( 88%)], Train Loss: 2.15051\n",
      "Epoch: 00 [216384/244490 ( 89%)], Train Loss: 2.14986\n",
      "Epoch: 00 [217024/244490 ( 89%)], Train Loss: 2.14932\n",
      "Epoch: 00 [217664/244490 ( 89%)], Train Loss: 2.14850\n",
      "Epoch: 00 [218304/244490 ( 89%)], Train Loss: 2.14789\n",
      "Epoch: 00 [218944/244490 ( 90%)], Train Loss: 2.14761\n",
      "Epoch: 00 [219584/244490 ( 90%)], Train Loss: 2.14713\n",
      "Epoch: 00 [220224/244490 ( 90%)], Train Loss: 2.14672\n",
      "Epoch: 00 [220864/244490 ( 90%)], Train Loss: 2.14606\n",
      "Epoch: 00 [221504/244490 ( 91%)], Train Loss: 2.14559\n",
      "Epoch: 00 [222144/244490 ( 91%)], Train Loss: 2.14481\n",
      "Epoch: 00 [222784/244490 ( 91%)], Train Loss: 2.14429\n",
      "Epoch: 00 [223424/244490 ( 91%)], Train Loss: 2.14379\n",
      "Epoch: 00 [224064/244490 ( 92%)], Train Loss: 2.14296\n",
      "Epoch: 00 [224704/244490 ( 92%)], Train Loss: 2.14243\n",
      "Epoch: 00 [225344/244490 ( 92%)], Train Loss: 2.14189\n",
      "Epoch: 00 [225984/244490 ( 92%)], Train Loss: 2.14128\n",
      "Epoch: 00 [226624/244490 ( 93%)], Train Loss: 2.14067\n",
      "Epoch: 00 [227264/244490 ( 93%)], Train Loss: 2.14001\n",
      "Epoch: 00 [227904/244490 ( 93%)], Train Loss: 2.13921\n",
      "Epoch: 00 [228544/244490 ( 93%)], Train Loss: 2.13891\n",
      "Epoch: 00 [229184/244490 ( 94%)], Train Loss: 2.13828\n",
      "Epoch: 00 [229824/244490 ( 94%)], Train Loss: 2.13808\n",
      "Epoch: 00 [230464/244490 ( 94%)], Train Loss: 2.13778\n",
      "Epoch: 00 [231104/244490 ( 95%)], Train Loss: 2.13732\n",
      "Epoch: 00 [231744/244490 ( 95%)], Train Loss: 2.13699\n",
      "Epoch: 00 [232384/244490 ( 95%)], Train Loss: 2.13644\n",
      "Epoch: 00 [233024/244490 ( 95%)], Train Loss: 2.13616\n",
      "Epoch: 00 [233664/244490 ( 96%)], Train Loss: 2.13525\n",
      "Epoch: 00 [234304/244490 ( 96%)], Train Loss: 2.13494\n",
      "Epoch: 00 [234944/244490 ( 96%)], Train Loss: 2.13447\n",
      "Epoch: 00 [235584/244490 ( 96%)], Train Loss: 2.13411\n",
      "Epoch: 00 [236224/244490 ( 97%)], Train Loss: 2.13353\n",
      "Epoch: 00 [236864/244490 ( 97%)], Train Loss: 2.13300\n",
      "Epoch: 00 [237504/244490 ( 97%)], Train Loss: 2.13268\n",
      "Epoch: 00 [238144/244490 ( 97%)], Train Loss: 2.13180\n",
      "Epoch: 00 [238784/244490 ( 98%)], Train Loss: 2.13140\n",
      "Epoch: 00 [239424/244490 ( 98%)], Train Loss: 2.13122\n",
      "Epoch: 00 [240064/244490 ( 98%)], Train Loss: 2.13070\n",
      "Epoch: 00 [240704/244490 ( 98%)], Train Loss: 2.13051\n",
      "Epoch: 00 [241344/244490 ( 99%)], Train Loss: 2.13033\n",
      "Epoch: 00 [241984/244490 ( 99%)], Train Loss: 2.12960\n",
      "Epoch: 00 [242624/244490 ( 99%)], Train Loss: 2.12906\n",
      "Epoch: 00 [243264/244490 ( 99%)], Train Loss: 2.12864\n",
      "Epoch: 00 [243904/244490 (100%)], Train Loss: 2.12807\n",
      "Epoch: 00 [244490/244490 (100%)], Train Loss: 2.12716\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 1.91271\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 1.91271\n",
      "Saving model checkpoint to output/checkpoint-fold-0.\n",
      "\n",
      "Epoch: 01 [    64/244490 (  0%)], Train Loss: 1.90732\n",
      "Epoch: 01 [   704/244490 (  0%)], Train Loss: 1.90034\n",
      "Epoch: 01 [  1344/244490 (  1%)], Train Loss: 1.92193\n",
      "Epoch: 01 [  1984/244490 (  1%)], Train Loss: 1.91093\n",
      "Epoch: 01 [  2624/244490 (  1%)], Train Loss: 1.89924\n",
      "Epoch: 01 [  3264/244490 (  1%)], Train Loss: 1.89188\n",
      "Epoch: 01 [  3904/244490 (  2%)], Train Loss: 1.88806\n",
      "Epoch: 01 [  4544/244490 (  2%)], Train Loss: 1.88757\n",
      "Epoch: 01 [  5184/244490 (  2%)], Train Loss: 1.89663\n",
      "Epoch: 01 [  5824/244490 (  2%)], Train Loss: 1.90461\n",
      "Epoch: 01 [  6464/244490 (  3%)], Train Loss: 1.90557\n",
      "Epoch: 01 [  7104/244490 (  3%)], Train Loss: 1.91223\n",
      "Epoch: 01 [  7744/244490 (  3%)], Train Loss: 1.91241\n",
      "Epoch: 01 [  8384/244490 (  3%)], Train Loss: 1.91157\n",
      "Epoch: 01 [  9024/244490 (  4%)], Train Loss: 1.91841\n",
      "Epoch: 01 [  9664/244490 (  4%)], Train Loss: 1.91222\n",
      "Epoch: 01 [ 10304/244490 (  4%)], Train Loss: 1.91139\n",
      "Epoch: 01 [ 10944/244490 (  4%)], Train Loss: 1.90735\n",
      "Epoch: 01 [ 11584/244490 (  5%)], Train Loss: 1.90769\n",
      "Epoch: 01 [ 12224/244490 (  5%)], Train Loss: 1.91179\n",
      "Epoch: 01 [ 12864/244490 (  5%)], Train Loss: 1.91703\n",
      "Epoch: 01 [ 13504/244490 (  6%)], Train Loss: 1.91932\n",
      "Epoch: 01 [ 14144/244490 (  6%)], Train Loss: 1.91781\n",
      "Epoch: 01 [ 14784/244490 (  6%)], Train Loss: 1.91622\n",
      "Epoch: 01 [ 15424/244490 (  6%)], Train Loss: 1.91909\n",
      "Epoch: 01 [ 16064/244490 (  7%)], Train Loss: 1.92191\n",
      "Epoch: 01 [ 16704/244490 (  7%)], Train Loss: 1.92461\n",
      "Epoch: 01 [ 17344/244490 (  7%)], Train Loss: 1.92067\n",
      "Epoch: 01 [ 17984/244490 (  7%)], Train Loss: 1.91965\n",
      "Epoch: 01 [ 18624/244490 (  8%)], Train Loss: 1.91706\n",
      "Epoch: 01 [ 19264/244490 (  8%)], Train Loss: 1.91746\n",
      "Epoch: 01 [ 19904/244490 (  8%)], Train Loss: 1.91698\n",
      "Epoch: 01 [ 20544/244490 (  8%)], Train Loss: 1.91585\n",
      "Epoch: 01 [ 21184/244490 (  9%)], Train Loss: 1.91654\n",
      "Epoch: 01 [ 21824/244490 (  9%)], Train Loss: 1.91726\n",
      "Epoch: 01 [ 22464/244490 (  9%)], Train Loss: 1.91786\n",
      "Epoch: 01 [ 23104/244490 (  9%)], Train Loss: 1.91647\n",
      "Epoch: 01 [ 23744/244490 ( 10%)], Train Loss: 1.91648\n",
      "Epoch: 01 [ 24384/244490 ( 10%)], Train Loss: 1.91857\n",
      "Epoch: 01 [ 25024/244490 ( 10%)], Train Loss: 1.91696\n",
      "Epoch: 01 [ 25664/244490 ( 10%)], Train Loss: 1.91423\n",
      "Epoch: 01 [ 26304/244490 ( 11%)], Train Loss: 1.91283\n",
      "Epoch: 01 [ 26944/244490 ( 11%)], Train Loss: 1.91079\n",
      "Epoch: 01 [ 27584/244490 ( 11%)], Train Loss: 1.90813\n",
      "Epoch: 01 [ 28224/244490 ( 12%)], Train Loss: 1.90620\n",
      "Epoch: 01 [ 28864/244490 ( 12%)], Train Loss: 1.90407\n",
      "Epoch: 01 [ 29504/244490 ( 12%)], Train Loss: 1.90404\n",
      "Epoch: 01 [ 30144/244490 ( 12%)], Train Loss: 1.90223\n",
      "Epoch: 01 [ 30784/244490 ( 13%)], Train Loss: 1.90164\n",
      "Epoch: 01 [ 31424/244490 ( 13%)], Train Loss: 1.89826\n",
      "Epoch: 01 [ 32064/244490 ( 13%)], Train Loss: 1.89617\n",
      "Epoch: 01 [ 32704/244490 ( 13%)], Train Loss: 1.89462\n",
      "Epoch: 01 [ 33344/244490 ( 14%)], Train Loss: 1.89385\n",
      "Epoch: 01 [ 33984/244490 ( 14%)], Train Loss: 1.89401\n",
      "Epoch: 01 [ 34624/244490 ( 14%)], Train Loss: 1.89600\n",
      "Epoch: 01 [ 35264/244490 ( 14%)], Train Loss: 1.89649\n",
      "Epoch: 01 [ 35904/244490 ( 15%)], Train Loss: 1.89575\n",
      "Epoch: 01 [ 36544/244490 ( 15%)], Train Loss: 1.89463\n",
      "Epoch: 01 [ 37184/244490 ( 15%)], Train Loss: 1.89253\n",
      "Epoch: 01 [ 37824/244490 ( 15%)], Train Loss: 1.88956\n",
      "Epoch: 01 [ 38464/244490 ( 16%)], Train Loss: 1.88755\n",
      "Epoch: 01 [ 39104/244490 ( 16%)], Train Loss: 1.88617\n",
      "Epoch: 01 [ 39744/244490 ( 16%)], Train Loss: 1.88677\n",
      "Epoch: 01 [ 40384/244490 ( 17%)], Train Loss: 1.88614\n",
      "Epoch: 01 [ 41024/244490 ( 17%)], Train Loss: 1.88491\n",
      "Epoch: 01 [ 41664/244490 ( 17%)], Train Loss: 1.88434\n",
      "Epoch: 01 [ 42304/244490 ( 17%)], Train Loss: 1.88592\n",
      "Epoch: 01 [ 42944/244490 ( 18%)], Train Loss: 1.88498\n",
      "Epoch: 01 [ 43584/244490 ( 18%)], Train Loss: 1.88276\n",
      "Epoch: 01 [ 44224/244490 ( 18%)], Train Loss: 1.88129\n",
      "Epoch: 01 [ 44864/244490 ( 18%)], Train Loss: 1.88016\n",
      "Epoch: 01 [ 45504/244490 ( 19%)], Train Loss: 1.88130\n",
      "Epoch: 01 [ 46144/244490 ( 19%)], Train Loss: 1.87964\n",
      "Epoch: 01 [ 46784/244490 ( 19%)], Train Loss: 1.87924\n",
      "Epoch: 01 [ 47424/244490 ( 19%)], Train Loss: 1.88076\n",
      "Epoch: 01 [ 48064/244490 ( 20%)], Train Loss: 1.87952\n",
      "Epoch: 01 [ 48704/244490 ( 20%)], Train Loss: 1.88063\n",
      "Epoch: 01 [ 49344/244490 ( 20%)], Train Loss: 1.88028\n",
      "Epoch: 01 [ 49984/244490 ( 20%)], Train Loss: 1.88047\n",
      "Epoch: 01 [ 50624/244490 ( 21%)], Train Loss: 1.87936\n",
      "Epoch: 01 [ 51264/244490 ( 21%)], Train Loss: 1.87904\n",
      "Epoch: 01 [ 51904/244490 ( 21%)], Train Loss: 1.87831\n",
      "Epoch: 01 [ 52544/244490 ( 21%)], Train Loss: 1.87790\n",
      "Epoch: 01 [ 53184/244490 ( 22%)], Train Loss: 1.87721\n",
      "Epoch: 01 [ 53824/244490 ( 22%)], Train Loss: 1.87683\n",
      "Epoch: 01 [ 54464/244490 ( 22%)], Train Loss: 1.87513\n",
      "Epoch: 01 [ 55104/244490 ( 23%)], Train Loss: 1.87597\n",
      "Epoch: 01 [ 55744/244490 ( 23%)], Train Loss: 1.87616\n",
      "Epoch: 01 [ 56384/244490 ( 23%)], Train Loss: 1.87417\n",
      "Epoch: 01 [ 57024/244490 ( 23%)], Train Loss: 1.87361\n",
      "Epoch: 01 [ 57664/244490 ( 24%)], Train Loss: 1.87318\n",
      "Epoch: 01 [ 58304/244490 ( 24%)], Train Loss: 1.87212\n",
      "Epoch: 01 [ 58944/244490 ( 24%)], Train Loss: 1.87061\n",
      "Epoch: 01 [ 59584/244490 ( 24%)], Train Loss: 1.87048\n",
      "Epoch: 01 [ 60224/244490 ( 25%)], Train Loss: 1.86951\n",
      "Epoch: 01 [ 60864/244490 ( 25%)], Train Loss: 1.87077\n",
      "Epoch: 01 [ 61504/244490 ( 25%)], Train Loss: 1.87078\n",
      "Epoch: 01 [ 62144/244490 ( 25%)], Train Loss: 1.87207\n",
      "Epoch: 01 [ 62784/244490 ( 26%)], Train Loss: 1.87061\n",
      "Epoch: 01 [ 63424/244490 ( 26%)], Train Loss: 1.87031\n",
      "Epoch: 01 [ 64064/244490 ( 26%)], Train Loss: 1.86943\n",
      "Epoch: 01 [ 64704/244490 ( 26%)], Train Loss: 1.87013\n",
      "Epoch: 01 [ 65344/244490 ( 27%)], Train Loss: 1.86979\n",
      "Epoch: 01 [ 65984/244490 ( 27%)], Train Loss: 1.86985\n",
      "Epoch: 01 [ 66624/244490 ( 27%)], Train Loss: 1.87091\n",
      "Epoch: 01 [ 67264/244490 ( 28%)], Train Loss: 1.87042\n",
      "Epoch: 01 [ 67904/244490 ( 28%)], Train Loss: 1.87058\n",
      "Epoch: 01 [ 68544/244490 ( 28%)], Train Loss: 1.87093\n",
      "Epoch: 01 [ 69184/244490 ( 28%)], Train Loss: 1.86948\n",
      "Epoch: 01 [ 69824/244490 ( 29%)], Train Loss: 1.86815\n",
      "Epoch: 01 [ 70464/244490 ( 29%)], Train Loss: 1.86797\n",
      "Epoch: 01 [ 71104/244490 ( 29%)], Train Loss: 1.86775\n",
      "Epoch: 01 [ 71744/244490 ( 29%)], Train Loss: 1.86620\n",
      "Epoch: 01 [ 72384/244490 ( 30%)], Train Loss: 1.86550\n",
      "Epoch: 01 [ 73024/244490 ( 30%)], Train Loss: 1.86471\n",
      "Epoch: 01 [ 73664/244490 ( 30%)], Train Loss: 1.86426\n",
      "Epoch: 01 [ 74304/244490 ( 30%)], Train Loss: 1.86488\n",
      "Epoch: 01 [ 74944/244490 ( 31%)], Train Loss: 1.86421\n",
      "Epoch: 01 [ 75584/244490 ( 31%)], Train Loss: 1.86258\n",
      "Epoch: 01 [ 76224/244490 ( 31%)], Train Loss: 1.86200\n",
      "Epoch: 01 [ 76864/244490 ( 31%)], Train Loss: 1.86005\n",
      "Epoch: 01 [ 77504/244490 ( 32%)], Train Loss: 1.85969\n",
      "Epoch: 01 [ 78144/244490 ( 32%)], Train Loss: 1.85995\n",
      "Epoch: 01 [ 78784/244490 ( 32%)], Train Loss: 1.86058\n",
      "Epoch: 01 [ 79424/244490 ( 32%)], Train Loss: 1.86144\n",
      "Epoch: 01 [ 80064/244490 ( 33%)], Train Loss: 1.85986\n",
      "Epoch: 01 [ 80704/244490 ( 33%)], Train Loss: 1.85959\n",
      "Epoch: 01 [ 81344/244490 ( 33%)], Train Loss: 1.85948\n",
      "Epoch: 01 [ 81984/244490 ( 34%)], Train Loss: 1.85885\n",
      "Epoch: 01 [ 82624/244490 ( 34%)], Train Loss: 1.85856\n",
      "Epoch: 01 [ 83264/244490 ( 34%)], Train Loss: 1.85853\n",
      "Epoch: 01 [ 83904/244490 ( 34%)], Train Loss: 1.85717\n",
      "Epoch: 01 [ 84544/244490 ( 35%)], Train Loss: 1.85557\n",
      "Epoch: 01 [ 85184/244490 ( 35%)], Train Loss: 1.85483\n",
      "Epoch: 01 [ 85824/244490 ( 35%)], Train Loss: 1.85319\n",
      "Epoch: 01 [ 86464/244490 ( 35%)], Train Loss: 1.85264\n",
      "Epoch: 01 [ 87104/244490 ( 36%)], Train Loss: 1.85196\n",
      "Epoch: 01 [ 87744/244490 ( 36%)], Train Loss: 1.85144\n",
      "Epoch: 01 [ 88384/244490 ( 36%)], Train Loss: 1.85141\n",
      "Epoch: 01 [ 89024/244490 ( 36%)], Train Loss: 1.85146\n",
      "Epoch: 01 [ 89664/244490 ( 37%)], Train Loss: 1.85098\n",
      "Epoch: 01 [ 90304/244490 ( 37%)], Train Loss: 1.85073\n",
      "Epoch: 01 [ 90944/244490 ( 37%)], Train Loss: 1.85015\n",
      "Epoch: 01 [ 91584/244490 ( 37%)], Train Loss: 1.84992\n",
      "Epoch: 01 [ 92224/244490 ( 38%)], Train Loss: 1.85054\n",
      "Epoch: 01 [ 92864/244490 ( 38%)], Train Loss: 1.84995\n",
      "Epoch: 01 [ 93504/244490 ( 38%)], Train Loss: 1.84898\n",
      "Epoch: 01 [ 94144/244490 ( 39%)], Train Loss: 1.84823\n",
      "Epoch: 01 [ 94784/244490 ( 39%)], Train Loss: 1.84771\n",
      "Epoch: 01 [ 95424/244490 ( 39%)], Train Loss: 1.84690\n",
      "Epoch: 01 [ 96064/244490 ( 39%)], Train Loss: 1.84624\n",
      "Epoch: 01 [ 96704/244490 ( 40%)], Train Loss: 1.84551\n",
      "Epoch: 01 [ 97344/244490 ( 40%)], Train Loss: 1.84530\n",
      "Epoch: 01 [ 97984/244490 ( 40%)], Train Loss: 1.84512\n",
      "Epoch: 01 [ 98624/244490 ( 40%)], Train Loss: 1.84428\n",
      "Epoch: 01 [ 99264/244490 ( 41%)], Train Loss: 1.84299\n",
      "Epoch: 01 [ 99904/244490 ( 41%)], Train Loss: 1.84232\n",
      "Epoch: 01 [100544/244490 ( 41%)], Train Loss: 1.84186\n",
      "Epoch: 01 [101184/244490 ( 41%)], Train Loss: 1.84241\n",
      "Epoch: 01 [101824/244490 ( 42%)], Train Loss: 1.84171\n",
      "Epoch: 01 [102464/244490 ( 42%)], Train Loss: 1.84131\n",
      "Epoch: 01 [103104/244490 ( 42%)], Train Loss: 1.84001\n",
      "Epoch: 01 [103744/244490 ( 42%)], Train Loss: 1.83922\n",
      "Epoch: 01 [104384/244490 ( 43%)], Train Loss: 1.83936\n",
      "Epoch: 01 [105024/244490 ( 43%)], Train Loss: 1.83918\n",
      "Epoch: 01 [105664/244490 ( 43%)], Train Loss: 1.83964\n",
      "Epoch: 01 [106304/244490 ( 43%)], Train Loss: 1.83959\n",
      "Epoch: 01 [106944/244490 ( 44%)], Train Loss: 1.83925\n",
      "Epoch: 01 [107584/244490 ( 44%)], Train Loss: 1.83904\n",
      "Epoch: 01 [108224/244490 ( 44%)], Train Loss: 1.83885\n",
      "Epoch: 01 [108864/244490 ( 45%)], Train Loss: 1.83875\n",
      "Epoch: 01 [109504/244490 ( 45%)], Train Loss: 1.83846\n",
      "Epoch: 01 [110144/244490 ( 45%)], Train Loss: 1.83740\n",
      "Epoch: 01 [110784/244490 ( 45%)], Train Loss: 1.83604\n",
      "Epoch: 01 [111424/244490 ( 46%)], Train Loss: 1.83533\n",
      "Epoch: 01 [112064/244490 ( 46%)], Train Loss: 1.83476\n",
      "Epoch: 01 [112704/244490 ( 46%)], Train Loss: 1.83390\n",
      "Epoch: 01 [113344/244490 ( 46%)], Train Loss: 1.83371\n",
      "Epoch: 01 [113984/244490 ( 47%)], Train Loss: 1.83345\n",
      "Epoch: 01 [114624/244490 ( 47%)], Train Loss: 1.83368\n",
      "Epoch: 01 [115264/244490 ( 47%)], Train Loss: 1.83301\n",
      "Epoch: 01 [115904/244490 ( 47%)], Train Loss: 1.83234\n",
      "Epoch: 01 [116544/244490 ( 48%)], Train Loss: 1.83241\n",
      "Epoch: 01 [117184/244490 ( 48%)], Train Loss: 1.83132\n",
      "Epoch: 01 [117824/244490 ( 48%)], Train Loss: 1.83094\n",
      "Epoch: 01 [118464/244490 ( 48%)], Train Loss: 1.82998\n",
      "Epoch: 01 [119104/244490 ( 49%)], Train Loss: 1.82988\n",
      "Epoch: 01 [119744/244490 ( 49%)], Train Loss: 1.82909\n",
      "Epoch: 01 [120384/244490 ( 49%)], Train Loss: 1.82823\n",
      "Epoch: 01 [121024/244490 ( 50%)], Train Loss: 1.82826\n",
      "Epoch: 01 [121664/244490 ( 50%)], Train Loss: 1.82771\n",
      "Epoch: 01 [122304/244490 ( 50%)], Train Loss: 1.82749\n",
      "Epoch: 01 [122944/244490 ( 50%)], Train Loss: 1.82697\n",
      "Epoch: 01 [123584/244490 ( 51%)], Train Loss: 1.82638\n",
      "Epoch: 01 [124224/244490 ( 51%)], Train Loss: 1.82613\n",
      "Epoch: 01 [124864/244490 ( 51%)], Train Loss: 1.82698\n",
      "Epoch: 01 [125504/244490 ( 51%)], Train Loss: 1.82716\n",
      "Epoch: 01 [126144/244490 ( 52%)], Train Loss: 1.82675\n",
      "Epoch: 01 [126784/244490 ( 52%)], Train Loss: 1.82661\n",
      "Epoch: 01 [127424/244490 ( 52%)], Train Loss: 1.82636\n",
      "Epoch: 01 [128064/244490 ( 52%)], Train Loss: 1.82719\n",
      "Epoch: 01 [128704/244490 ( 53%)], Train Loss: 1.82676\n",
      "Epoch: 01 [129344/244490 ( 53%)], Train Loss: 1.82714\n",
      "Epoch: 01 [129984/244490 ( 53%)], Train Loss: 1.82602\n",
      "Epoch: 01 [130624/244490 ( 53%)], Train Loss: 1.82567\n",
      "Epoch: 01 [131264/244490 ( 54%)], Train Loss: 1.82579\n",
      "Epoch: 01 [131904/244490 ( 54%)], Train Loss: 1.82569\n",
      "Epoch: 01 [132544/244490 ( 54%)], Train Loss: 1.82516\n",
      "Epoch: 01 [133184/244490 ( 54%)], Train Loss: 1.82453\n",
      "Epoch: 01 [133824/244490 ( 55%)], Train Loss: 1.82431\n",
      "Epoch: 01 [134464/244490 ( 55%)], Train Loss: 1.82375\n",
      "Epoch: 01 [135104/244490 ( 55%)], Train Loss: 1.82373\n",
      "Epoch: 01 [135744/244490 ( 56%)], Train Loss: 1.82378\n",
      "Epoch: 01 [136384/244490 ( 56%)], Train Loss: 1.82394\n",
      "Epoch: 01 [137024/244490 ( 56%)], Train Loss: 1.82324\n",
      "Epoch: 01 [137664/244490 ( 56%)], Train Loss: 1.82292\n",
      "Epoch: 01 [138304/244490 ( 57%)], Train Loss: 1.82270\n",
      "Epoch: 01 [138944/244490 ( 57%)], Train Loss: 1.82180\n",
      "Epoch: 01 [139584/244490 ( 57%)], Train Loss: 1.82096\n",
      "Epoch: 01 [140224/244490 ( 57%)], Train Loss: 1.82051\n",
      "Epoch: 01 [140864/244490 ( 58%)], Train Loss: 1.82029\n",
      "Epoch: 01 [141504/244490 ( 58%)], Train Loss: 1.81930\n",
      "Epoch: 01 [142144/244490 ( 58%)], Train Loss: 1.81909\n",
      "Epoch: 01 [142784/244490 ( 58%)], Train Loss: 1.81905\n",
      "Epoch: 01 [143424/244490 ( 59%)], Train Loss: 1.81851\n",
      "Epoch: 01 [144064/244490 ( 59%)], Train Loss: 1.81809\n",
      "Epoch: 01 [144704/244490 ( 59%)], Train Loss: 1.81772\n",
      "Epoch: 01 [145344/244490 ( 59%)], Train Loss: 1.81696\n",
      "Epoch: 01 [145984/244490 ( 60%)], Train Loss: 1.81707\n",
      "Epoch: 01 [146624/244490 ( 60%)], Train Loss: 1.81696\n",
      "Epoch: 01 [147264/244490 ( 60%)], Train Loss: 1.81701\n",
      "Epoch: 01 [147904/244490 ( 60%)], Train Loss: 1.81711\n",
      "Epoch: 01 [148544/244490 ( 61%)], Train Loss: 1.81690\n",
      "Epoch: 01 [149184/244490 ( 61%)], Train Loss: 1.81711\n",
      "Epoch: 01 [149824/244490 ( 61%)], Train Loss: 1.81662\n",
      "Epoch: 01 [150464/244490 ( 62%)], Train Loss: 1.81607\n",
      "Epoch: 01 [151104/244490 ( 62%)], Train Loss: 1.81613\n",
      "Epoch: 01 [151744/244490 ( 62%)], Train Loss: 1.81588\n",
      "Epoch: 01 [152384/244490 ( 62%)], Train Loss: 1.81587\n",
      "Epoch: 01 [153024/244490 ( 63%)], Train Loss: 1.81507\n",
      "Epoch: 01 [153664/244490 ( 63%)], Train Loss: 1.81448\n",
      "Epoch: 01 [154304/244490 ( 63%)], Train Loss: 1.81404\n",
      "Epoch: 01 [154944/244490 ( 63%)], Train Loss: 1.81390\n",
      "Epoch: 01 [155584/244490 ( 64%)], Train Loss: 1.81339\n",
      "Epoch: 01 [156224/244490 ( 64%)], Train Loss: 1.81377\n",
      "Epoch: 01 [156864/244490 ( 64%)], Train Loss: 1.81325\n",
      "Epoch: 01 [157504/244490 ( 64%)], Train Loss: 1.81239\n",
      "Epoch: 01 [158144/244490 ( 65%)], Train Loss: 1.81216\n",
      "Epoch: 01 [158784/244490 ( 65%)], Train Loss: 1.81192\n",
      "Epoch: 01 [159424/244490 ( 65%)], Train Loss: 1.81218\n",
      "Epoch: 01 [160064/244490 ( 65%)], Train Loss: 1.81153\n",
      "Epoch: 01 [160704/244490 ( 66%)], Train Loss: 1.81088\n",
      "Epoch: 01 [161344/244490 ( 66%)], Train Loss: 1.81087\n",
      "Epoch: 01 [161984/244490 ( 66%)], Train Loss: 1.81027\n",
      "Epoch: 01 [162624/244490 ( 67%)], Train Loss: 1.80948\n",
      "Epoch: 01 [163264/244490 ( 67%)], Train Loss: 1.80917\n",
      "Epoch: 01 [163904/244490 ( 67%)], Train Loss: 1.80865\n",
      "Epoch: 01 [164544/244490 ( 67%)], Train Loss: 1.80799\n",
      "Epoch: 01 [165184/244490 ( 68%)], Train Loss: 1.80751\n",
      "Epoch: 01 [165824/244490 ( 68%)], Train Loss: 1.80703\n",
      "Epoch: 01 [166464/244490 ( 68%)], Train Loss: 1.80704\n",
      "Epoch: 01 [167104/244490 ( 68%)], Train Loss: 1.80693\n",
      "Epoch: 01 [167744/244490 ( 69%)], Train Loss: 1.80661\n",
      "Epoch: 01 [168384/244490 ( 69%)], Train Loss: 1.80597\n",
      "Epoch: 01 [169024/244490 ( 69%)], Train Loss: 1.80559\n",
      "Epoch: 01 [169664/244490 ( 69%)], Train Loss: 1.80525\n",
      "Epoch: 01 [170304/244490 ( 70%)], Train Loss: 1.80452\n",
      "Epoch: 01 [170944/244490 ( 70%)], Train Loss: 1.80384\n",
      "Epoch: 01 [171584/244490 ( 70%)], Train Loss: 1.80361\n",
      "Epoch: 01 [172224/244490 ( 70%)], Train Loss: 1.80363\n",
      "Epoch: 01 [172864/244490 ( 71%)], Train Loss: 1.80302\n",
      "Epoch: 01 [173504/244490 ( 71%)], Train Loss: 1.80301\n",
      "Epoch: 01 [174144/244490 ( 71%)], Train Loss: 1.80266\n",
      "Epoch: 01 [174784/244490 ( 71%)], Train Loss: 1.80228\n",
      "Epoch: 01 [175424/244490 ( 72%)], Train Loss: 1.80131\n",
      "Epoch: 01 [176064/244490 ( 72%)], Train Loss: 1.80092\n",
      "Epoch: 01 [176704/244490 ( 72%)], Train Loss: 1.80089\n",
      "Epoch: 01 [177344/244490 ( 73%)], Train Loss: 1.80053\n",
      "Epoch: 01 [177984/244490 ( 73%)], Train Loss: 1.80035\n",
      "Epoch: 01 [178624/244490 ( 73%)], Train Loss: 1.80000\n",
      "Epoch: 01 [179264/244490 ( 73%)], Train Loss: 1.79963\n",
      "Epoch: 01 [179904/244490 ( 74%)], Train Loss: 1.79966\n",
      "Epoch: 01 [180544/244490 ( 74%)], Train Loss: 1.79938\n",
      "Epoch: 01 [181184/244490 ( 74%)], Train Loss: 1.79866\n",
      "Epoch: 01 [181824/244490 ( 74%)], Train Loss: 1.79763\n",
      "Epoch: 01 [182464/244490 ( 75%)], Train Loss: 1.79703\n",
      "Epoch: 01 [183104/244490 ( 75%)], Train Loss: 1.79674\n",
      "Epoch: 01 [183744/244490 ( 75%)], Train Loss: 1.79604\n",
      "Epoch: 01 [184384/244490 ( 75%)], Train Loss: 1.79548\n",
      "Epoch: 01 [185024/244490 ( 76%)], Train Loss: 1.79485\n",
      "Epoch: 01 [185664/244490 ( 76%)], Train Loss: 1.79398\n",
      "Epoch: 01 [186304/244490 ( 76%)], Train Loss: 1.79348\n",
      "Epoch: 01 [186944/244490 ( 76%)], Train Loss: 1.79289\n",
      "Epoch: 01 [187584/244490 ( 77%)], Train Loss: 1.79232\n",
      "Epoch: 01 [188224/244490 ( 77%)], Train Loss: 1.79196\n",
      "Epoch: 01 [188864/244490 ( 77%)], Train Loss: 1.79138\n",
      "Epoch: 01 [189504/244490 ( 78%)], Train Loss: 1.79113\n",
      "Epoch: 01 [190144/244490 ( 78%)], Train Loss: 1.79079\n",
      "Epoch: 01 [190784/244490 ( 78%)], Train Loss: 1.79043\n",
      "Epoch: 01 [191424/244490 ( 78%)], Train Loss: 1.79031\n",
      "Epoch: 01 [192064/244490 ( 79%)], Train Loss: 1.78987\n",
      "Epoch: 01 [192704/244490 ( 79%)], Train Loss: 1.78940\n",
      "Epoch: 01 [193344/244490 ( 79%)], Train Loss: 1.78910\n",
      "Epoch: 01 [193984/244490 ( 79%)], Train Loss: 1.78887\n",
      "Epoch: 01 [194624/244490 ( 80%)], Train Loss: 1.78819\n",
      "Epoch: 01 [195264/244490 ( 80%)], Train Loss: 1.78798\n",
      "Epoch: 01 [195904/244490 ( 80%)], Train Loss: 1.78722\n",
      "Epoch: 01 [196544/244490 ( 80%)], Train Loss: 1.78666\n",
      "Epoch: 01 [197184/244490 ( 81%)], Train Loss: 1.78603\n",
      "Epoch: 01 [197824/244490 ( 81%)], Train Loss: 1.78590\n",
      "Epoch: 01 [198464/244490 ( 81%)], Train Loss: 1.78546\n",
      "Epoch: 01 [199104/244490 ( 81%)], Train Loss: 1.78494\n",
      "Epoch: 01 [199744/244490 ( 82%)], Train Loss: 1.78475\n",
      "Epoch: 01 [200384/244490 ( 82%)], Train Loss: 1.78435\n",
      "Epoch: 01 [201024/244490 ( 82%)], Train Loss: 1.78360\n",
      "Epoch: 01 [201664/244490 ( 82%)], Train Loss: 1.78320\n",
      "Epoch: 01 [202304/244490 ( 83%)], Train Loss: 1.78297\n",
      "Epoch: 01 [202944/244490 ( 83%)], Train Loss: 1.78243\n",
      "Epoch: 01 [203584/244490 ( 83%)], Train Loss: 1.78161\n",
      "Epoch: 01 [204224/244490 ( 84%)], Train Loss: 1.78148\n",
      "Epoch: 01 [204864/244490 ( 84%)], Train Loss: 1.78103\n",
      "Epoch: 01 [205504/244490 ( 84%)], Train Loss: 1.78080\n",
      "Epoch: 01 [206144/244490 ( 84%)], Train Loss: 1.78056\n",
      "Epoch: 01 [206784/244490 ( 85%)], Train Loss: 1.78000\n",
      "Epoch: 01 [207424/244490 ( 85%)], Train Loss: 1.77964\n",
      "Epoch: 01 [208064/244490 ( 85%)], Train Loss: 1.77875\n",
      "Epoch: 01 [208704/244490 ( 85%)], Train Loss: 1.77878\n",
      "Epoch: 01 [209344/244490 ( 86%)], Train Loss: 1.77822\n",
      "Epoch: 01 [209984/244490 ( 86%)], Train Loss: 1.77801\n",
      "Epoch: 01 [210624/244490 ( 86%)], Train Loss: 1.77752\n",
      "Epoch: 01 [211264/244490 ( 86%)], Train Loss: 1.77716\n",
      "Epoch: 01 [211904/244490 ( 87%)], Train Loss: 1.77657\n",
      "Epoch: 01 [212544/244490 ( 87%)], Train Loss: 1.77618\n",
      "Epoch: 01 [213184/244490 ( 87%)], Train Loss: 1.77621\n",
      "Epoch: 01 [213824/244490 ( 87%)], Train Loss: 1.77582\n",
      "Epoch: 01 [214464/244490 ( 88%)], Train Loss: 1.77547\n",
      "Epoch: 01 [215104/244490 ( 88%)], Train Loss: 1.77519\n",
      "Epoch: 01 [215744/244490 ( 88%)], Train Loss: 1.77456\n",
      "Epoch: 01 [216384/244490 ( 89%)], Train Loss: 1.77393\n",
      "Epoch: 01 [217024/244490 ( 89%)], Train Loss: 1.77364\n",
      "Epoch: 01 [217664/244490 ( 89%)], Train Loss: 1.77315\n",
      "Epoch: 01 [218304/244490 ( 89%)], Train Loss: 1.77280\n",
      "Epoch: 01 [218944/244490 ( 90%)], Train Loss: 1.77264\n",
      "Epoch: 01 [219584/244490 ( 90%)], Train Loss: 1.77240\n",
      "Epoch: 01 [220224/244490 ( 90%)], Train Loss: 1.77201\n",
      "Epoch: 01 [220864/244490 ( 90%)], Train Loss: 1.77158\n",
      "Epoch: 01 [221504/244490 ( 91%)], Train Loss: 1.77126\n",
      "Epoch: 01 [222144/244490 ( 91%)], Train Loss: 1.77075\n",
      "Epoch: 01 [222784/244490 ( 91%)], Train Loss: 1.77030\n",
      "Epoch: 01 [223424/244490 ( 91%)], Train Loss: 1.76991\n",
      "Epoch: 01 [224064/244490 ( 92%)], Train Loss: 1.76909\n",
      "Epoch: 01 [224704/244490 ( 92%)], Train Loss: 1.76883\n",
      "Epoch: 01 [225344/244490 ( 92%)], Train Loss: 1.76848\n",
      "Epoch: 01 [225984/244490 ( 92%)], Train Loss: 1.76810\n",
      "Epoch: 01 [226624/244490 ( 93%)], Train Loss: 1.76762\n",
      "Epoch: 01 [227264/244490 ( 93%)], Train Loss: 1.76722\n",
      "Epoch: 01 [227904/244490 ( 93%)], Train Loss: 1.76653\n",
      "Epoch: 01 [228544/244490 ( 93%)], Train Loss: 1.76637\n",
      "Epoch: 01 [229184/244490 ( 94%)], Train Loss: 1.76592\n",
      "Epoch: 01 [229824/244490 ( 94%)], Train Loss: 1.76607\n",
      "Epoch: 01 [230464/244490 ( 94%)], Train Loss: 1.76590\n",
      "Epoch: 01 [231104/244490 ( 95%)], Train Loss: 1.76560\n",
      "Epoch: 01 [231744/244490 ( 95%)], Train Loss: 1.76516\n",
      "Epoch: 01 [232384/244490 ( 95%)], Train Loss: 1.76469\n",
      "Epoch: 01 [233024/244490 ( 95%)], Train Loss: 1.76433\n",
      "Epoch: 01 [233664/244490 ( 96%)], Train Loss: 1.76344\n",
      "Epoch: 01 [234304/244490 ( 96%)], Train Loss: 1.76323\n",
      "Epoch: 01 [234944/244490 ( 96%)], Train Loss: 1.76285\n",
      "Epoch: 01 [235584/244490 ( 96%)], Train Loss: 1.76254\n",
      "Epoch: 01 [236224/244490 ( 97%)], Train Loss: 1.76215\n",
      "Epoch: 01 [236864/244490 ( 97%)], Train Loss: 1.76176\n",
      "Epoch: 01 [237504/244490 ( 97%)], Train Loss: 1.76163\n",
      "Epoch: 01 [238144/244490 ( 97%)], Train Loss: 1.76110\n",
      "Epoch: 01 [238784/244490 ( 98%)], Train Loss: 1.76098\n",
      "Epoch: 01 [239424/244490 ( 98%)], Train Loss: 1.76103\n",
      "Epoch: 01 [240064/244490 ( 98%)], Train Loss: 1.76077\n",
      "Epoch: 01 [240704/244490 ( 98%)], Train Loss: 1.76062\n",
      "Epoch: 01 [241344/244490 ( 99%)], Train Loss: 1.76035\n",
      "Epoch: 01 [241984/244490 ( 99%)], Train Loss: 1.75978\n",
      "Epoch: 01 [242624/244490 ( 99%)], Train Loss: 1.75938\n",
      "Epoch: 01 [243264/244490 ( 99%)], Train Loss: 1.75910\n",
      "Epoch: 01 [243904/244490 (100%)], Train Loss: 1.75873\n",
      "Epoch: 01 [244490/244490 (100%)], Train Loss: 1.75806\n",
      "----Validation Results Summary----\n",
      "Epoch: [1] Valid Loss: 1.79845\n",
      "1 Epoch, Best epoch was updated! Valid Loss: 1.79845\n",
      "Saving model checkpoint to output/checkpoint-fold-0.\n",
      "\n",
      "Epoch: 02 [    64/244490 (  0%)], Train Loss: 1.62389\n",
      "Epoch: 02 [   704/244490 (  0%)], Train Loss: 1.58636\n",
      "Epoch: 02 [  1344/244490 (  1%)], Train Loss: 1.60178\n",
      "Epoch: 02 [  1984/244490 (  1%)], Train Loss: 1.59168\n",
      "Epoch: 02 [  2624/244490 (  1%)], Train Loss: 1.59153\n",
      "Epoch: 02 [  3264/244490 (  1%)], Train Loss: 1.56542\n",
      "Epoch: 02 [  3904/244490 (  2%)], Train Loss: 1.55886\n",
      "Epoch: 02 [  4544/244490 (  2%)], Train Loss: 1.57586\n",
      "Epoch: 02 [  5184/244490 (  2%)], Train Loss: 1.59149\n",
      "Epoch: 02 [  5824/244490 (  2%)], Train Loss: 1.59873\n",
      "Epoch: 02 [  6464/244490 (  3%)], Train Loss: 1.59662\n",
      "Epoch: 02 [  7104/244490 (  3%)], Train Loss: 1.60073\n",
      "Epoch: 02 [  7744/244490 (  3%)], Train Loss: 1.60195\n",
      "Epoch: 02 [  8384/244490 (  3%)], Train Loss: 1.59855\n",
      "Epoch: 02 [  9024/244490 (  4%)], Train Loss: 1.60306\n",
      "Epoch: 02 [  9664/244490 (  4%)], Train Loss: 1.59622\n",
      "Epoch: 02 [ 10304/244490 (  4%)], Train Loss: 1.59644\n",
      "Epoch: 02 [ 10944/244490 (  4%)], Train Loss: 1.59581\n",
      "Epoch: 02 [ 11584/244490 (  5%)], Train Loss: 1.59998\n",
      "Epoch: 02 [ 12224/244490 (  5%)], Train Loss: 1.60568\n",
      "Epoch: 02 [ 12864/244490 (  5%)], Train Loss: 1.60935\n",
      "Epoch: 02 [ 13504/244490 (  6%)], Train Loss: 1.60822\n",
      "Epoch: 02 [ 14144/244490 (  6%)], Train Loss: 1.60575\n",
      "Epoch: 02 [ 14784/244490 (  6%)], Train Loss: 1.60374\n",
      "Epoch: 02 [ 15424/244490 (  6%)], Train Loss: 1.60690\n",
      "Epoch: 02 [ 16064/244490 (  7%)], Train Loss: 1.60803\n",
      "Epoch: 02 [ 16704/244490 (  7%)], Train Loss: 1.60937\n",
      "Epoch: 02 [ 17344/244490 (  7%)], Train Loss: 1.60368\n",
      "Epoch: 02 [ 17984/244490 (  7%)], Train Loss: 1.60222\n",
      "Epoch: 02 [ 18624/244490 (  8%)], Train Loss: 1.59959\n",
      "Epoch: 02 [ 19264/244490 (  8%)], Train Loss: 1.60127\n",
      "Epoch: 02 [ 19904/244490 (  8%)], Train Loss: 1.60165\n",
      "Epoch: 02 [ 20544/244490 (  8%)], Train Loss: 1.59947\n",
      "Epoch: 02 [ 21184/244490 (  9%)], Train Loss: 1.59940\n",
      "Epoch: 02 [ 21824/244490 (  9%)], Train Loss: 1.59982\n",
      "Epoch: 02 [ 22464/244490 (  9%)], Train Loss: 1.60039\n",
      "Epoch: 02 [ 23104/244490 (  9%)], Train Loss: 1.59989\n",
      "Epoch: 02 [ 23744/244490 ( 10%)], Train Loss: 1.59803\n",
      "Epoch: 02 [ 24384/244490 ( 10%)], Train Loss: 1.59986\n",
      "Epoch: 02 [ 25024/244490 ( 10%)], Train Loss: 1.59706\n",
      "Epoch: 02 [ 25664/244490 ( 10%)], Train Loss: 1.59495\n",
      "Epoch: 02 [ 26304/244490 ( 11%)], Train Loss: 1.59353\n",
      "Epoch: 02 [ 26944/244490 ( 11%)], Train Loss: 1.59147\n",
      "Epoch: 02 [ 27584/244490 ( 11%)], Train Loss: 1.58833\n",
      "Epoch: 02 [ 28224/244490 ( 12%)], Train Loss: 1.58636\n",
      "Epoch: 02 [ 28864/244490 ( 12%)], Train Loss: 1.58423\n",
      "Epoch: 02 [ 29504/244490 ( 12%)], Train Loss: 1.58442\n",
      "Epoch: 02 [ 30144/244490 ( 12%)], Train Loss: 1.58435\n",
      "Epoch: 02 [ 30784/244490 ( 13%)], Train Loss: 1.58332\n",
      "Epoch: 02 [ 31424/244490 ( 13%)], Train Loss: 1.58040\n",
      "Epoch: 02 [ 32064/244490 ( 13%)], Train Loss: 1.57911\n",
      "Epoch: 02 [ 32704/244490 ( 13%)], Train Loss: 1.57872\n",
      "Epoch: 02 [ 33344/244490 ( 14%)], Train Loss: 1.57869\n",
      "Epoch: 02 [ 33984/244490 ( 14%)], Train Loss: 1.57876\n",
      "Epoch: 02 [ 34624/244490 ( 14%)], Train Loss: 1.58112\n",
      "Epoch: 02 [ 35264/244490 ( 14%)], Train Loss: 1.58117\n",
      "Epoch: 02 [ 35904/244490 ( 15%)], Train Loss: 1.58053\n",
      "Epoch: 02 [ 36544/244490 ( 15%)], Train Loss: 1.57876\n",
      "Epoch: 02 [ 37184/244490 ( 15%)], Train Loss: 1.57560\n",
      "Epoch: 02 [ 37824/244490 ( 15%)], Train Loss: 1.57251\n",
      "Epoch: 02 [ 38464/244490 ( 16%)], Train Loss: 1.57014\n",
      "Epoch: 02 [ 39104/244490 ( 16%)], Train Loss: 1.56787\n",
      "Epoch: 02 [ 39744/244490 ( 16%)], Train Loss: 1.56833\n",
      "Epoch: 02 [ 40384/244490 ( 17%)], Train Loss: 1.56766\n",
      "Epoch: 02 [ 41024/244490 ( 17%)], Train Loss: 1.56617\n",
      "Epoch: 02 [ 41664/244490 ( 17%)], Train Loss: 1.56583\n",
      "Epoch: 02 [ 42304/244490 ( 17%)], Train Loss: 1.56674\n",
      "Epoch: 02 [ 42944/244490 ( 18%)], Train Loss: 1.56531\n",
      "Epoch: 02 [ 43584/244490 ( 18%)], Train Loss: 1.56318\n",
      "Epoch: 02 [ 44224/244490 ( 18%)], Train Loss: 1.56142\n",
      "Epoch: 02 [ 44864/244490 ( 18%)], Train Loss: 1.55973\n",
      "Epoch: 02 [ 45504/244490 ( 19%)], Train Loss: 1.55978\n",
      "Epoch: 02 [ 46144/244490 ( 19%)], Train Loss: 1.55825\n",
      "Epoch: 02 [ 46784/244490 ( 19%)], Train Loss: 1.55778\n",
      "Epoch: 02 [ 47424/244490 ( 19%)], Train Loss: 1.55809\n",
      "Epoch: 02 [ 48064/244490 ( 20%)], Train Loss: 1.55653\n",
      "Epoch: 02 [ 48704/244490 ( 20%)], Train Loss: 1.55743\n",
      "Epoch: 02 [ 49344/244490 ( 20%)], Train Loss: 1.55786\n",
      "Epoch: 02 [ 49984/244490 ( 20%)], Train Loss: 1.55769\n",
      "Epoch: 02 [ 50624/244490 ( 21%)], Train Loss: 1.55653\n",
      "Epoch: 02 [ 51264/244490 ( 21%)], Train Loss: 1.55581\n",
      "Epoch: 02 [ 51904/244490 ( 21%)], Train Loss: 1.55489\n",
      "Epoch: 02 [ 52544/244490 ( 21%)], Train Loss: 1.55367\n",
      "Epoch: 02 [ 53184/244490 ( 22%)], Train Loss: 1.55409\n",
      "Epoch: 02 [ 53824/244490 ( 22%)], Train Loss: 1.55345\n",
      "Epoch: 02 [ 54464/244490 ( 22%)], Train Loss: 1.55211\n",
      "Epoch: 02 [ 55104/244490 ( 23%)], Train Loss: 1.55237\n",
      "Epoch: 02 [ 55744/244490 ( 23%)], Train Loss: 1.55264\n",
      "Epoch: 02 [ 56384/244490 ( 23%)], Train Loss: 1.55082\n",
      "Epoch: 02 [ 57024/244490 ( 23%)], Train Loss: 1.55059\n",
      "Epoch: 02 [ 57664/244490 ( 24%)], Train Loss: 1.54973\n",
      "Epoch: 02 [ 58304/244490 ( 24%)], Train Loss: 1.54807\n",
      "Epoch: 02 [ 58944/244490 ( 24%)], Train Loss: 1.54623\n",
      "Epoch: 02 [ 59584/244490 ( 24%)], Train Loss: 1.54549\n",
      "Epoch: 02 [ 60224/244490 ( 25%)], Train Loss: 1.54463\n",
      "Epoch: 02 [ 60864/244490 ( 25%)], Train Loss: 1.54534\n",
      "Epoch: 02 [ 61504/244490 ( 25%)], Train Loss: 1.54535\n",
      "Epoch: 02 [ 62144/244490 ( 25%)], Train Loss: 1.54651\n",
      "Epoch: 02 [ 62784/244490 ( 26%)], Train Loss: 1.54477\n",
      "Epoch: 02 [ 63424/244490 ( 26%)], Train Loss: 1.54429\n",
      "Epoch: 02 [ 64064/244490 ( 26%)], Train Loss: 1.54396\n",
      "Epoch: 02 [ 64704/244490 ( 26%)], Train Loss: 1.54455\n",
      "Epoch: 02 [ 65344/244490 ( 27%)], Train Loss: 1.54337\n",
      "Epoch: 02 [ 65984/244490 ( 27%)], Train Loss: 1.54325\n",
      "Epoch: 02 [ 66624/244490 ( 27%)], Train Loss: 1.54395\n",
      "Epoch: 02 [ 67264/244490 ( 28%)], Train Loss: 1.54340\n",
      "Epoch: 02 [ 67904/244490 ( 28%)], Train Loss: 1.54340\n",
      "Epoch: 02 [ 68544/244490 ( 28%)], Train Loss: 1.54333\n",
      "Epoch: 02 [ 69184/244490 ( 28%)], Train Loss: 1.54194\n",
      "Epoch: 02 [ 69824/244490 ( 29%)], Train Loss: 1.54070\n",
      "Epoch: 02 [ 70464/244490 ( 29%)], Train Loss: 1.53984\n",
      "Epoch: 02 [ 71104/244490 ( 29%)], Train Loss: 1.53894\n",
      "Epoch: 02 [ 71744/244490 ( 29%)], Train Loss: 1.53727\n",
      "Epoch: 02 [ 72384/244490 ( 30%)], Train Loss: 1.53709\n",
      "Epoch: 02 [ 73024/244490 ( 30%)], Train Loss: 1.53597\n",
      "Epoch: 02 [ 73664/244490 ( 30%)], Train Loss: 1.53532\n",
      "Epoch: 02 [ 74304/244490 ( 30%)], Train Loss: 1.53598\n",
      "Epoch: 02 [ 74944/244490 ( 31%)], Train Loss: 1.53578\n",
      "Epoch: 02 [ 75584/244490 ( 31%)], Train Loss: 1.53475\n",
      "Epoch: 02 [ 76224/244490 ( 31%)], Train Loss: 1.53412\n",
      "Epoch: 02 [ 76864/244490 ( 31%)], Train Loss: 1.53224\n",
      "Epoch: 02 [ 77504/244490 ( 32%)], Train Loss: 1.53223\n",
      "Epoch: 02 [ 78144/244490 ( 32%)], Train Loss: 1.53258\n",
      "Epoch: 02 [ 78784/244490 ( 32%)], Train Loss: 1.53300\n",
      "Epoch: 02 [ 79424/244490 ( 32%)], Train Loss: 1.53362\n",
      "Epoch: 02 [ 80064/244490 ( 33%)], Train Loss: 1.53238\n",
      "Epoch: 02 [ 80704/244490 ( 33%)], Train Loss: 1.53268\n",
      "Epoch: 02 [ 81344/244490 ( 33%)], Train Loss: 1.53251\n",
      "Epoch: 02 [ 81984/244490 ( 34%)], Train Loss: 1.53220\n",
      "Epoch: 02 [ 82624/244490 ( 34%)], Train Loss: 1.53211\n",
      "Epoch: 02 [ 83264/244490 ( 34%)], Train Loss: 1.53200\n",
      "Epoch: 02 [ 83904/244490 ( 34%)], Train Loss: 1.53038\n",
      "Epoch: 02 [ 84544/244490 ( 35%)], Train Loss: 1.52866\n",
      "Epoch: 02 [ 85184/244490 ( 35%)], Train Loss: 1.52796\n",
      "Epoch: 02 [ 85824/244490 ( 35%)], Train Loss: 1.52628\n",
      "Epoch: 02 [ 86464/244490 ( 35%)], Train Loss: 1.52549\n",
      "Epoch: 02 [ 87104/244490 ( 36%)], Train Loss: 1.52494\n",
      "Epoch: 02 [ 87744/244490 ( 36%)], Train Loss: 1.52433\n",
      "Epoch: 02 [ 88384/244490 ( 36%)], Train Loss: 1.52407\n",
      "Epoch: 02 [ 89024/244490 ( 36%)], Train Loss: 1.52347\n",
      "Epoch: 02 [ 89664/244490 ( 37%)], Train Loss: 1.52267\n",
      "Epoch: 02 [ 90304/244490 ( 37%)], Train Loss: 1.52289\n",
      "Epoch: 02 [ 90944/244490 ( 37%)], Train Loss: 1.52246\n",
      "Epoch: 02 [ 91584/244490 ( 37%)], Train Loss: 1.52263\n",
      "Epoch: 02 [ 92224/244490 ( 38%)], Train Loss: 1.52330\n",
      "Epoch: 02 [ 92864/244490 ( 38%)], Train Loss: 1.52300\n",
      "Epoch: 02 [ 93504/244490 ( 38%)], Train Loss: 1.52192\n",
      "Epoch: 02 [ 94144/244490 ( 39%)], Train Loss: 1.52128\n",
      "Epoch: 02 [ 94784/244490 ( 39%)], Train Loss: 1.52035\n",
      "Epoch: 02 [ 95424/244490 ( 39%)], Train Loss: 1.51943\n",
      "Epoch: 02 [ 96064/244490 ( 39%)], Train Loss: 1.51865\n",
      "Epoch: 02 [ 96704/244490 ( 40%)], Train Loss: 1.51760\n",
      "Epoch: 02 [ 97344/244490 ( 40%)], Train Loss: 1.51697\n",
      "Epoch: 02 [ 97984/244490 ( 40%)], Train Loss: 1.51613\n",
      "Epoch: 02 [ 98624/244490 ( 40%)], Train Loss: 1.51498\n",
      "Epoch: 02 [ 99264/244490 ( 41%)], Train Loss: 1.51409\n",
      "Epoch: 02 [ 99904/244490 ( 41%)], Train Loss: 1.51411\n",
      "Epoch: 02 [100544/244490 ( 41%)], Train Loss: 1.51401\n",
      "Epoch: 02 [101184/244490 ( 41%)], Train Loss: 1.51438\n",
      "Epoch: 02 [101824/244490 ( 42%)], Train Loss: 1.51332\n",
      "Epoch: 02 [102464/244490 ( 42%)], Train Loss: 1.51246\n",
      "Epoch: 02 [103104/244490 ( 42%)], Train Loss: 1.51101\n",
      "Epoch: 02 [103744/244490 ( 42%)], Train Loss: 1.51034\n",
      "Epoch: 02 [104384/244490 ( 43%)], Train Loss: 1.51021\n",
      "Epoch: 02 [105024/244490 ( 43%)], Train Loss: 1.50988\n",
      "Epoch: 02 [105664/244490 ( 43%)], Train Loss: 1.51045\n",
      "Epoch: 02 [106304/244490 ( 43%)], Train Loss: 1.50998\n",
      "Epoch: 02 [106944/244490 ( 44%)], Train Loss: 1.50973\n",
      "Epoch: 02 [107584/244490 ( 44%)], Train Loss: 1.50926\n",
      "Epoch: 02 [108224/244490 ( 44%)], Train Loss: 1.50943\n",
      "Epoch: 02 [108864/244490 ( 45%)], Train Loss: 1.51011\n",
      "Epoch: 02 [109504/244490 ( 45%)], Train Loss: 1.51056\n",
      "Epoch: 02 [110144/244490 ( 45%)], Train Loss: 1.51015\n",
      "Epoch: 02 [110784/244490 ( 45%)], Train Loss: 1.50925\n",
      "Epoch: 02 [111424/244490 ( 46%)], Train Loss: 1.50805\n",
      "Epoch: 02 [112064/244490 ( 46%)], Train Loss: 1.50710\n",
      "Epoch: 02 [112704/244490 ( 46%)], Train Loss: 1.50618\n",
      "Epoch: 02 [113344/244490 ( 46%)], Train Loss: 1.50628\n",
      "Epoch: 02 [113984/244490 ( 47%)], Train Loss: 1.50591\n",
      "Epoch: 02 [114624/244490 ( 47%)], Train Loss: 1.50601\n",
      "Epoch: 02 [115264/244490 ( 47%)], Train Loss: 1.50572\n",
      "Epoch: 02 [115904/244490 ( 47%)], Train Loss: 1.50501\n",
      "Epoch: 02 [116544/244490 ( 48%)], Train Loss: 1.50495\n",
      "Epoch: 02 [117184/244490 ( 48%)], Train Loss: 1.50414\n",
      "Epoch: 02 [117824/244490 ( 48%)], Train Loss: 1.50362\n",
      "Epoch: 02 [118464/244490 ( 48%)], Train Loss: 1.50269\n",
      "Epoch: 02 [119104/244490 ( 49%)], Train Loss: 1.50252\n",
      "Epoch: 02 [119744/244490 ( 49%)], Train Loss: 1.50152\n",
      "Epoch: 02 [120384/244490 ( 49%)], Train Loss: 1.50047\n",
      "Epoch: 02 [121024/244490 ( 50%)], Train Loss: 1.50023\n",
      "Epoch: 02 [121664/244490 ( 50%)], Train Loss: 1.49932\n",
      "Epoch: 02 [122304/244490 ( 50%)], Train Loss: 1.49920\n",
      "Epoch: 02 [122944/244490 ( 50%)], Train Loss: 1.49886\n",
      "Epoch: 02 [123584/244490 ( 51%)], Train Loss: 1.49802\n",
      "Epoch: 02 [124224/244490 ( 51%)], Train Loss: 1.49770\n",
      "Epoch: 02 [124864/244490 ( 51%)], Train Loss: 1.49820\n",
      "Epoch: 02 [125504/244490 ( 51%)], Train Loss: 1.49818\n",
      "Epoch: 02 [126144/244490 ( 52%)], Train Loss: 1.49813\n",
      "Epoch: 02 [126784/244490 ( 52%)], Train Loss: 1.49780\n",
      "Epoch: 02 [127424/244490 ( 52%)], Train Loss: 1.49766\n",
      "Epoch: 02 [128064/244490 ( 52%)], Train Loss: 1.49794\n",
      "Epoch: 02 [128704/244490 ( 53%)], Train Loss: 1.49746\n",
      "Epoch: 02 [129344/244490 ( 53%)], Train Loss: 1.49790\n",
      "Epoch: 02 [129984/244490 ( 53%)], Train Loss: 1.49715\n",
      "Epoch: 02 [130624/244490 ( 53%)], Train Loss: 1.49681\n",
      "Epoch: 02 [131264/244490 ( 54%)], Train Loss: 1.49644\n",
      "Epoch: 02 [131904/244490 ( 54%)], Train Loss: 1.49625\n",
      "Epoch: 02 [132544/244490 ( 54%)], Train Loss: 1.49551\n",
      "Epoch: 02 [133184/244490 ( 54%)], Train Loss: 1.49481\n",
      "Epoch: 02 [133824/244490 ( 55%)], Train Loss: 1.49421\n",
      "Epoch: 02 [134464/244490 ( 55%)], Train Loss: 1.49358\n",
      "Epoch: 02 [135104/244490 ( 55%)], Train Loss: 1.49337\n",
      "Epoch: 02 [135744/244490 ( 56%)], Train Loss: 1.49337\n",
      "Epoch: 02 [136384/244490 ( 56%)], Train Loss: 1.49363\n",
      "Epoch: 02 [137024/244490 ( 56%)], Train Loss: 1.49319\n",
      "Epoch: 02 [137664/244490 ( 56%)], Train Loss: 1.49306\n",
      "Epoch: 02 [138304/244490 ( 57%)], Train Loss: 1.49265\n",
      "Epoch: 02 [138944/244490 ( 57%)], Train Loss: 1.49174\n",
      "Epoch: 02 [139584/244490 ( 57%)], Train Loss: 1.49095\n",
      "Epoch: 02 [140224/244490 ( 57%)], Train Loss: 1.49029\n",
      "Epoch: 02 [140864/244490 ( 58%)], Train Loss: 1.48990\n",
      "Epoch: 02 [141504/244490 ( 58%)], Train Loss: 1.48918\n",
      "Epoch: 02 [142144/244490 ( 58%)], Train Loss: 1.48949\n",
      "Epoch: 02 [142784/244490 ( 58%)], Train Loss: 1.48962\n",
      "Epoch: 02 [143424/244490 ( 59%)], Train Loss: 1.48935\n",
      "Epoch: 02 [144064/244490 ( 59%)], Train Loss: 1.48920\n",
      "Epoch: 02 [144704/244490 ( 59%)], Train Loss: 1.48897\n",
      "Epoch: 02 [145344/244490 ( 59%)], Train Loss: 1.48805\n",
      "Epoch: 02 [145984/244490 ( 60%)], Train Loss: 1.48810\n",
      "Epoch: 02 [146624/244490 ( 60%)], Train Loss: 1.48792\n",
      "Epoch: 02 [147264/244490 ( 60%)], Train Loss: 1.48787\n",
      "Epoch: 02 [147904/244490 ( 60%)], Train Loss: 1.48787\n",
      "Epoch: 02 [148544/244490 ( 61%)], Train Loss: 1.48804\n",
      "Epoch: 02 [149184/244490 ( 61%)], Train Loss: 1.48814\n",
      "Epoch: 02 [149824/244490 ( 61%)], Train Loss: 1.48770\n",
      "Epoch: 02 [150464/244490 ( 62%)], Train Loss: 1.48682\n",
      "Epoch: 02 [151104/244490 ( 62%)], Train Loss: 1.48642\n",
      "Epoch: 02 [151744/244490 ( 62%)], Train Loss: 1.48607\n",
      "Epoch: 02 [152384/244490 ( 62%)], Train Loss: 1.48574\n",
      "Epoch: 02 [153024/244490 ( 63%)], Train Loss: 1.48506\n",
      "Epoch: 02 [153664/244490 ( 63%)], Train Loss: 1.48468\n",
      "Epoch: 02 [154304/244490 ( 63%)], Train Loss: 1.48432\n",
      "Epoch: 02 [154944/244490 ( 63%)], Train Loss: 1.48427\n",
      "Epoch: 02 [155584/244490 ( 64%)], Train Loss: 1.48377\n",
      "Epoch: 02 [156224/244490 ( 64%)], Train Loss: 1.48395\n",
      "Epoch: 02 [156864/244490 ( 64%)], Train Loss: 1.48361\n",
      "Epoch: 02 [157504/244490 ( 64%)], Train Loss: 1.48316\n",
      "Epoch: 02 [158144/244490 ( 65%)], Train Loss: 1.48295\n",
      "Epoch: 02 [158784/244490 ( 65%)], Train Loss: 1.48266\n",
      "Epoch: 02 [159424/244490 ( 65%)], Train Loss: 1.48291\n",
      "Epoch: 02 [160064/244490 ( 65%)], Train Loss: 1.48225\n",
      "Epoch: 02 [160704/244490 ( 66%)], Train Loss: 1.48173\n",
      "Epoch: 02 [161344/244490 ( 66%)], Train Loss: 1.48161\n",
      "Epoch: 02 [161984/244490 ( 66%)], Train Loss: 1.48127\n",
      "Epoch: 02 [162624/244490 ( 67%)], Train Loss: 1.48059\n",
      "Epoch: 02 [163264/244490 ( 67%)], Train Loss: 1.48004\n",
      "Epoch: 02 [163904/244490 ( 67%)], Train Loss: 1.47918\n",
      "Epoch: 02 [164544/244490 ( 67%)], Train Loss: 1.47842\n",
      "Epoch: 02 [165184/244490 ( 68%)], Train Loss: 1.47791\n",
      "Epoch: 02 [165824/244490 ( 68%)], Train Loss: 1.47745\n",
      "Epoch: 02 [166464/244490 ( 68%)], Train Loss: 1.47748\n",
      "Epoch: 02 [167104/244490 ( 68%)], Train Loss: 1.47709\n",
      "Epoch: 02 [167744/244490 ( 69%)], Train Loss: 1.47681\n",
      "Epoch: 02 [168384/244490 ( 69%)], Train Loss: 1.47618\n",
      "Epoch: 02 [169024/244490 ( 69%)], Train Loss: 1.47568\n",
      "Epoch: 02 [169664/244490 ( 69%)], Train Loss: 1.47525\n",
      "Epoch: 02 [170304/244490 ( 70%)], Train Loss: 1.47441\n",
      "Epoch: 02 [170944/244490 ( 70%)], Train Loss: 1.47367\n",
      "Epoch: 02 [171584/244490 ( 70%)], Train Loss: 1.47341\n",
      "Epoch: 02 [172224/244490 ( 70%)], Train Loss: 1.47344\n",
      "Epoch: 02 [172864/244490 ( 71%)], Train Loss: 1.47298\n",
      "Epoch: 02 [173504/244490 ( 71%)], Train Loss: 1.47297\n",
      "Epoch: 02 [174144/244490 ( 71%)], Train Loss: 1.47293\n",
      "Epoch: 02 [174784/244490 ( 71%)], Train Loss: 1.47283\n",
      "Epoch: 02 [175424/244490 ( 72%)], Train Loss: 1.47208\n",
      "Epoch: 02 [176064/244490 ( 72%)], Train Loss: 1.47177\n",
      "Epoch: 02 [176704/244490 ( 72%)], Train Loss: 1.47183\n",
      "Epoch: 02 [177344/244490 ( 73%)], Train Loss: 1.47121\n",
      "Epoch: 02 [177984/244490 ( 73%)], Train Loss: 1.47089\n",
      "Epoch: 02 [178624/244490 ( 73%)], Train Loss: 1.47050\n",
      "Epoch: 02 [179264/244490 ( 73%)], Train Loss: 1.47000\n",
      "Epoch: 02 [179904/244490 ( 74%)], Train Loss: 1.46997\n",
      "Epoch: 02 [180544/244490 ( 74%)], Train Loss: 1.46943\n",
      "Epoch: 02 [181184/244490 ( 74%)], Train Loss: 1.46896\n",
      "Epoch: 02 [181824/244490 ( 74%)], Train Loss: 1.46821\n",
      "Epoch: 02 [182464/244490 ( 75%)], Train Loss: 1.46765\n",
      "Epoch: 02 [183104/244490 ( 75%)], Train Loss: 1.46745\n",
      "Epoch: 02 [183744/244490 ( 75%)], Train Loss: 1.46708\n",
      "Epoch: 02 [184384/244490 ( 75%)], Train Loss: 1.46639\n",
      "Epoch: 02 [185024/244490 ( 76%)], Train Loss: 1.46581\n",
      "Epoch: 02 [185664/244490 ( 76%)], Train Loss: 1.46541\n",
      "Epoch: 02 [186304/244490 ( 76%)], Train Loss: 1.46503\n",
      "Epoch: 02 [186944/244490 ( 76%)], Train Loss: 1.46456\n",
      "Epoch: 02 [187584/244490 ( 77%)], Train Loss: 1.46400\n",
      "Epoch: 02 [188224/244490 ( 77%)], Train Loss: 1.46340\n",
      "Epoch: 02 [188864/244490 ( 77%)], Train Loss: 1.46268\n",
      "Epoch: 02 [189504/244490 ( 78%)], Train Loss: 1.46245\n",
      "Epoch: 02 [190144/244490 ( 78%)], Train Loss: 1.46173\n",
      "Epoch: 02 [190784/244490 ( 78%)], Train Loss: 1.46115\n",
      "Epoch: 02 [191424/244490 ( 78%)], Train Loss: 1.46105\n",
      "Epoch: 02 [192064/244490 ( 79%)], Train Loss: 1.46070\n",
      "Epoch: 02 [192704/244490 ( 79%)], Train Loss: 1.46021\n",
      "Epoch: 02 [193344/244490 ( 79%)], Train Loss: 1.45986\n",
      "Epoch: 02 [193984/244490 ( 79%)], Train Loss: 1.45959\n",
      "Epoch: 02 [194624/244490 ( 80%)], Train Loss: 1.45891\n",
      "Epoch: 02 [195264/244490 ( 80%)], Train Loss: 1.45870\n",
      "Epoch: 02 [195904/244490 ( 80%)], Train Loss: 1.45802\n",
      "Epoch: 02 [196544/244490 ( 80%)], Train Loss: 1.45759\n",
      "Epoch: 02 [197184/244490 ( 81%)], Train Loss: 1.45686\n",
      "Epoch: 02 [197824/244490 ( 81%)], Train Loss: 1.45670\n",
      "Epoch: 02 [198464/244490 ( 81%)], Train Loss: 1.45651\n",
      "Epoch: 02 [199104/244490 ( 81%)], Train Loss: 1.45629\n",
      "Epoch: 02 [199744/244490 ( 82%)], Train Loss: 1.45636\n",
      "Epoch: 02 [200384/244490 ( 82%)], Train Loss: 1.45599\n",
      "Epoch: 02 [201024/244490 ( 82%)], Train Loss: 1.45534\n",
      "Epoch: 02 [201664/244490 ( 82%)], Train Loss: 1.45512\n",
      "Epoch: 02 [202304/244490 ( 83%)], Train Loss: 1.45508\n",
      "Epoch: 02 [202944/244490 ( 83%)], Train Loss: 1.45437\n",
      "Epoch: 02 [203584/244490 ( 83%)], Train Loss: 1.45345\n",
      "Epoch: 02 [204224/244490 ( 84%)], Train Loss: 1.45323\n",
      "Epoch: 02 [204864/244490 ( 84%)], Train Loss: 1.45263\n",
      "Epoch: 02 [205504/244490 ( 84%)], Train Loss: 1.45233\n",
      "Epoch: 02 [206144/244490 ( 84%)], Train Loss: 1.45185\n",
      "Epoch: 02 [206784/244490 ( 85%)], Train Loss: 1.45113\n",
      "Epoch: 02 [207424/244490 ( 85%)], Train Loss: 1.45085\n",
      "Epoch: 02 [208064/244490 ( 85%)], Train Loss: 1.45003\n",
      "Epoch: 02 [208704/244490 ( 85%)], Train Loss: 1.45017\n",
      "Epoch: 02 [209344/244490 ( 86%)], Train Loss: 1.44973\n",
      "Epoch: 02 [209984/244490 ( 86%)], Train Loss: 1.44947\n",
      "Epoch: 02 [210624/244490 ( 86%)], Train Loss: 1.44913\n",
      "Epoch: 02 [211264/244490 ( 86%)], Train Loss: 1.44905\n",
      "Epoch: 02 [211904/244490 ( 87%)], Train Loss: 1.44885\n",
      "Epoch: 02 [212544/244490 ( 87%)], Train Loss: 1.44861\n",
      "Epoch: 02 [213184/244490 ( 87%)], Train Loss: 1.44821\n",
      "Epoch: 02 [213824/244490 ( 87%)], Train Loss: 1.44769\n",
      "Epoch: 02 [214464/244490 ( 88%)], Train Loss: 1.44751\n",
      "Epoch: 02 [215104/244490 ( 88%)], Train Loss: 1.44719\n",
      "Epoch: 02 [215744/244490 ( 88%)], Train Loss: 1.44654\n",
      "Epoch: 02 [216384/244490 ( 89%)], Train Loss: 1.44573\n",
      "Epoch: 02 [217024/244490 ( 89%)], Train Loss: 1.44520\n",
      "Epoch: 02 [217664/244490 ( 89%)], Train Loss: 1.44480\n",
      "Epoch: 02 [218304/244490 ( 89%)], Train Loss: 1.44430\n",
      "Epoch: 02 [218944/244490 ( 90%)], Train Loss: 1.44417\n",
      "Epoch: 02 [219584/244490 ( 90%)], Train Loss: 1.44381\n",
      "Epoch: 02 [220224/244490 ( 90%)], Train Loss: 1.44323\n",
      "Epoch: 02 [220864/244490 ( 90%)], Train Loss: 1.44270\n",
      "Epoch: 02 [221504/244490 ( 91%)], Train Loss: 1.44247\n",
      "Epoch: 02 [222144/244490 ( 91%)], Train Loss: 1.44230\n",
      "Epoch: 02 [222784/244490 ( 91%)], Train Loss: 1.44199\n",
      "Epoch: 02 [223424/244490 ( 91%)], Train Loss: 1.44158\n",
      "Epoch: 02 [224064/244490 ( 92%)], Train Loss: 1.44080\n",
      "Epoch: 02 [224704/244490 ( 92%)], Train Loss: 1.44043\n",
      "Epoch: 02 [225344/244490 ( 92%)], Train Loss: 1.44001\n",
      "Epoch: 02 [225984/244490 ( 92%)], Train Loss: 1.43966\n",
      "Epoch: 02 [226624/244490 ( 93%)], Train Loss: 1.43933\n",
      "Epoch: 02 [227264/244490 ( 93%)], Train Loss: 1.43879\n",
      "Epoch: 02 [227904/244490 ( 93%)], Train Loss: 1.43830\n",
      "Epoch: 02 [228544/244490 ( 93%)], Train Loss: 1.43838\n",
      "Epoch: 02 [229184/244490 ( 94%)], Train Loss: 1.43795\n",
      "Epoch: 02 [229824/244490 ( 94%)], Train Loss: 1.43812\n",
      "Epoch: 02 [230464/244490 ( 94%)], Train Loss: 1.43791\n",
      "Epoch: 02 [231104/244490 ( 95%)], Train Loss: 1.43770\n",
      "Epoch: 02 [231744/244490 ( 95%)], Train Loss: 1.43732\n",
      "Epoch: 02 [232384/244490 ( 95%)], Train Loss: 1.43708\n",
      "Epoch: 02 [233024/244490 ( 95%)], Train Loss: 1.43672\n",
      "Epoch: 02 [233664/244490 ( 96%)], Train Loss: 1.43608\n",
      "Epoch: 02 [234304/244490 ( 96%)], Train Loss: 1.43589\n",
      "Epoch: 02 [234944/244490 ( 96%)], Train Loss: 1.43578\n",
      "Epoch: 02 [235584/244490 ( 96%)], Train Loss: 1.43537\n",
      "Epoch: 02 [236224/244490 ( 97%)], Train Loss: 1.43493\n",
      "Epoch: 02 [236864/244490 ( 97%)], Train Loss: 1.43453\n",
      "Epoch: 02 [237504/244490 ( 97%)], Train Loss: 1.43446\n",
      "Epoch: 02 [238144/244490 ( 97%)], Train Loss: 1.43391\n",
      "Epoch: 02 [238784/244490 ( 98%)], Train Loss: 1.43372\n",
      "Epoch: 02 [239424/244490 ( 98%)], Train Loss: 1.43360\n",
      "Epoch: 02 [240064/244490 ( 98%)], Train Loss: 1.43349\n",
      "Epoch: 02 [240704/244490 ( 98%)], Train Loss: 1.43319\n",
      "Epoch: 02 [241344/244490 ( 99%)], Train Loss: 1.43303\n",
      "Epoch: 02 [241984/244490 ( 99%)], Train Loss: 1.43249\n",
      "Epoch: 02 [242624/244490 ( 99%)], Train Loss: 1.43209\n",
      "Epoch: 02 [243264/244490 ( 99%)], Train Loss: 1.43178\n",
      "Epoch: 02 [243904/244490 (100%)], Train Loss: 1.43175\n",
      "Epoch: 02 [244490/244490 (100%)], Train Loss: 1.43148\n",
      "----Validation Results Summary----\n",
      "Epoch: [2] Valid Loss: 1.76508\n",
      "2 Epoch, Best epoch was updated! Valid Loss: 1.76508\n",
      "Saving model checkpoint to output/checkpoint-fold-0.\n",
      "\n",
      "Epoch: 03 [    64/244490 (  0%)], Train Loss: 1.53063\n",
      "Epoch: 03 [   704/244490 (  0%)], Train Loss: 1.33446\n",
      "Epoch: 03 [  1344/244490 (  1%)], Train Loss: 1.32490\n",
      "Epoch: 03 [  1984/244490 (  1%)], Train Loss: 1.30681\n",
      "Epoch: 03 [  2624/244490 (  1%)], Train Loss: 1.29409\n",
      "Epoch: 03 [  3264/244490 (  1%)], Train Loss: 1.25705\n",
      "Epoch: 03 [  3904/244490 (  2%)], Train Loss: 1.26643\n",
      "Epoch: 03 [  4544/244490 (  2%)], Train Loss: 1.27531\n",
      "Epoch: 03 [  5184/244490 (  2%)], Train Loss: 1.28871\n",
      "Epoch: 03 [  5824/244490 (  2%)], Train Loss: 1.29547\n",
      "Epoch: 03 [  6464/244490 (  3%)], Train Loss: 1.29457\n",
      "Epoch: 03 [  7104/244490 (  3%)], Train Loss: 1.29692\n",
      "Epoch: 03 [  7744/244490 (  3%)], Train Loss: 1.29456\n",
      "Epoch: 03 [  8384/244490 (  3%)], Train Loss: 1.28530\n",
      "Epoch: 03 [  9024/244490 (  4%)], Train Loss: 1.28820\n",
      "Epoch: 03 [  9664/244490 (  4%)], Train Loss: 1.28298\n",
      "Epoch: 03 [ 10304/244490 (  4%)], Train Loss: 1.28186\n",
      "Epoch: 03 [ 10944/244490 (  4%)], Train Loss: 1.27526\n",
      "Epoch: 03 [ 11584/244490 (  5%)], Train Loss: 1.27979\n",
      "Epoch: 03 [ 12224/244490 (  5%)], Train Loss: 1.28442\n",
      "Epoch: 03 [ 12864/244490 (  5%)], Train Loss: 1.28898\n",
      "Epoch: 03 [ 13504/244490 (  6%)], Train Loss: 1.28801\n",
      "Epoch: 03 [ 14144/244490 (  6%)], Train Loss: 1.28541\n",
      "Epoch: 03 [ 14784/244490 (  6%)], Train Loss: 1.28310\n",
      "Epoch: 03 [ 15424/244490 (  6%)], Train Loss: 1.28519\n",
      "Epoch: 03 [ 16064/244490 (  7%)], Train Loss: 1.28791\n",
      "Epoch: 03 [ 16704/244490 (  7%)], Train Loss: 1.29052\n",
      "Epoch: 03 [ 17344/244490 (  7%)], Train Loss: 1.28408\n",
      "Epoch: 03 [ 17984/244490 (  7%)], Train Loss: 1.28071\n",
      "Epoch: 03 [ 18624/244490 (  8%)], Train Loss: 1.27616\n",
      "Epoch: 03 [ 19264/244490 (  8%)], Train Loss: 1.27626\n",
      "Epoch: 03 [ 19904/244490 (  8%)], Train Loss: 1.27586\n",
      "Epoch: 03 [ 20544/244490 (  8%)], Train Loss: 1.27292\n",
      "Epoch: 03 [ 21184/244490 (  9%)], Train Loss: 1.27301\n",
      "Epoch: 03 [ 21824/244490 (  9%)], Train Loss: 1.27407\n",
      "Epoch: 03 [ 22464/244490 (  9%)], Train Loss: 1.27187\n",
      "Epoch: 03 [ 23104/244490 (  9%)], Train Loss: 1.27145\n",
      "Epoch: 03 [ 23744/244490 ( 10%)], Train Loss: 1.27225\n",
      "Epoch: 03 [ 24384/244490 ( 10%)], Train Loss: 1.27560\n",
      "Epoch: 03 [ 25024/244490 ( 10%)], Train Loss: 1.27424\n",
      "Epoch: 03 [ 25664/244490 ( 10%)], Train Loss: 1.27401\n",
      "Epoch: 03 [ 26304/244490 ( 11%)], Train Loss: 1.27124\n",
      "Epoch: 03 [ 26944/244490 ( 11%)], Train Loss: 1.26874\n",
      "Epoch: 03 [ 27584/244490 ( 11%)], Train Loss: 1.26587\n",
      "Epoch: 03 [ 28224/244490 ( 12%)], Train Loss: 1.26465\n",
      "Epoch: 03 [ 28864/244490 ( 12%)], Train Loss: 1.26472\n",
      "Epoch: 03 [ 29504/244490 ( 12%)], Train Loss: 1.26392\n",
      "Epoch: 03 [ 30144/244490 ( 12%)], Train Loss: 1.26378\n",
      "Epoch: 03 [ 30784/244490 ( 13%)], Train Loss: 1.26214\n",
      "Epoch: 03 [ 31424/244490 ( 13%)], Train Loss: 1.25958\n",
      "Epoch: 03 [ 32064/244490 ( 13%)], Train Loss: 1.25880\n",
      "Epoch: 03 [ 32704/244490 ( 13%)], Train Loss: 1.25913\n",
      "Epoch: 03 [ 33344/244490 ( 14%)], Train Loss: 1.25963\n",
      "Epoch: 03 [ 33984/244490 ( 14%)], Train Loss: 1.25954\n",
      "Epoch: 03 [ 34624/244490 ( 14%)], Train Loss: 1.26208\n",
      "Epoch: 03 [ 35264/244490 ( 14%)], Train Loss: 1.26212\n",
      "Epoch: 03 [ 35904/244490 ( 15%)], Train Loss: 1.26174\n",
      "Epoch: 03 [ 36544/244490 ( 15%)], Train Loss: 1.26029\n",
      "Epoch: 03 [ 37184/244490 ( 15%)], Train Loss: 1.25710\n",
      "Epoch: 03 [ 37824/244490 ( 15%)], Train Loss: 1.25454\n",
      "Epoch: 03 [ 38464/244490 ( 16%)], Train Loss: 1.25263\n",
      "Epoch: 03 [ 39104/244490 ( 16%)], Train Loss: 1.25052\n",
      "Epoch: 03 [ 39744/244490 ( 16%)], Train Loss: 1.24931\n",
      "Epoch: 03 [ 40384/244490 ( 17%)], Train Loss: 1.24825\n",
      "Epoch: 03 [ 41024/244490 ( 17%)], Train Loss: 1.24605\n",
      "Epoch: 03 [ 41664/244490 ( 17%)], Train Loss: 1.24573\n",
      "Epoch: 03 [ 42304/244490 ( 17%)], Train Loss: 1.24514\n",
      "Epoch: 03 [ 42944/244490 ( 18%)], Train Loss: 1.24436\n",
      "Epoch: 03 [ 43584/244490 ( 18%)], Train Loss: 1.24237\n",
      "Epoch: 03 [ 44224/244490 ( 18%)], Train Loss: 1.24171\n",
      "Epoch: 03 [ 44864/244490 ( 18%)], Train Loss: 1.24020\n",
      "Epoch: 03 [ 45504/244490 ( 19%)], Train Loss: 1.24063\n",
      "Epoch: 03 [ 46144/244490 ( 19%)], Train Loss: 1.23932\n",
      "Epoch: 03 [ 46784/244490 ( 19%)], Train Loss: 1.23987\n",
      "Epoch: 03 [ 47424/244490 ( 19%)], Train Loss: 1.24079\n",
      "Epoch: 03 [ 48064/244490 ( 20%)], Train Loss: 1.23980\n",
      "Epoch: 03 [ 48704/244490 ( 20%)], Train Loss: 1.24098\n",
      "Epoch: 03 [ 49344/244490 ( 20%)], Train Loss: 1.24124\n",
      "Epoch: 03 [ 49984/244490 ( 20%)], Train Loss: 1.24139\n",
      "Epoch: 03 [ 50624/244490 ( 21%)], Train Loss: 1.24146\n",
      "Epoch: 03 [ 51264/244490 ( 21%)], Train Loss: 1.24111\n",
      "Epoch: 03 [ 51904/244490 ( 21%)], Train Loss: 1.24084\n",
      "Epoch: 03 [ 52544/244490 ( 21%)], Train Loss: 1.23980\n",
      "Epoch: 03 [ 53184/244490 ( 22%)], Train Loss: 1.24066\n",
      "Epoch: 03 [ 53824/244490 ( 22%)], Train Loss: 1.24005\n",
      "Epoch: 03 [ 54464/244490 ( 22%)], Train Loss: 1.23967\n",
      "Epoch: 03 [ 55104/244490 ( 23%)], Train Loss: 1.23977\n",
      "Epoch: 03 [ 55744/244490 ( 23%)], Train Loss: 1.24023\n",
      "Epoch: 03 [ 56384/244490 ( 23%)], Train Loss: 1.23937\n",
      "Epoch: 03 [ 57024/244490 ( 23%)], Train Loss: 1.23985\n",
      "Epoch: 03 [ 57664/244490 ( 24%)], Train Loss: 1.23929\n",
      "Epoch: 03 [ 58304/244490 ( 24%)], Train Loss: 1.23966\n",
      "Epoch: 03 [ 58944/244490 ( 24%)], Train Loss: 1.23910\n",
      "Epoch: 03 [ 59584/244490 ( 24%)], Train Loss: 1.23940\n",
      "Epoch: 03 [ 60224/244490 ( 25%)], Train Loss: 1.23798\n",
      "Epoch: 03 [ 60864/244490 ( 25%)], Train Loss: 1.23869\n",
      "Epoch: 03 [ 61504/244490 ( 25%)], Train Loss: 1.23978\n",
      "Epoch: 03 [ 62144/244490 ( 25%)], Train Loss: 1.24090\n",
      "Epoch: 03 [ 62784/244490 ( 26%)], Train Loss: 1.24035\n",
      "Epoch: 03 [ 63424/244490 ( 26%)], Train Loss: 1.24000\n",
      "Epoch: 03 [ 64064/244490 ( 26%)], Train Loss: 1.24045\n",
      "Epoch: 03 [ 64704/244490 ( 26%)], Train Loss: 1.24133\n",
      "Epoch: 03 [ 65344/244490 ( 27%)], Train Loss: 1.24104\n",
      "Epoch: 03 [ 65984/244490 ( 27%)], Train Loss: 1.24051\n",
      "Epoch: 03 [ 66624/244490 ( 27%)], Train Loss: 1.24115\n",
      "Epoch: 03 [ 67264/244490 ( 28%)], Train Loss: 1.24054\n",
      "Epoch: 03 [ 67904/244490 ( 28%)], Train Loss: 1.24009\n",
      "Epoch: 03 [ 68544/244490 ( 28%)], Train Loss: 1.24053\n",
      "Epoch: 03 [ 69184/244490 ( 28%)], Train Loss: 1.23982\n",
      "Epoch: 03 [ 69824/244490 ( 29%)], Train Loss: 1.23872\n",
      "Epoch: 03 [ 70464/244490 ( 29%)], Train Loss: 1.23798\n",
      "Epoch: 03 [ 71104/244490 ( 29%)], Train Loss: 1.23681\n",
      "Epoch: 03 [ 71744/244490 ( 29%)], Train Loss: 1.23507\n",
      "Epoch: 03 [ 72384/244490 ( 30%)], Train Loss: 1.23531\n",
      "Epoch: 03 [ 73024/244490 ( 30%)], Train Loss: 1.23583\n",
      "Epoch: 03 [ 73664/244490 ( 30%)], Train Loss: 1.23667\n",
      "Epoch: 03 [ 74304/244490 ( 30%)], Train Loss: 1.23765\n",
      "Epoch: 03 [ 74944/244490 ( 31%)], Train Loss: 1.23718\n",
      "Epoch: 03 [ 75584/244490 ( 31%)], Train Loss: 1.23681\n",
      "Epoch: 03 [ 76224/244490 ( 31%)], Train Loss: 1.23685\n",
      "Epoch: 03 [ 76864/244490 ( 31%)], Train Loss: 1.23608\n",
      "Epoch: 03 [ 77504/244490 ( 32%)], Train Loss: 1.23630\n",
      "Epoch: 03 [ 78144/244490 ( 32%)], Train Loss: 1.23692\n",
      "Epoch: 03 [ 78784/244490 ( 32%)], Train Loss: 1.23739\n",
      "Epoch: 03 [ 79424/244490 ( 32%)], Train Loss: 1.23767\n",
      "Epoch: 03 [ 80064/244490 ( 33%)], Train Loss: 1.23663\n",
      "Epoch: 03 [ 80704/244490 ( 33%)], Train Loss: 1.23690\n",
      "Epoch: 03 [ 81344/244490 ( 33%)], Train Loss: 1.23747\n",
      "Epoch: 03 [ 81984/244490 ( 34%)], Train Loss: 1.23789\n",
      "Epoch: 03 [ 82624/244490 ( 34%)], Train Loss: 1.23830\n",
      "Epoch: 03 [ 83264/244490 ( 34%)], Train Loss: 1.23818\n",
      "Epoch: 03 [ 83904/244490 ( 34%)], Train Loss: 1.23737\n",
      "Epoch: 03 [ 84544/244490 ( 35%)], Train Loss: 1.23603\n",
      "Epoch: 03 [ 85184/244490 ( 35%)], Train Loss: 1.23574\n",
      "Epoch: 03 [ 85824/244490 ( 35%)], Train Loss: 1.23478\n",
      "Epoch: 03 [ 86464/244490 ( 35%)], Train Loss: 1.23431\n",
      "Epoch: 03 [ 87104/244490 ( 36%)], Train Loss: 1.23495\n",
      "Epoch: 03 [ 87744/244490 ( 36%)], Train Loss: 1.23433\n",
      "Epoch: 03 [ 88384/244490 ( 36%)], Train Loss: 1.23388\n",
      "Epoch: 03 [ 89024/244490 ( 36%)], Train Loss: 1.23329\n",
      "Epoch: 03 [ 89664/244490 ( 37%)], Train Loss: 1.23273\n",
      "Epoch: 03 [ 90304/244490 ( 37%)], Train Loss: 1.23271\n",
      "Epoch: 03 [ 90944/244490 ( 37%)], Train Loss: 1.23286\n",
      "Epoch: 03 [ 91584/244490 ( 37%)], Train Loss: 1.23338\n",
      "Epoch: 03 [ 92224/244490 ( 38%)], Train Loss: 1.23422\n",
      "Epoch: 03 [ 92864/244490 ( 38%)], Train Loss: 1.23420\n",
      "Epoch: 03 [ 93504/244490 ( 38%)], Train Loss: 1.23312\n",
      "Epoch: 03 [ 94144/244490 ( 39%)], Train Loss: 1.23289\n",
      "Epoch: 03 [ 94784/244490 ( 39%)], Train Loss: 1.23266\n",
      "Epoch: 03 [ 95424/244490 ( 39%)], Train Loss: 1.23189\n",
      "Epoch: 03 [ 96064/244490 ( 39%)], Train Loss: 1.23149\n",
      "Epoch: 03 [ 96704/244490 ( 40%)], Train Loss: 1.23059\n",
      "Epoch: 03 [ 97344/244490 ( 40%)], Train Loss: 1.23011\n",
      "Epoch: 03 [ 97984/244490 ( 40%)], Train Loss: 1.22941\n",
      "Epoch: 03 [ 98624/244490 ( 40%)], Train Loss: 1.22886\n",
      "Epoch: 03 [ 99264/244490 ( 41%)], Train Loss: 1.22853\n",
      "Epoch: 03 [ 99904/244490 ( 41%)], Train Loss: 1.22940\n",
      "Epoch: 03 [100544/244490 ( 41%)], Train Loss: 1.22981\n",
      "Epoch: 03 [101184/244490 ( 41%)], Train Loss: 1.23099\n",
      "Epoch: 03 [101824/244490 ( 42%)], Train Loss: 1.23056\n",
      "Epoch: 03 [102464/244490 ( 42%)], Train Loss: 1.22995\n",
      "Epoch: 03 [103104/244490 ( 42%)], Train Loss: 1.22887\n",
      "Epoch: 03 [103744/244490 ( 42%)], Train Loss: 1.22826\n",
      "Epoch: 03 [104384/244490 ( 43%)], Train Loss: 1.22785\n",
      "Epoch: 03 [105024/244490 ( 43%)], Train Loss: 1.22736\n",
      "Epoch: 03 [105664/244490 ( 43%)], Train Loss: 1.22804\n",
      "Epoch: 03 [106304/244490 ( 43%)], Train Loss: 1.22814\n",
      "Epoch: 03 [106944/244490 ( 44%)], Train Loss: 1.22760\n",
      "Epoch: 03 [107584/244490 ( 44%)], Train Loss: 1.22738\n",
      "Epoch: 03 [108224/244490 ( 44%)], Train Loss: 1.22794\n",
      "Epoch: 03 [108864/244490 ( 45%)], Train Loss: 1.22851\n",
      "Epoch: 03 [109504/244490 ( 45%)], Train Loss: 1.22936\n",
      "Epoch: 03 [110144/244490 ( 45%)], Train Loss: 1.22931\n",
      "Epoch: 03 [110784/244490 ( 45%)], Train Loss: 1.22915\n",
      "Epoch: 03 [111424/244490 ( 46%)], Train Loss: 1.22866\n",
      "Epoch: 03 [112064/244490 ( 46%)], Train Loss: 1.22843\n",
      "Epoch: 03 [112704/244490 ( 46%)], Train Loss: 1.22795\n",
      "Epoch: 03 [113344/244490 ( 46%)], Train Loss: 1.22811\n",
      "Epoch: 03 [113984/244490 ( 47%)], Train Loss: 1.22728\n",
      "Epoch: 03 [114624/244490 ( 47%)], Train Loss: 1.22777\n",
      "Epoch: 03 [115264/244490 ( 47%)], Train Loss: 1.22819\n",
      "Epoch: 03 [115904/244490 ( 47%)], Train Loss: 1.22878\n",
      "Epoch: 03 [116544/244490 ( 48%)], Train Loss: 1.22908\n",
      "Epoch: 03 [117184/244490 ( 48%)], Train Loss: 1.22826\n",
      "Epoch: 03 [117824/244490 ( 48%)], Train Loss: 1.22797\n",
      "Epoch: 03 [118464/244490 ( 48%)], Train Loss: 1.22760\n",
      "Epoch: 03 [119104/244490 ( 49%)], Train Loss: 1.22751\n",
      "Epoch: 03 [119744/244490 ( 49%)], Train Loss: 1.22699\n",
      "Epoch: 03 [120384/244490 ( 49%)], Train Loss: 1.22613\n",
      "Epoch: 03 [121024/244490 ( 50%)], Train Loss: 1.22583\n",
      "Epoch: 03 [121664/244490 ( 50%)], Train Loss: 1.22513\n",
      "Epoch: 03 [122304/244490 ( 50%)], Train Loss: 1.22448\n",
      "Epoch: 03 [122944/244490 ( 50%)], Train Loss: 1.22404\n",
      "Epoch: 03 [123584/244490 ( 51%)], Train Loss: 1.22316\n",
      "Epoch: 03 [124224/244490 ( 51%)], Train Loss: 1.22288\n",
      "Epoch: 03 [124864/244490 ( 51%)], Train Loss: 1.22288\n",
      "Epoch: 03 [125504/244490 ( 51%)], Train Loss: 1.22299\n",
      "Epoch: 03 [126144/244490 ( 52%)], Train Loss: 1.22270\n",
      "Epoch: 03 [126784/244490 ( 52%)], Train Loss: 1.22233\n",
      "Epoch: 03 [127424/244490 ( 52%)], Train Loss: 1.22228\n",
      "Epoch: 03 [128064/244490 ( 52%)], Train Loss: 1.22250\n",
      "Epoch: 03 [128704/244490 ( 53%)], Train Loss: 1.22224\n",
      "Epoch: 03 [129344/244490 ( 53%)], Train Loss: 1.22277\n",
      "Epoch: 03 [129984/244490 ( 53%)], Train Loss: 1.22248\n",
      "Epoch: 03 [130624/244490 ( 53%)], Train Loss: 1.22260\n",
      "Epoch: 03 [131264/244490 ( 54%)], Train Loss: 1.22272\n",
      "Epoch: 03 [131904/244490 ( 54%)], Train Loss: 1.22276\n",
      "Epoch: 03 [132544/244490 ( 54%)], Train Loss: 1.22193\n",
      "Epoch: 03 [133184/244490 ( 54%)], Train Loss: 1.22145\n",
      "Epoch: 03 [133824/244490 ( 55%)], Train Loss: 1.22096\n",
      "Epoch: 03 [134464/244490 ( 55%)], Train Loss: 1.22038\n",
      "Epoch: 03 [135104/244490 ( 55%)], Train Loss: 1.22058\n",
      "Epoch: 03 [135744/244490 ( 56%)], Train Loss: 1.22096\n",
      "Epoch: 03 [136384/244490 ( 56%)], Train Loss: 1.22172\n",
      "Epoch: 03 [137024/244490 ( 56%)], Train Loss: 1.22169\n",
      "Epoch: 03 [137664/244490 ( 56%)], Train Loss: 1.22199\n",
      "Epoch: 03 [138304/244490 ( 57%)], Train Loss: 1.22224\n",
      "Epoch: 03 [138944/244490 ( 57%)], Train Loss: 1.22176\n",
      "Epoch: 03 [139584/244490 ( 57%)], Train Loss: 1.22110\n",
      "Epoch: 03 [140224/244490 ( 57%)], Train Loss: 1.22081\n",
      "Epoch: 03 [140864/244490 ( 58%)], Train Loss: 1.22064\n",
      "Epoch: 03 [141504/244490 ( 58%)], Train Loss: 1.22033\n",
      "Epoch: 03 [142144/244490 ( 58%)], Train Loss: 1.22044\n",
      "Epoch: 03 [142784/244490 ( 58%)], Train Loss: 1.22044\n",
      "Epoch: 03 [143424/244490 ( 59%)], Train Loss: 1.22043\n",
      "Epoch: 03 [144064/244490 ( 59%)], Train Loss: 1.22026\n",
      "Epoch: 03 [144704/244490 ( 59%)], Train Loss: 1.22060\n",
      "Epoch: 03 [145344/244490 ( 59%)], Train Loss: 1.22073\n",
      "Epoch: 03 [145984/244490 ( 60%)], Train Loss: 1.22124\n",
      "Epoch: 03 [146624/244490 ( 60%)], Train Loss: 1.22138\n",
      "Epoch: 03 [147264/244490 ( 60%)], Train Loss: 1.22144\n",
      "Epoch: 03 [147904/244490 ( 60%)], Train Loss: 1.22165\n",
      "Epoch: 03 [148544/244490 ( 61%)], Train Loss: 1.22142\n",
      "Epoch: 03 [149184/244490 ( 61%)], Train Loss: 1.22149\n",
      "Epoch: 03 [149824/244490 ( 61%)], Train Loss: 1.22133\n",
      "Epoch: 03 [150464/244490 ( 62%)], Train Loss: 1.22135\n",
      "Epoch: 03 [151104/244490 ( 62%)], Train Loss: 1.22107\n",
      "Epoch: 03 [151744/244490 ( 62%)], Train Loss: 1.22100\n",
      "Epoch: 03 [152384/244490 ( 62%)], Train Loss: 1.22079\n",
      "Epoch: 03 [153024/244490 ( 63%)], Train Loss: 1.22026\n",
      "Epoch: 03 [153664/244490 ( 63%)], Train Loss: 1.22008\n",
      "Epoch: 03 [154304/244490 ( 63%)], Train Loss: 1.22012\n",
      "Epoch: 03 [154944/244490 ( 63%)], Train Loss: 1.22010\n",
      "Epoch: 03 [155584/244490 ( 64%)], Train Loss: 1.21958\n",
      "Epoch: 03 [156224/244490 ( 64%)], Train Loss: 1.21985\n",
      "Epoch: 03 [156864/244490 ( 64%)], Train Loss: 1.21949\n",
      "Epoch: 03 [157504/244490 ( 64%)], Train Loss: 1.21908\n",
      "Epoch: 03 [158144/244490 ( 65%)], Train Loss: 1.21918\n",
      "Epoch: 03 [158784/244490 ( 65%)], Train Loss: 1.21907\n",
      "Epoch: 03 [159424/244490 ( 65%)], Train Loss: 1.21951\n",
      "Epoch: 03 [160064/244490 ( 65%)], Train Loss: 1.21934\n",
      "Epoch: 03 [160704/244490 ( 66%)], Train Loss: 1.21960\n",
      "Epoch: 03 [161344/244490 ( 66%)], Train Loss: 1.21960\n",
      "Epoch: 03 [161984/244490 ( 66%)], Train Loss: 1.21963\n",
      "Epoch: 03 [162624/244490 ( 67%)], Train Loss: 1.21938\n",
      "Epoch: 03 [163264/244490 ( 67%)], Train Loss: 1.21926\n",
      "Epoch: 03 [163904/244490 ( 67%)], Train Loss: 1.21923\n",
      "Epoch: 03 [164544/244490 ( 67%)], Train Loss: 1.21924\n",
      "Epoch: 03 [165184/244490 ( 68%)], Train Loss: 1.21926\n",
      "Epoch: 03 [165824/244490 ( 68%)], Train Loss: 1.21870\n",
      "Epoch: 03 [166464/244490 ( 68%)], Train Loss: 1.21841\n",
      "Epoch: 03 [167104/244490 ( 68%)], Train Loss: 1.21820\n",
      "Epoch: 03 [167744/244490 ( 69%)], Train Loss: 1.21816\n",
      "Epoch: 03 [168384/244490 ( 69%)], Train Loss: 1.21765\n",
      "Epoch: 03 [169024/244490 ( 69%)], Train Loss: 1.21774\n",
      "Epoch: 03 [169664/244490 ( 69%)], Train Loss: 1.21786\n",
      "Epoch: 03 [170304/244490 ( 70%)], Train Loss: 1.21742\n",
      "Epoch: 03 [170944/244490 ( 70%)], Train Loss: 1.21700\n",
      "Epoch: 03 [171584/244490 ( 70%)], Train Loss: 1.21669\n",
      "Epoch: 03 [172224/244490 ( 70%)], Train Loss: 1.21699\n",
      "Epoch: 03 [172864/244490 ( 71%)], Train Loss: 1.21696\n",
      "Epoch: 03 [173504/244490 ( 71%)], Train Loss: 1.21713\n",
      "Epoch: 03 [174144/244490 ( 71%)], Train Loss: 1.21704\n",
      "Epoch: 03 [174784/244490 ( 71%)], Train Loss: 1.21688\n",
      "Epoch: 03 [175424/244490 ( 72%)], Train Loss: 1.21667\n",
      "Epoch: 03 [176064/244490 ( 72%)], Train Loss: 1.21666\n",
      "Epoch: 03 [176704/244490 ( 72%)], Train Loss: 1.21684\n",
      "Epoch: 03 [177344/244490 ( 73%)], Train Loss: 1.21638\n",
      "Epoch: 03 [177984/244490 ( 73%)], Train Loss: 1.21604\n",
      "Epoch: 03 [178624/244490 ( 73%)], Train Loss: 1.21574\n",
      "Epoch: 03 [179264/244490 ( 73%)], Train Loss: 1.21527\n",
      "Epoch: 03 [179904/244490 ( 74%)], Train Loss: 1.21517\n",
      "Epoch: 03 [180544/244490 ( 74%)], Train Loss: 1.21488\n",
      "Epoch: 03 [181184/244490 ( 74%)], Train Loss: 1.21478\n",
      "Epoch: 03 [181824/244490 ( 74%)], Train Loss: 1.21441\n",
      "Epoch: 03 [182464/244490 ( 75%)], Train Loss: 1.21393\n",
      "Epoch: 03 [183104/244490 ( 75%)], Train Loss: 1.21386\n",
      "Epoch: 03 [183744/244490 ( 75%)], Train Loss: 1.21378\n",
      "Epoch: 03 [184384/244490 ( 75%)], Train Loss: 1.21318\n",
      "Epoch: 03 [185024/244490 ( 76%)], Train Loss: 1.21278\n",
      "Epoch: 03 [185664/244490 ( 76%)], Train Loss: 1.21250\n",
      "Epoch: 03 [186304/244490 ( 76%)], Train Loss: 1.21236\n",
      "Epoch: 03 [186944/244490 ( 76%)], Train Loss: 1.21217\n",
      "Epoch: 03 [187584/244490 ( 77%)], Train Loss: 1.21172\n",
      "Epoch: 03 [188224/244490 ( 77%)], Train Loss: 1.21135\n",
      "Epoch: 03 [188864/244490 ( 77%)], Train Loss: 1.21088\n",
      "Epoch: 03 [189504/244490 ( 78%)], Train Loss: 1.21079\n",
      "Epoch: 03 [190144/244490 ( 78%)], Train Loss: 1.21038\n",
      "Epoch: 03 [190784/244490 ( 78%)], Train Loss: 1.21000\n",
      "Epoch: 03 [191424/244490 ( 78%)], Train Loss: 1.20986\n",
      "Epoch: 03 [192064/244490 ( 79%)], Train Loss: 1.20973\n",
      "Epoch: 03 [192704/244490 ( 79%)], Train Loss: 1.20939\n",
      "Epoch: 03 [193344/244490 ( 79%)], Train Loss: 1.20890\n",
      "Epoch: 03 [193984/244490 ( 79%)], Train Loss: 1.20857\n",
      "Epoch: 03 [194624/244490 ( 80%)], Train Loss: 1.20815\n",
      "Epoch: 03 [195264/244490 ( 80%)], Train Loss: 1.20844\n",
      "Epoch: 03 [195904/244490 ( 80%)], Train Loss: 1.20804\n",
      "Epoch: 03 [196544/244490 ( 80%)], Train Loss: 1.20785\n",
      "Epoch: 03 [197184/244490 ( 81%)], Train Loss: 1.20766\n",
      "Epoch: 03 [197824/244490 ( 81%)], Train Loss: 1.20766\n",
      "Epoch: 03 [198464/244490 ( 81%)], Train Loss: 1.20776\n",
      "Epoch: 03 [199104/244490 ( 81%)], Train Loss: 1.20785\n",
      "Epoch: 03 [199744/244490 ( 82%)], Train Loss: 1.20830\n",
      "Epoch: 03 [200384/244490 ( 82%)], Train Loss: 1.20831\n",
      "Epoch: 03 [201024/244490 ( 82%)], Train Loss: 1.20775\n",
      "Epoch: 03 [201664/244490 ( 82%)], Train Loss: 1.20746\n",
      "Epoch: 03 [202304/244490 ( 83%)], Train Loss: 1.20723\n",
      "Epoch: 03 [202944/244490 ( 83%)], Train Loss: 1.20674\n",
      "Epoch: 03 [203584/244490 ( 83%)], Train Loss: 1.20631\n",
      "Epoch: 03 [204224/244490 ( 84%)], Train Loss: 1.20624\n",
      "Epoch: 03 [204864/244490 ( 84%)], Train Loss: 1.20588\n",
      "Epoch: 03 [205504/244490 ( 84%)], Train Loss: 1.20558\n",
      "Epoch: 03 [206144/244490 ( 84%)], Train Loss: 1.20538\n",
      "Epoch: 03 [206784/244490 ( 85%)], Train Loss: 1.20498\n",
      "Epoch: 03 [207424/244490 ( 85%)], Train Loss: 1.20483\n",
      "Epoch: 03 [208064/244490 ( 85%)], Train Loss: 1.20432\n",
      "Epoch: 03 [208704/244490 ( 85%)], Train Loss: 1.20462\n",
      "Epoch: 03 [209344/244490 ( 86%)], Train Loss: 1.20471\n",
      "Epoch: 03 [209984/244490 ( 86%)], Train Loss: 1.20471\n",
      "Epoch: 03 [210624/244490 ( 86%)], Train Loss: 1.20463\n",
      "Epoch: 03 [211264/244490 ( 86%)], Train Loss: 1.20489\n",
      "Epoch: 03 [211904/244490 ( 87%)], Train Loss: 1.20460\n",
      "Epoch: 03 [212544/244490 ( 87%)], Train Loss: 1.20454\n",
      "Epoch: 03 [213184/244490 ( 87%)], Train Loss: 1.20406\n",
      "Epoch: 03 [213824/244490 ( 87%)], Train Loss: 1.20398\n",
      "Epoch: 03 [214464/244490 ( 88%)], Train Loss: 1.20390\n",
      "Epoch: 03 [215104/244490 ( 88%)], Train Loss: 1.20360\n",
      "Epoch: 03 [215744/244490 ( 88%)], Train Loss: 1.20321\n",
      "Epoch: 03 [216384/244490 ( 89%)], Train Loss: 1.20266\n",
      "Epoch: 03 [217024/244490 ( 89%)], Train Loss: 1.20232\n",
      "Epoch: 03 [217664/244490 ( 89%)], Train Loss: 1.20209\n",
      "Epoch: 03 [218304/244490 ( 89%)], Train Loss: 1.20169\n",
      "Epoch: 03 [218944/244490 ( 90%)], Train Loss: 1.20183\n",
      "Epoch: 03 [219584/244490 ( 90%)], Train Loss: 1.20187\n",
      "Epoch: 03 [220224/244490 ( 90%)], Train Loss: 1.20174\n",
      "Epoch: 03 [220864/244490 ( 90%)], Train Loss: 1.20139\n",
      "Epoch: 03 [221504/244490 ( 91%)], Train Loss: 1.20105\n",
      "Epoch: 03 [222144/244490 ( 91%)], Train Loss: 1.20100\n",
      "Epoch: 03 [222784/244490 ( 91%)], Train Loss: 1.20110\n",
      "Epoch: 03 [223424/244490 ( 91%)], Train Loss: 1.20121\n",
      "Epoch: 03 [224064/244490 ( 92%)], Train Loss: 1.20083\n",
      "Epoch: 03 [224704/244490 ( 92%)], Train Loss: 1.20081\n",
      "Epoch: 03 [225344/244490 ( 92%)], Train Loss: 1.20061\n",
      "Epoch: 03 [225984/244490 ( 92%)], Train Loss: 1.20066\n",
      "Epoch: 03 [226624/244490 ( 93%)], Train Loss: 1.20079\n",
      "Epoch: 03 [227264/244490 ( 93%)], Train Loss: 1.20073\n",
      "Epoch: 03 [227904/244490 ( 93%)], Train Loss: 1.20065\n",
      "Epoch: 03 [228544/244490 ( 93%)], Train Loss: 1.20109\n",
      "Epoch: 03 [229184/244490 ( 94%)], Train Loss: 1.20107\n",
      "Epoch: 03 [229824/244490 ( 94%)], Train Loss: 1.20113\n",
      "Epoch: 03 [230464/244490 ( 94%)], Train Loss: 1.20105\n",
      "Epoch: 03 [231104/244490 ( 95%)], Train Loss: 1.20103\n",
      "Epoch: 03 [231744/244490 ( 95%)], Train Loss: 1.20113\n",
      "Epoch: 03 [232384/244490 ( 95%)], Train Loss: 1.20109\n",
      "Epoch: 03 [233024/244490 ( 95%)], Train Loss: 1.20097\n",
      "Epoch: 03 [233664/244490 ( 96%)], Train Loss: 1.20054\n",
      "Epoch: 03 [234304/244490 ( 96%)], Train Loss: 1.20061\n",
      "Epoch: 03 [234944/244490 ( 96%)], Train Loss: 1.20072\n",
      "Epoch: 03 [235584/244490 ( 96%)], Train Loss: 1.20088\n",
      "Epoch: 03 [236224/244490 ( 97%)], Train Loss: 1.20110\n",
      "Epoch: 03 [236864/244490 ( 97%)], Train Loss: 1.20110\n",
      "Epoch: 03 [237504/244490 ( 97%)], Train Loss: 1.20129\n",
      "Epoch: 03 [238144/244490 ( 97%)], Train Loss: 1.20082\n",
      "Epoch: 03 [238784/244490 ( 98%)], Train Loss: 1.20066\n",
      "Epoch: 03 [239424/244490 ( 98%)], Train Loss: 1.20079\n",
      "Epoch: 03 [240064/244490 ( 98%)], Train Loss: 1.20077\n",
      "Epoch: 03 [240704/244490 ( 98%)], Train Loss: 1.20062\n",
      "Epoch: 03 [241344/244490 ( 99%)], Train Loss: 1.20056\n",
      "Epoch: 03 [241984/244490 ( 99%)], Train Loss: 1.20039\n",
      "Epoch: 03 [242624/244490 ( 99%)], Train Loss: 1.20006\n",
      "Epoch: 03 [243264/244490 ( 99%)], Train Loss: 1.19994\n",
      "Epoch: 03 [243904/244490 (100%)], Train Loss: 1.19998\n",
      "Epoch: 03 [244490/244490 (100%)], Train Loss: 1.19994\n",
      "----Validation Results Summary----\n",
      "Epoch: [3] Valid Loss: 1.74029\n",
      "3 Epoch, Best epoch was updated! Valid Loss: 1.74029\n",
      "Saving model checkpoint to output/checkpoint-fold-0.\n",
      "\n",
      "Epoch: 04 [    64/244490 (  0%)], Train Loss: 1.29105\n",
      "Epoch: 04 [   704/244490 (  0%)], Train Loss: 1.15326\n",
      "Epoch: 04 [  1344/244490 (  1%)], Train Loss: 1.28135\n",
      "Epoch: 04 [  1984/244490 (  1%)], Train Loss: 1.34194\n",
      "Epoch: 04 [  2624/244490 (  1%)], Train Loss: 1.41338\n",
      "Epoch: 04 [  3264/244490 (  1%)], Train Loss: 1.43785\n",
      "Epoch: 04 [  3904/244490 (  2%)], Train Loss: 1.47027\n",
      "Epoch: 04 [  4544/244490 (  2%)], Train Loss: 1.50445\n",
      "Epoch: 04 [  5184/244490 (  2%)], Train Loss: 1.54991\n",
      "Epoch: 04 [  5824/244490 (  2%)], Train Loss: 1.60096\n",
      "Epoch: 04 [  6464/244490 (  3%)], Train Loss: 1.62299\n",
      "Epoch: 04 [  7104/244490 (  3%)], Train Loss: 1.64546\n",
      "Epoch: 04 [  7744/244490 (  3%)], Train Loss: 1.66199\n",
      "Epoch: 04 [  8384/244490 (  3%)], Train Loss: 1.67454\n",
      "Epoch: 04 [  9024/244490 (  4%)], Train Loss: 1.69563\n",
      "Epoch: 04 [  9664/244490 (  4%)], Train Loss: 1.70609\n",
      "Epoch: 04 [ 10304/244490 (  4%)], Train Loss: 1.72071\n",
      "Epoch: 04 [ 10944/244490 (  4%)], Train Loss: 1.72794\n",
      "Epoch: 04 [ 11584/244490 (  5%)], Train Loss: 1.74312\n",
      "Epoch: 04 [ 12224/244490 (  5%)], Train Loss: 1.75836\n",
      "Epoch: 04 [ 12864/244490 (  5%)], Train Loss: 1.77650\n",
      "Epoch: 04 [ 13504/244490 (  6%)], Train Loss: 1.78609\n",
      "Epoch: 04 [ 14144/244490 (  6%)], Train Loss: 1.78997\n",
      "Epoch: 04 [ 14784/244490 (  6%)], Train Loss: 1.79476\n",
      "Epoch: 04 [ 15424/244490 (  6%)], Train Loss: 1.80325\n",
      "Epoch: 04 [ 16064/244490 (  7%)], Train Loss: 1.81514\n",
      "Epoch: 04 [ 16704/244490 (  7%)], Train Loss: 1.82501\n",
      "Epoch: 04 [ 17344/244490 (  7%)], Train Loss: 1.82639\n",
      "Epoch: 04 [ 17984/244490 (  7%)], Train Loss: 1.82762\n",
      "Epoch: 04 [ 18624/244490 (  8%)], Train Loss: 1.82707\n",
      "Epoch: 04 [ 19264/244490 (  8%)], Train Loss: 1.83286\n",
      "Epoch: 04 [ 19904/244490 (  8%)], Train Loss: 1.83674\n",
      "Epoch: 04 [ 20544/244490 (  8%)], Train Loss: 1.83838\n",
      "Epoch: 04 [ 21184/244490 (  9%)], Train Loss: 1.84447\n",
      "Epoch: 04 [ 21824/244490 (  9%)], Train Loss: 1.85009\n",
      "Epoch: 04 [ 22464/244490 (  9%)], Train Loss: 1.85294\n",
      "Epoch: 04 [ 23104/244490 (  9%)], Train Loss: 1.85314\n",
      "Epoch: 04 [ 23744/244490 ( 10%)], Train Loss: 1.85530\n",
      "Epoch: 04 [ 24384/244490 ( 10%)], Train Loss: 1.85895\n",
      "Epoch: 04 [ 25024/244490 ( 10%)], Train Loss: 1.85968\n",
      "Epoch: 04 [ 25664/244490 ( 10%)], Train Loss: 1.86355\n",
      "Epoch: 04 [ 26304/244490 ( 11%)], Train Loss: 1.86398\n",
      "Epoch: 04 [ 26944/244490 ( 11%)], Train Loss: 1.86413\n",
      "Epoch: 04 [ 27584/244490 ( 11%)], Train Loss: 1.86526\n",
      "Epoch: 04 [ 28224/244490 ( 12%)], Train Loss: 1.86712\n",
      "Epoch: 04 [ 28864/244490 ( 12%)], Train Loss: 1.86672\n",
      "Epoch: 04 [ 29504/244490 ( 12%)], Train Loss: 1.86870\n",
      "Epoch: 04 [ 30144/244490 ( 12%)], Train Loss: 1.86964\n",
      "Epoch: 04 [ 30784/244490 ( 13%)], Train Loss: 1.87152\n",
      "Epoch: 04 [ 31424/244490 ( 13%)], Train Loss: 1.87035\n",
      "Epoch: 04 [ 32064/244490 ( 13%)], Train Loss: 1.87082\n",
      "Epoch: 04 [ 32704/244490 ( 13%)], Train Loss: 1.87264\n",
      "Epoch: 04 [ 33344/244490 ( 14%)], Train Loss: 1.87682\n",
      "Epoch: 04 [ 33984/244490 ( 14%)], Train Loss: 1.88114\n",
      "Epoch: 04 [ 34624/244490 ( 14%)], Train Loss: 1.88624\n",
      "Epoch: 04 [ 35264/244490 ( 14%)], Train Loss: 1.88765\n",
      "Epoch: 04 [ 35904/244490 ( 15%)], Train Loss: 1.88934\n",
      "Epoch: 04 [ 36544/244490 ( 15%)], Train Loss: 1.89140\n",
      "Epoch: 04 [ 37184/244490 ( 15%)], Train Loss: 1.89123\n",
      "Epoch: 04 [ 37824/244490 ( 15%)], Train Loss: 1.89235\n",
      "Epoch: 04 [ 38464/244490 ( 16%)], Train Loss: 1.89319\n",
      "Epoch: 04 [ 39104/244490 ( 16%)], Train Loss: 1.89385\n",
      "Epoch: 04 [ 39744/244490 ( 16%)], Train Loss: 1.89483\n",
      "Epoch: 04 [ 40384/244490 ( 17%)], Train Loss: 1.89739\n",
      "Epoch: 04 [ 41024/244490 ( 17%)], Train Loss: 1.89811\n",
      "Epoch: 04 [ 41664/244490 ( 17%)], Train Loss: 1.89887\n",
      "Epoch: 04 [ 42304/244490 ( 17%)], Train Loss: 1.90153\n",
      "Epoch: 04 [ 42944/244490 ( 18%)], Train Loss: 1.90159\n",
      "Epoch: 04 [ 43584/244490 ( 18%)], Train Loss: 1.90160\n",
      "Epoch: 04 [ 44224/244490 ( 18%)], Train Loss: 1.90157\n",
      "Epoch: 04 [ 44864/244490 ( 18%)], Train Loss: 1.90233\n",
      "Epoch: 04 [ 45504/244490 ( 19%)], Train Loss: 1.90399\n",
      "Epoch: 04 [ 46144/244490 ( 19%)], Train Loss: 1.90281\n",
      "Epoch: 04 [ 46784/244490 ( 19%)], Train Loss: 1.90301\n",
      "Epoch: 04 [ 47424/244490 ( 19%)], Train Loss: 1.90553\n",
      "Epoch: 04 [ 48064/244490 ( 20%)], Train Loss: 1.90480\n",
      "Epoch: 04 [ 48704/244490 ( 20%)], Train Loss: 1.90639\n",
      "Epoch: 04 [ 49344/244490 ( 20%)], Train Loss: 1.90690\n",
      "Epoch: 04 [ 49984/244490 ( 20%)], Train Loss: 1.90754\n",
      "Epoch: 04 [ 50624/244490 ( 21%)], Train Loss: 1.90724\n",
      "Epoch: 04 [ 51264/244490 ( 21%)], Train Loss: 1.90707\n",
      "Epoch: 04 [ 51904/244490 ( 21%)], Train Loss: 1.90562\n",
      "Epoch: 04 [ 52544/244490 ( 21%)], Train Loss: 1.90564\n",
      "Epoch: 04 [ 53184/244490 ( 22%)], Train Loss: 1.90620\n",
      "Epoch: 04 [ 53824/244490 ( 22%)], Train Loss: 1.90593\n",
      "Epoch: 04 [ 54464/244490 ( 22%)], Train Loss: 1.90443\n",
      "Epoch: 04 [ 55104/244490 ( 23%)], Train Loss: 1.90590\n",
      "Epoch: 04 [ 55744/244490 ( 23%)], Train Loss: 1.90776\n",
      "Epoch: 04 [ 56384/244490 ( 23%)], Train Loss: 1.90698\n",
      "Epoch: 04 [ 57024/244490 ( 23%)], Train Loss: 1.90744\n",
      "Epoch: 04 [ 57664/244490 ( 24%)], Train Loss: 1.90795\n",
      "Epoch: 04 [ 58304/244490 ( 24%)], Train Loss: 1.90702\n",
      "Epoch: 04 [ 58944/244490 ( 24%)], Train Loss: 1.90645\n",
      "Epoch: 04 [ 59584/244490 ( 24%)], Train Loss: 1.90760\n",
      "Epoch: 04 [ 60224/244490 ( 25%)], Train Loss: 1.90648\n",
      "Epoch: 04 [ 60864/244490 ( 25%)], Train Loss: 1.90819\n",
      "Epoch: 04 [ 61504/244490 ( 25%)], Train Loss: 1.90832\n",
      "Epoch: 04 [ 62144/244490 ( 25%)], Train Loss: 1.90929\n",
      "Epoch: 04 [ 62784/244490 ( 26%)], Train Loss: 1.90871\n",
      "Epoch: 04 [ 63424/244490 ( 26%)], Train Loss: 1.90879\n",
      "Epoch: 04 [ 64064/244490 ( 26%)], Train Loss: 1.90801\n",
      "Epoch: 04 [ 64704/244490 ( 26%)], Train Loss: 1.90947\n",
      "Epoch: 04 [ 65344/244490 ( 27%)], Train Loss: 1.90993\n",
      "Epoch: 04 [ 65984/244490 ( 27%)], Train Loss: 1.91065\n",
      "Epoch: 04 [ 66624/244490 ( 27%)], Train Loss: 1.91196\n",
      "Epoch: 04 [ 67264/244490 ( 28%)], Train Loss: 1.91218\n",
      "Epoch: 04 [ 67904/244490 ( 28%)], Train Loss: 1.91317\n",
      "Epoch: 04 [ 68544/244490 ( 28%)], Train Loss: 1.91410\n",
      "Epoch: 04 [ 69184/244490 ( 28%)], Train Loss: 1.91384\n",
      "Epoch: 04 [ 69824/244490 ( 29%)], Train Loss: 1.91264\n",
      "Epoch: 04 [ 70464/244490 ( 29%)], Train Loss: 1.91284\n",
      "Epoch: 04 [ 71104/244490 ( 29%)], Train Loss: 1.91320\n",
      "Epoch: 04 [ 71744/244490 ( 29%)], Train Loss: 1.91229\n",
      "Epoch: 04 [ 72384/244490 ( 30%)], Train Loss: 1.91223\n",
      "Epoch: 04 [ 73024/244490 ( 30%)], Train Loss: 1.91155\n",
      "Epoch: 04 [ 73664/244490 ( 30%)], Train Loss: 1.91153\n",
      "Epoch: 04 [ 74304/244490 ( 30%)], Train Loss: 1.91186\n",
      "Epoch: 04 [ 74944/244490 ( 31%)], Train Loss: 1.91105\n",
      "Epoch: 04 [ 75584/244490 ( 31%)], Train Loss: 1.90954\n",
      "Epoch: 04 [ 76224/244490 ( 31%)], Train Loss: 1.90953\n",
      "Epoch: 04 [ 76864/244490 ( 31%)], Train Loss: 1.90860\n",
      "Epoch: 04 [ 77504/244490 ( 32%)], Train Loss: 1.90969\n",
      "Epoch: 04 [ 78144/244490 ( 32%)], Train Loss: 1.91001\n",
      "Epoch: 04 [ 78784/244490 ( 32%)], Train Loss: 1.91120\n",
      "Epoch: 04 [ 79424/244490 ( 32%)], Train Loss: 1.91238\n",
      "Epoch: 04 [ 80064/244490 ( 33%)], Train Loss: 1.91189\n",
      "Epoch: 04 [ 80704/244490 ( 33%)], Train Loss: 1.91179\n",
      "Epoch: 04 [ 81344/244490 ( 33%)], Train Loss: 1.91242\n",
      "Epoch: 04 [ 81984/244490 ( 34%)], Train Loss: 1.91251\n",
      "Epoch: 04 [ 82624/244490 ( 34%)], Train Loss: 1.91267\n",
      "Epoch: 04 [ 83264/244490 ( 34%)], Train Loss: 1.91295\n",
      "Epoch: 04 [ 83904/244490 ( 34%)], Train Loss: 1.91234\n",
      "Epoch: 04 [ 84544/244490 ( 35%)], Train Loss: 1.91096\n",
      "Epoch: 04 [ 85184/244490 ( 35%)], Train Loss: 1.91116\n",
      "Epoch: 04 [ 85824/244490 ( 35%)], Train Loss: 1.91007\n",
      "Epoch: 04 [ 86464/244490 ( 35%)], Train Loss: 1.90994\n",
      "Epoch: 04 [ 87104/244490 ( 36%)], Train Loss: 1.91009\n",
      "Epoch: 04 [ 87744/244490 ( 36%)], Train Loss: 1.91020\n",
      "Epoch: 04 [ 88384/244490 ( 36%)], Train Loss: 1.91037\n",
      "Epoch: 04 [ 89024/244490 ( 36%)], Train Loss: 1.91075\n",
      "Epoch: 04 [ 89664/244490 ( 37%)], Train Loss: 1.91040\n",
      "Epoch: 04 [ 90304/244490 ( 37%)], Train Loss: 1.91056\n",
      "Epoch: 04 [ 90944/244490 ( 37%)], Train Loss: 1.91047\n",
      "Epoch: 04 [ 91584/244490 ( 37%)], Train Loss: 1.91035\n",
      "Epoch: 04 [ 92224/244490 ( 38%)], Train Loss: 1.91132\n",
      "Epoch: 04 [ 92864/244490 ( 38%)], Train Loss: 1.91130\n",
      "Epoch: 04 [ 93504/244490 ( 38%)], Train Loss: 1.91072\n",
      "Epoch: 04 [ 94144/244490 ( 39%)], Train Loss: 1.91052\n",
      "Epoch: 04 [ 94784/244490 ( 39%)], Train Loss: 1.91038\n",
      "Epoch: 04 [ 95424/244490 ( 39%)], Train Loss: 1.90999\n",
      "Epoch: 04 [ 96064/244490 ( 39%)], Train Loss: 1.90993\n",
      "Epoch: 04 [ 96704/244490 ( 40%)], Train Loss: 1.90923\n",
      "Epoch: 04 [ 97344/244490 ( 40%)], Train Loss: 1.90884\n",
      "Epoch: 04 [ 97984/244490 ( 40%)], Train Loss: 1.90903\n",
      "Epoch: 04 [ 98624/244490 ( 40%)], Train Loss: 1.90906\n",
      "Epoch: 04 [ 99264/244490 ( 41%)], Train Loss: 1.90829\n",
      "Epoch: 04 [ 99904/244490 ( 41%)], Train Loss: 1.90816\n",
      "Epoch: 04 [100544/244490 ( 41%)], Train Loss: 1.90818\n",
      "Epoch: 04 [101184/244490 ( 41%)], Train Loss: 1.90895\n",
      "Epoch: 04 [101824/244490 ( 42%)], Train Loss: 1.90887\n",
      "Epoch: 04 [102464/244490 ( 42%)], Train Loss: 1.90868\n",
      "Epoch: 04 [103104/244490 ( 42%)], Train Loss: 1.90792\n",
      "Epoch: 04 [103744/244490 ( 42%)], Train Loss: 1.90805\n",
      "Epoch: 04 [104384/244490 ( 43%)], Train Loss: 1.90887\n",
      "Epoch: 04 [105024/244490 ( 43%)], Train Loss: 1.90896\n",
      "Epoch: 04 [105664/244490 ( 43%)], Train Loss: 1.90987\n",
      "Epoch: 04 [106304/244490 ( 43%)], Train Loss: 1.91030\n",
      "Epoch: 04 [106944/244490 ( 44%)], Train Loss: 1.91010\n",
      "Epoch: 04 [107584/244490 ( 44%)], Train Loss: 1.91004\n",
      "Epoch: 04 [108224/244490 ( 44%)], Train Loss: 1.91056\n",
      "Epoch: 04 [108864/244490 ( 45%)], Train Loss: 1.91105\n",
      "Epoch: 04 [109504/244490 ( 45%)], Train Loss: 1.91083\n",
      "Epoch: 04 [110144/244490 ( 45%)], Train Loss: 1.90993\n",
      "Epoch: 04 [110784/244490 ( 45%)], Train Loss: 1.90897\n",
      "Epoch: 04 [111424/244490 ( 46%)], Train Loss: 1.90880\n",
      "Epoch: 04 [112064/244490 ( 46%)], Train Loss: 1.90860\n",
      "Epoch: 04 [112704/244490 ( 46%)], Train Loss: 1.90804\n",
      "Epoch: 04 [113344/244490 ( 46%)], Train Loss: 1.90809\n",
      "Epoch: 04 [113984/244490 ( 47%)], Train Loss: 1.90816\n",
      "Epoch: 04 [114624/244490 ( 47%)], Train Loss: 1.90853\n",
      "Epoch: 04 [115264/244490 ( 47%)], Train Loss: 1.90791\n",
      "Epoch: 04 [115904/244490 ( 47%)], Train Loss: 1.90760\n",
      "Epoch: 04 [116544/244490 ( 48%)], Train Loss: 1.90797\n",
      "Epoch: 04 [117184/244490 ( 48%)], Train Loss: 1.90730\n",
      "Epoch: 04 [117824/244490 ( 48%)], Train Loss: 1.90733\n",
      "Epoch: 04 [118464/244490 ( 48%)], Train Loss: 1.90685\n",
      "Epoch: 04 [119104/244490 ( 49%)], Train Loss: 1.90698\n",
      "Epoch: 04 [119744/244490 ( 49%)], Train Loss: 1.90658\n",
      "Epoch: 04 [120384/244490 ( 49%)], Train Loss: 1.90619\n",
      "Epoch: 04 [121024/244490 ( 50%)], Train Loss: 1.90673\n",
      "Epoch: 04 [121664/244490 ( 50%)], Train Loss: 1.90663\n",
      "Epoch: 04 [122304/244490 ( 50%)], Train Loss: 1.90690\n",
      "Epoch: 04 [122944/244490 ( 50%)], Train Loss: 1.90665\n",
      "Epoch: 04 [123584/244490 ( 51%)], Train Loss: 1.90585\n",
      "Epoch: 04 [124224/244490 ( 51%)], Train Loss: 1.90602\n",
      "Epoch: 04 [124864/244490 ( 51%)], Train Loss: 1.90686\n",
      "Epoch: 04 [125504/244490 ( 51%)], Train Loss: 1.90721\n",
      "Epoch: 04 [126144/244490 ( 52%)], Train Loss: 1.90672\n",
      "Epoch: 04 [126784/244490 ( 52%)], Train Loss: 1.90634\n",
      "Epoch: 04 [127424/244490 ( 52%)], Train Loss: 1.90660\n",
      "Epoch: 04 [128064/244490 ( 52%)], Train Loss: 1.90737\n",
      "Epoch: 04 [128704/244490 ( 53%)], Train Loss: 1.90746\n",
      "Epoch: 04 [129344/244490 ( 53%)], Train Loss: 1.90851\n",
      "Epoch: 04 [129984/244490 ( 53%)], Train Loss: 1.90779\n",
      "Epoch: 04 [130624/244490 ( 53%)], Train Loss: 1.90775\n",
      "Epoch: 04 [131264/244490 ( 54%)], Train Loss: 1.90800\n",
      "Epoch: 04 [131904/244490 ( 54%)], Train Loss: 1.90815\n",
      "Epoch: 04 [132544/244490 ( 54%)], Train Loss: 1.90828\n",
      "Epoch: 04 [133184/244490 ( 54%)], Train Loss: 1.90787\n",
      "Epoch: 04 [133824/244490 ( 55%)], Train Loss: 1.90778\n",
      "Epoch: 04 [134464/244490 ( 55%)], Train Loss: 1.90776\n",
      "Epoch: 04 [135104/244490 ( 55%)], Train Loss: 1.90780\n",
      "Epoch: 04 [135744/244490 ( 56%)], Train Loss: 1.90844\n",
      "Epoch: 04 [136384/244490 ( 56%)], Train Loss: 1.90874\n",
      "Epoch: 04 [137024/244490 ( 56%)], Train Loss: 1.90847\n",
      "Epoch: 04 [137664/244490 ( 56%)], Train Loss: 1.90868\n",
      "Epoch: 04 [138304/244490 ( 57%)], Train Loss: 1.90858\n",
      "Epoch: 04 [138944/244490 ( 57%)], Train Loss: 1.90810\n",
      "Epoch: 04 [139584/244490 ( 57%)], Train Loss: 1.90783\n",
      "Epoch: 04 [140224/244490 ( 57%)], Train Loss: 1.90797\n",
      "Epoch: 04 [140864/244490 ( 58%)], Train Loss: 1.90803\n",
      "Epoch: 04 [141504/244490 ( 58%)], Train Loss: 1.90741\n",
      "Epoch: 04 [142144/244490 ( 58%)], Train Loss: 1.90730\n",
      "Epoch: 04 [142784/244490 ( 58%)], Train Loss: 1.90740\n",
      "Epoch: 04 [143424/244490 ( 59%)], Train Loss: 1.90692\n",
      "Epoch: 04 [144064/244490 ( 59%)], Train Loss: 1.90676\n",
      "Epoch: 04 [144704/244490 ( 59%)], Train Loss: 1.90658\n",
      "Epoch: 04 [145344/244490 ( 59%)], Train Loss: 1.90621\n",
      "Epoch: 04 [145984/244490 ( 60%)], Train Loss: 1.90633\n",
      "Epoch: 04 [146624/244490 ( 60%)], Train Loss: 1.90621\n",
      "Epoch: 04 [147264/244490 ( 60%)], Train Loss: 1.90623\n",
      "Epoch: 04 [147904/244490 ( 60%)], Train Loss: 1.90612\n",
      "Epoch: 04 [148544/244490 ( 61%)], Train Loss: 1.90621\n",
      "Epoch: 04 [149184/244490 ( 61%)], Train Loss: 1.90686\n",
      "Epoch: 04 [149824/244490 ( 61%)], Train Loss: 1.90701\n",
      "Epoch: 04 [150464/244490 ( 62%)], Train Loss: 1.90670\n",
      "Epoch: 04 [151104/244490 ( 62%)], Train Loss: 1.90713\n",
      "Epoch: 04 [151744/244490 ( 62%)], Train Loss: 1.90731\n",
      "Epoch: 04 [152384/244490 ( 62%)], Train Loss: 1.90752\n",
      "Epoch: 04 [153024/244490 ( 63%)], Train Loss: 1.90712\n",
      "Epoch: 04 [153664/244490 ( 63%)], Train Loss: 1.90708\n",
      "Epoch: 04 [154304/244490 ( 63%)], Train Loss: 1.90688\n",
      "Epoch: 04 [154944/244490 ( 63%)], Train Loss: 1.90689\n",
      "Epoch: 04 [155584/244490 ( 64%)], Train Loss: 1.90672\n",
      "Epoch: 04 [156224/244490 ( 64%)], Train Loss: 1.90742\n",
      "Epoch: 04 [156864/244490 ( 64%)], Train Loss: 1.90718\n",
      "Epoch: 04 [157504/244490 ( 64%)], Train Loss: 1.90666\n",
      "Epoch: 04 [158144/244490 ( 65%)], Train Loss: 1.90691\n",
      "Epoch: 04 [158784/244490 ( 65%)], Train Loss: 1.90719\n",
      "Epoch: 04 [159424/244490 ( 65%)], Train Loss: 1.90761\n",
      "Epoch: 04 [160064/244490 ( 65%)], Train Loss: 1.90720\n",
      "Epoch: 04 [160704/244490 ( 66%)], Train Loss: 1.90686\n",
      "Epoch: 04 [161344/244490 ( 66%)], Train Loss: 1.90703\n",
      "Epoch: 04 [161984/244490 ( 66%)], Train Loss: 1.90675\n",
      "Epoch: 04 [162624/244490 ( 67%)], Train Loss: 1.90633\n",
      "Epoch: 04 [163264/244490 ( 67%)], Train Loss: 1.90620\n",
      "Epoch: 04 [163904/244490 ( 67%)], Train Loss: 1.90605\n",
      "Epoch: 04 [164544/244490 ( 67%)], Train Loss: 1.90564\n",
      "Epoch: 04 [165184/244490 ( 68%)], Train Loss: 1.90537\n",
      "Epoch: 04 [165824/244490 ( 68%)], Train Loss: 1.90511\n",
      "Epoch: 04 [166464/244490 ( 68%)], Train Loss: 1.90528\n",
      "Epoch: 04 [167104/244490 ( 68%)], Train Loss: 1.90544\n",
      "Epoch: 04 [167744/244490 ( 69%)], Train Loss: 1.90529\n",
      "Epoch: 04 [168384/244490 ( 69%)], Train Loss: 1.90502\n",
      "Epoch: 04 [169024/244490 ( 69%)], Train Loss: 1.90490\n",
      "Epoch: 04 [169664/244490 ( 69%)], Train Loss: 1.90473\n",
      "Epoch: 04 [170304/244490 ( 70%)], Train Loss: 1.90410\n",
      "Epoch: 04 [170944/244490 ( 70%)], Train Loss: 1.90391\n",
      "Epoch: 04 [171584/244490 ( 70%)], Train Loss: 1.90417\n",
      "Epoch: 04 [172224/244490 ( 70%)], Train Loss: 1.90420\n",
      "Epoch: 04 [172864/244490 ( 71%)], Train Loss: 1.90373\n",
      "Epoch: 04 [173504/244490 ( 71%)], Train Loss: 1.90362\n",
      "Epoch: 04 [174144/244490 ( 71%)], Train Loss: 1.90381\n",
      "Epoch: 04 [174784/244490 ( 71%)], Train Loss: 1.90364\n",
      "Epoch: 04 [175424/244490 ( 72%)], Train Loss: 1.90313\n",
      "Epoch: 04 [176064/244490 ( 72%)], Train Loss: 1.90320\n",
      "Epoch: 04 [176704/244490 ( 72%)], Train Loss: 1.90337\n",
      "Epoch: 04 [177344/244490 ( 73%)], Train Loss: 1.90298\n",
      "Epoch: 04 [177984/244490 ( 73%)], Train Loss: 1.90274\n",
      "Epoch: 04 [178624/244490 ( 73%)], Train Loss: 1.90265\n",
      "Epoch: 04 [179264/244490 ( 73%)], Train Loss: 1.90255\n",
      "Epoch: 04 [179904/244490 ( 74%)], Train Loss: 1.90263\n",
      "Epoch: 04 [180544/244490 ( 74%)], Train Loss: 1.90245\n",
      "Epoch: 04 [181184/244490 ( 74%)], Train Loss: 1.90213\n",
      "Epoch: 04 [181824/244490 ( 74%)], Train Loss: 1.90150\n",
      "Epoch: 04 [182464/244490 ( 75%)], Train Loss: 1.90107\n",
      "Epoch: 04 [183104/244490 ( 75%)], Train Loss: 1.90100\n",
      "Epoch: 04 [183744/244490 ( 75%)], Train Loss: 1.90058\n",
      "Epoch: 04 [184384/244490 ( 75%)], Train Loss: 1.90036\n",
      "Epoch: 04 [185024/244490 ( 76%)], Train Loss: 1.90016\n",
      "Epoch: 04 [185664/244490 ( 76%)], Train Loss: 1.89964\n",
      "Epoch: 04 [186304/244490 ( 76%)], Train Loss: 1.89928\n",
      "Epoch: 04 [186944/244490 ( 76%)], Train Loss: 1.89920\n",
      "Epoch: 04 [187584/244490 ( 77%)], Train Loss: 1.89879\n",
      "Epoch: 04 [188224/244490 ( 77%)], Train Loss: 1.89855\n",
      "Epoch: 04 [188864/244490 ( 77%)], Train Loss: 1.89815\n",
      "Epoch: 04 [189504/244490 ( 78%)], Train Loss: 1.89810\n",
      "Epoch: 04 [190144/244490 ( 78%)], Train Loss: 1.89797\n",
      "Epoch: 04 [190784/244490 ( 78%)], Train Loss: 1.89799\n",
      "Epoch: 04 [191424/244490 ( 78%)], Train Loss: 1.89804\n",
      "Epoch: 04 [192064/244490 ( 79%)], Train Loss: 1.89813\n",
      "Epoch: 04 [192704/244490 ( 79%)], Train Loss: 1.89788\n",
      "Epoch: 04 [193344/244490 ( 79%)], Train Loss: 1.89752\n",
      "Epoch: 04 [193984/244490 ( 79%)], Train Loss: 1.89757\n",
      "Epoch: 04 [194624/244490 ( 80%)], Train Loss: 1.89717\n",
      "Epoch: 04 [195264/244490 ( 80%)], Train Loss: 1.89711\n",
      "Epoch: 04 [195904/244490 ( 80%)], Train Loss: 1.89651\n",
      "Epoch: 04 [196544/244490 ( 80%)], Train Loss: 1.89610\n",
      "Epoch: 04 [197184/244490 ( 81%)], Train Loss: 1.89615\n",
      "Epoch: 04 [197824/244490 ( 81%)], Train Loss: 1.89637\n",
      "Epoch: 04 [198464/244490 ( 81%)], Train Loss: 1.89636\n",
      "Epoch: 04 [199104/244490 ( 81%)], Train Loss: 1.89643\n",
      "Epoch: 04 [199744/244490 ( 82%)], Train Loss: 1.89647\n",
      "Epoch: 04 [200384/244490 ( 82%)], Train Loss: 1.89618\n",
      "Epoch: 04 [201024/244490 ( 82%)], Train Loss: 1.89583\n",
      "Epoch: 04 [201664/244490 ( 82%)], Train Loss: 1.89546\n",
      "Epoch: 04 [202304/244490 ( 83%)], Train Loss: 1.89539\n",
      "Epoch: 04 [202944/244490 ( 83%)], Train Loss: 1.89492\n",
      "Epoch: 04 [203584/244490 ( 83%)], Train Loss: 1.89452\n",
      "Epoch: 04 [204224/244490 ( 84%)], Train Loss: 1.89473\n",
      "Epoch: 04 [204864/244490 ( 84%)], Train Loss: 1.89463\n",
      "Epoch: 04 [205504/244490 ( 84%)], Train Loss: 1.89465\n",
      "Epoch: 04 [206144/244490 ( 84%)], Train Loss: 1.89466\n",
      "Epoch: 04 [206784/244490 ( 85%)], Train Loss: 1.89455\n",
      "Epoch: 04 [207424/244490 ( 85%)], Train Loss: 1.89439\n",
      "Epoch: 04 [208064/244490 ( 85%)], Train Loss: 1.89365\n",
      "Epoch: 04 [208704/244490 ( 85%)], Train Loss: 1.89392\n",
      "Epoch: 04 [209344/244490 ( 86%)], Train Loss: 1.89352\n",
      "Epoch: 04 [209984/244490 ( 86%)], Train Loss: 1.89357\n",
      "Epoch: 04 [210624/244490 ( 86%)], Train Loss: 1.89337\n",
      "Epoch: 04 [211264/244490 ( 86%)], Train Loss: 1.89315\n",
      "Epoch: 04 [211904/244490 ( 87%)], Train Loss: 1.89293\n",
      "Epoch: 04 [212544/244490 ( 87%)], Train Loss: 1.89286\n",
      "Epoch: 04 [213184/244490 ( 87%)], Train Loss: 1.89304\n",
      "Epoch: 04 [213824/244490 ( 87%)], Train Loss: 1.89270\n",
      "Epoch: 04 [214464/244490 ( 88%)], Train Loss: 1.89259\n",
      "Epoch: 04 [215104/244490 ( 88%)], Train Loss: 1.89253\n",
      "Epoch: 04 [215744/244490 ( 88%)], Train Loss: 1.89240\n",
      "Epoch: 04 [216384/244490 ( 89%)], Train Loss: 1.89214\n",
      "Epoch: 04 [217024/244490 ( 89%)], Train Loss: 1.89217\n",
      "Epoch: 04 [217664/244490 ( 89%)], Train Loss: 1.89169\n",
      "Epoch: 04 [218304/244490 ( 89%)], Train Loss: 1.89145\n",
      "Epoch: 04 [218944/244490 ( 90%)], Train Loss: 1.89157\n",
      "Epoch: 04 [219584/244490 ( 90%)], Train Loss: 1.89151\n",
      "Epoch: 04 [220224/244490 ( 90%)], Train Loss: 1.89142\n",
      "Epoch: 04 [220864/244490 ( 90%)], Train Loss: 1.89107\n",
      "Epoch: 04 [221504/244490 ( 91%)], Train Loss: 1.89102\n",
      "Epoch: 04 [222144/244490 ( 91%)], Train Loss: 1.89040\n",
      "Epoch: 04 [222784/244490 ( 91%)], Train Loss: 1.89031\n",
      "Epoch: 04 [223424/244490 ( 91%)], Train Loss: 1.89021\n",
      "Epoch: 04 [224064/244490 ( 92%)], Train Loss: 1.88974\n",
      "Epoch: 04 [224704/244490 ( 92%)], Train Loss: 1.88976\n",
      "Epoch: 04 [225344/244490 ( 92%)], Train Loss: 1.88968\n",
      "Epoch: 04 [225984/244490 ( 92%)], Train Loss: 1.88958\n",
      "Epoch: 04 [226624/244490 ( 93%)], Train Loss: 1.88953\n",
      "Epoch: 04 [227264/244490 ( 93%)], Train Loss: 1.88928\n",
      "Epoch: 04 [227904/244490 ( 93%)], Train Loss: 1.88896\n",
      "Epoch: 04 [228544/244490 ( 93%)], Train Loss: 1.88919\n",
      "Epoch: 04 [229184/244490 ( 94%)], Train Loss: 1.88908\n",
      "Epoch: 04 [229824/244490 ( 94%)], Train Loss: 1.88932\n",
      "Epoch: 04 [230464/244490 ( 94%)], Train Loss: 1.88928\n",
      "Epoch: 04 [231104/244490 ( 95%)], Train Loss: 1.88924\n",
      "Epoch: 04 [231744/244490 ( 95%)], Train Loss: 1.88930\n",
      "Epoch: 04 [232384/244490 ( 95%)], Train Loss: 1.88920\n",
      "Epoch: 04 [233024/244490 ( 95%)], Train Loss: 1.88927\n",
      "Epoch: 04 [233664/244490 ( 96%)], Train Loss: 1.88886\n",
      "Epoch: 04 [234304/244490 ( 96%)], Train Loss: 1.88873\n",
      "Epoch: 04 [234944/244490 ( 96%)], Train Loss: 1.88864\n",
      "Epoch: 04 [235584/244490 ( 96%)], Train Loss: 1.88872\n",
      "Epoch: 04 [236224/244490 ( 97%)], Train Loss: 1.88855\n",
      "Epoch: 04 [236864/244490 ( 97%)], Train Loss: 1.88844\n",
      "Epoch: 04 [237504/244490 ( 97%)], Train Loss: 1.88845\n",
      "Epoch: 04 [238144/244490 ( 97%)], Train Loss: 1.88810\n",
      "Epoch: 04 [238784/244490 ( 98%)], Train Loss: 1.88803\n",
      "Epoch: 04 [239424/244490 ( 98%)], Train Loss: 1.88830\n",
      "Epoch: 04 [240064/244490 ( 98%)], Train Loss: 1.88828\n",
      "Epoch: 04 [240704/244490 ( 98%)], Train Loss: 1.88834\n",
      "Epoch: 04 [241344/244490 ( 99%)], Train Loss: 1.88863\n",
      "Epoch: 04 [241984/244490 ( 99%)], Train Loss: 1.88844\n",
      "Epoch: 04 [242624/244490 ( 99%)], Train Loss: 1.88814\n",
      "Epoch: 04 [243264/244490 ( 99%)], Train Loss: 1.88817\n",
      "Epoch: 04 [243904/244490 (100%)], Train Loss: 1.88804\n",
      "Epoch: 04 [244490/244490 (100%)], Train Loss: 1.88753\n",
      "----Validation Results Summary----\n",
      "Epoch: [4] Valid Loss: 1.87835\n",
      "\n",
      "Epoch: 05 [    64/244490 (  0%)], Train Loss: 1.78221\n",
      "Epoch: 05 [   704/244490 (  0%)], Train Loss: 1.82222\n",
      "Epoch: 05 [  1344/244490 (  1%)], Train Loss: 1.81760\n",
      "Epoch: 05 [  1984/244490 (  1%)], Train Loss: 1.81141\n",
      "Epoch: 05 [  2624/244490 (  1%)], Train Loss: 1.81659\n",
      "Epoch: 05 [  3264/244490 (  1%)], Train Loss: 1.82229\n",
      "Epoch: 05 [  3904/244490 (  2%)], Train Loss: 1.82192\n",
      "Epoch: 05 [  4544/244490 (  2%)], Train Loss: 1.81794\n",
      "Epoch: 05 [  5184/244490 (  2%)], Train Loss: 1.82688\n",
      "Epoch: 05 [  5824/244490 (  2%)], Train Loss: 1.84524\n",
      "Epoch: 05 [  6464/244490 (  3%)], Train Loss: 1.84449\n",
      "Epoch: 05 [  7104/244490 (  3%)], Train Loss: 1.85455\n",
      "Epoch: 05 [  7744/244490 (  3%)], Train Loss: 1.86522\n",
      "Epoch: 05 [  8384/244490 (  3%)], Train Loss: 1.86507\n",
      "Epoch: 05 [  9024/244490 (  4%)], Train Loss: 1.86795\n",
      "Epoch: 05 [  9664/244490 (  4%)], Train Loss: 1.86351\n",
      "Epoch: 05 [ 10304/244490 (  4%)], Train Loss: 1.86057\n",
      "Epoch: 05 [ 10944/244490 (  4%)], Train Loss: 1.85732\n",
      "Epoch: 05 [ 11584/244490 (  5%)], Train Loss: 1.85873\n",
      "Epoch: 05 [ 12224/244490 (  5%)], Train Loss: 1.86176\n",
      "Epoch: 05 [ 12864/244490 (  5%)], Train Loss: 1.86578\n",
      "Epoch: 05 [ 13504/244490 (  6%)], Train Loss: 1.86713\n",
      "Epoch: 05 [ 14144/244490 (  6%)], Train Loss: 1.86744\n",
      "Epoch: 05 [ 14784/244490 (  6%)], Train Loss: 1.86695\n",
      "Epoch: 05 [ 15424/244490 (  6%)], Train Loss: 1.86999\n",
      "Epoch: 05 [ 16064/244490 (  7%)], Train Loss: 1.87325\n",
      "Epoch: 05 [ 16704/244490 (  7%)], Train Loss: 1.87806\n",
      "Epoch: 05 [ 17344/244490 (  7%)], Train Loss: 1.87536\n",
      "Epoch: 05 [ 17984/244490 (  7%)], Train Loss: 1.87475\n",
      "Epoch: 05 [ 18624/244490 (  8%)], Train Loss: 1.87399\n",
      "Epoch: 05 [ 19264/244490 (  8%)], Train Loss: 1.87985\n",
      "Epoch: 05 [ 19904/244490 (  8%)], Train Loss: 1.88000\n",
      "Epoch: 05 [ 20544/244490 (  8%)], Train Loss: 1.87801\n",
      "Epoch: 05 [ 21184/244490 (  9%)], Train Loss: 1.87844\n",
      "Epoch: 05 [ 21824/244490 (  9%)], Train Loss: 1.87963\n",
      "Epoch: 05 [ 22464/244490 (  9%)], Train Loss: 1.88116\n",
      "Epoch: 05 [ 23104/244490 (  9%)], Train Loss: 1.88301\n",
      "Epoch: 05 [ 23744/244490 ( 10%)], Train Loss: 1.88495\n",
      "Epoch: 05 [ 24384/244490 ( 10%)], Train Loss: 1.88729\n",
      "Epoch: 05 [ 25024/244490 ( 10%)], Train Loss: 1.88658\n",
      "Epoch: 05 [ 25664/244490 ( 10%)], Train Loss: 1.88693\n",
      "Epoch: 05 [ 26304/244490 ( 11%)], Train Loss: 1.88485\n",
      "Epoch: 05 [ 26944/244490 ( 11%)], Train Loss: 1.88197\n",
      "Epoch: 05 [ 27584/244490 ( 11%)], Train Loss: 1.88077\n",
      "Epoch: 05 [ 28224/244490 ( 12%)], Train Loss: 1.87778\n",
      "Epoch: 05 [ 28864/244490 ( 12%)], Train Loss: 1.87547\n",
      "Epoch: 05 [ 29504/244490 ( 12%)], Train Loss: 1.87536\n",
      "Epoch: 05 [ 30144/244490 ( 12%)], Train Loss: 1.87479\n",
      "Epoch: 05 [ 30784/244490 ( 13%)], Train Loss: 1.87364\n",
      "Epoch: 05 [ 31424/244490 ( 13%)], Train Loss: 1.87247\n",
      "Epoch: 05 [ 32064/244490 ( 13%)], Train Loss: 1.87168\n",
      "Epoch: 05 [ 32704/244490 ( 13%)], Train Loss: 1.87096\n",
      "Epoch: 05 [ 33344/244490 ( 14%)], Train Loss: 1.87038\n",
      "Epoch: 05 [ 33984/244490 ( 14%)], Train Loss: 1.87033\n",
      "Epoch: 05 [ 34624/244490 ( 14%)], Train Loss: 1.87485\n",
      "Epoch: 05 [ 35264/244490 ( 14%)], Train Loss: 1.87537\n",
      "Epoch: 05 [ 35904/244490 ( 15%)], Train Loss: 1.87506\n",
      "Epoch: 05 [ 36544/244490 ( 15%)], Train Loss: 1.87536\n",
      "Epoch: 05 [ 37184/244490 ( 15%)], Train Loss: 1.87400\n",
      "Epoch: 05 [ 37824/244490 ( 15%)], Train Loss: 1.87231\n",
      "Epoch: 05 [ 38464/244490 ( 16%)], Train Loss: 1.86987\n",
      "Epoch: 05 [ 39104/244490 ( 16%)], Train Loss: 1.87046\n",
      "Epoch: 05 [ 39744/244490 ( 16%)], Train Loss: 1.87060\n",
      "Epoch: 05 [ 40384/244490 ( 17%)], Train Loss: 1.86943\n",
      "Epoch: 05 [ 41024/244490 ( 17%)], Train Loss: 1.86855\n",
      "Epoch: 05 [ 41664/244490 ( 17%)], Train Loss: 1.86817\n",
      "Epoch: 05 [ 42304/244490 ( 17%)], Train Loss: 1.87040\n",
      "Epoch: 05 [ 42944/244490 ( 18%)], Train Loss: 1.87019\n",
      "Epoch: 05 [ 43584/244490 ( 18%)], Train Loss: 1.86944\n",
      "Epoch: 05 [ 44224/244490 ( 18%)], Train Loss: 1.86888\n",
      "Epoch: 05 [ 44864/244490 ( 18%)], Train Loss: 1.86818\n",
      "Epoch: 05 [ 45504/244490 ( 19%)], Train Loss: 1.86974\n",
      "Epoch: 05 [ 46144/244490 ( 19%)], Train Loss: 1.86788\n",
      "Epoch: 05 [ 46784/244490 ( 19%)], Train Loss: 1.86854\n",
      "Epoch: 05 [ 47424/244490 ( 19%)], Train Loss: 1.87036\n",
      "Epoch: 05 [ 48064/244490 ( 20%)], Train Loss: 1.86953\n",
      "Epoch: 05 [ 48704/244490 ( 20%)], Train Loss: 1.87046\n",
      "Epoch: 05 [ 49344/244490 ( 20%)], Train Loss: 1.87031\n",
      "Epoch: 05 [ 49984/244490 ( 20%)], Train Loss: 1.86980\n",
      "Epoch: 05 [ 50624/244490 ( 21%)], Train Loss: 1.87178\n",
      "Epoch: 05 [ 51264/244490 ( 21%)], Train Loss: 1.87213\n",
      "Epoch: 05 [ 51904/244490 ( 21%)], Train Loss: 1.87196\n",
      "Epoch: 05 [ 52544/244490 ( 21%)], Train Loss: 1.87179\n",
      "Epoch: 05 [ 53184/244490 ( 22%)], Train Loss: 1.87119\n",
      "Epoch: 05 [ 53824/244490 ( 22%)], Train Loss: 1.86959\n",
      "Epoch: 05 [ 54464/244490 ( 22%)], Train Loss: 1.86778\n",
      "Epoch: 05 [ 55104/244490 ( 23%)], Train Loss: 1.86871\n",
      "Epoch: 05 [ 55744/244490 ( 23%)], Train Loss: 1.87106\n",
      "Epoch: 05 [ 56384/244490 ( 23%)], Train Loss: 1.86981\n",
      "Epoch: 05 [ 57024/244490 ( 23%)], Train Loss: 1.86993\n",
      "Epoch: 05 [ 57664/244490 ( 24%)], Train Loss: 1.87066\n",
      "Epoch: 05 [ 58304/244490 ( 24%)], Train Loss: 1.87092\n",
      "Epoch: 05 [ 58944/244490 ( 24%)], Train Loss: 1.87030\n",
      "Epoch: 05 [ 59584/244490 ( 24%)], Train Loss: 1.87167\n",
      "Epoch: 05 [ 60224/244490 ( 25%)], Train Loss: 1.87018\n",
      "Epoch: 05 [ 60864/244490 ( 25%)], Train Loss: 1.87127\n",
      "Epoch: 05 [ 61504/244490 ( 25%)], Train Loss: 1.87057\n",
      "Epoch: 05 [ 62144/244490 ( 25%)], Train Loss: 1.87136\n",
      "Epoch: 05 [ 62784/244490 ( 26%)], Train Loss: 1.87122\n",
      "Epoch: 05 [ 63424/244490 ( 26%)], Train Loss: 1.87110\n",
      "Epoch: 05 [ 64064/244490 ( 26%)], Train Loss: 1.87096\n",
      "Epoch: 05 [ 64704/244490 ( 26%)], Train Loss: 1.87220\n",
      "Epoch: 05 [ 65344/244490 ( 27%)], Train Loss: 1.87266\n",
      "Epoch: 05 [ 65984/244490 ( 27%)], Train Loss: 1.87323\n",
      "Epoch: 05 [ 66624/244490 ( 27%)], Train Loss: 1.87436\n",
      "Epoch: 05 [ 67264/244490 ( 28%)], Train Loss: 1.87428\n",
      "Epoch: 05 [ 67904/244490 ( 28%)], Train Loss: 1.87443\n",
      "Epoch: 05 [ 68544/244490 ( 28%)], Train Loss: 1.87498\n",
      "Epoch: 05 [ 69184/244490 ( 28%)], Train Loss: 1.87415\n",
      "Epoch: 05 [ 69824/244490 ( 29%)], Train Loss: 1.87389\n",
      "Epoch: 05 [ 70464/244490 ( 29%)], Train Loss: 1.87424\n",
      "Epoch: 05 [ 71104/244490 ( 29%)], Train Loss: 1.87445\n",
      "Epoch: 05 [ 71744/244490 ( 29%)], Train Loss: 1.87384\n",
      "Epoch: 05 [ 72384/244490 ( 30%)], Train Loss: 1.87347\n",
      "Epoch: 05 [ 73024/244490 ( 30%)], Train Loss: 1.87264\n",
      "Epoch: 05 [ 73664/244490 ( 30%)], Train Loss: 1.87255\n",
      "Epoch: 05 [ 74304/244490 ( 30%)], Train Loss: 1.87273\n",
      "Epoch: 05 [ 74944/244490 ( 31%)], Train Loss: 1.87171\n",
      "Epoch: 05 [ 75584/244490 ( 31%)], Train Loss: 1.87022\n",
      "Epoch: 05 [ 76224/244490 ( 31%)], Train Loss: 1.87054\n",
      "Epoch: 05 [ 76864/244490 ( 31%)], Train Loss: 1.86923\n",
      "Epoch: 05 [ 77504/244490 ( 32%)], Train Loss: 1.86988\n",
      "Epoch: 05 [ 78144/244490 ( 32%)], Train Loss: 1.86976\n",
      "Epoch: 05 [ 78784/244490 ( 32%)], Train Loss: 1.87031\n",
      "Epoch: 05 [ 79424/244490 ( 32%)], Train Loss: 1.87121\n",
      "Epoch: 05 [ 80064/244490 ( 33%)], Train Loss: 1.87002\n",
      "Epoch: 05 [ 80704/244490 ( 33%)], Train Loss: 1.87052\n",
      "Epoch: 05 [ 81344/244490 ( 33%)], Train Loss: 1.87082\n",
      "Epoch: 05 [ 81984/244490 ( 34%)], Train Loss: 1.87050\n",
      "Epoch: 05 [ 82624/244490 ( 34%)], Train Loss: 1.87042\n",
      "Epoch: 05 [ 83264/244490 ( 34%)], Train Loss: 1.87023\n",
      "Epoch: 05 [ 83904/244490 ( 34%)], Train Loss: 1.86923\n",
      "Epoch: 05 [ 84544/244490 ( 35%)], Train Loss: 1.86786\n",
      "Epoch: 05 [ 85184/244490 ( 35%)], Train Loss: 1.86748\n",
      "Epoch: 05 [ 85824/244490 ( 35%)], Train Loss: 1.86592\n",
      "Epoch: 05 [ 86464/244490 ( 35%)], Train Loss: 1.86575\n",
      "Epoch: 05 [ 87104/244490 ( 36%)], Train Loss: 1.86585\n",
      "Epoch: 05 [ 87744/244490 ( 36%)], Train Loss: 1.86578\n",
      "Epoch: 05 [ 88384/244490 ( 36%)], Train Loss: 1.86596\n",
      "Epoch: 05 [ 89024/244490 ( 36%)], Train Loss: 1.86624\n",
      "Epoch: 05 [ 89664/244490 ( 37%)], Train Loss: 1.86538\n",
      "Epoch: 05 [ 90304/244490 ( 37%)], Train Loss: 1.86569\n",
      "Epoch: 05 [ 90944/244490 ( 37%)], Train Loss: 1.86559\n",
      "Epoch: 05 [ 91584/244490 ( 37%)], Train Loss: 1.86528\n",
      "Epoch: 05 [ 92224/244490 ( 38%)], Train Loss: 1.86598\n",
      "Epoch: 05 [ 92864/244490 ( 38%)], Train Loss: 1.86576\n",
      "Epoch: 05 [ 93504/244490 ( 38%)], Train Loss: 1.86477\n",
      "Epoch: 05 [ 94144/244490 ( 39%)], Train Loss: 1.86442\n",
      "Epoch: 05 [ 94784/244490 ( 39%)], Train Loss: 1.86389\n",
      "Epoch: 05 [ 95424/244490 ( 39%)], Train Loss: 1.86333\n",
      "Epoch: 05 [ 96064/244490 ( 39%)], Train Loss: 1.86335\n",
      "Epoch: 05 [ 96704/244490 ( 40%)], Train Loss: 1.86297\n",
      "Epoch: 05 [ 97344/244490 ( 40%)], Train Loss: 1.86298\n",
      "Epoch: 05 [ 97984/244490 ( 40%)], Train Loss: 1.86294\n",
      "Epoch: 05 [ 98624/244490 ( 40%)], Train Loss: 1.86226\n",
      "Epoch: 05 [ 99264/244490 ( 41%)], Train Loss: 1.86144\n",
      "Epoch: 05 [ 99904/244490 ( 41%)], Train Loss: 1.86121\n",
      "Epoch: 05 [100544/244490 ( 41%)], Train Loss: 1.86102\n",
      "Epoch: 05 [101184/244490 ( 41%)], Train Loss: 1.86214\n",
      "Epoch: 05 [101824/244490 ( 42%)], Train Loss: 1.86230\n",
      "Epoch: 05 [102464/244490 ( 42%)], Train Loss: 1.86212\n",
      "Epoch: 05 [103104/244490 ( 42%)], Train Loss: 1.86150\n",
      "Epoch: 05 [103744/244490 ( 42%)], Train Loss: 1.86152\n",
      "Epoch: 05 [104384/244490 ( 43%)], Train Loss: 1.86239\n",
      "Epoch: 05 [105024/244490 ( 43%)], Train Loss: 1.86232\n",
      "Epoch: 05 [105664/244490 ( 43%)], Train Loss: 1.86350\n",
      "Epoch: 05 [106304/244490 ( 43%)], Train Loss: 1.86346\n",
      "Epoch: 05 [106944/244490 ( 44%)], Train Loss: 1.86362\n",
      "Epoch: 05 [107584/244490 ( 44%)], Train Loss: 1.86329\n",
      "Epoch: 05 [108224/244490 ( 44%)], Train Loss: 1.86357\n",
      "Epoch: 05 [108864/244490 ( 45%)], Train Loss: 1.86431\n",
      "Epoch: 05 [109504/244490 ( 45%)], Train Loss: 1.86438\n",
      "Epoch: 05 [110144/244490 ( 45%)], Train Loss: 1.86366\n",
      "Epoch: 05 [110784/244490 ( 45%)], Train Loss: 1.86268\n",
      "Epoch: 05 [111424/244490 ( 46%)], Train Loss: 1.86221\n",
      "Epoch: 05 [112064/244490 ( 46%)], Train Loss: 1.86147\n",
      "Epoch: 05 [112704/244490 ( 46%)], Train Loss: 1.86066\n",
      "Epoch: 05 [113344/244490 ( 46%)], Train Loss: 1.86079\n",
      "Epoch: 05 [113984/244490 ( 47%)], Train Loss: 1.86060\n",
      "Epoch: 05 [114624/244490 ( 47%)], Train Loss: 1.86102\n",
      "Epoch: 05 [115264/244490 ( 47%)], Train Loss: 1.86055\n",
      "Epoch: 05 [115904/244490 ( 47%)], Train Loss: 1.85993\n",
      "Epoch: 05 [116544/244490 ( 48%)], Train Loss: 1.86010\n",
      "Epoch: 05 [117184/244490 ( 48%)], Train Loss: 1.85923\n",
      "Epoch: 05 [117824/244490 ( 48%)], Train Loss: 1.85931\n",
      "Epoch: 05 [118464/244490 ( 48%)], Train Loss: 1.85893\n",
      "Epoch: 05 [119104/244490 ( 49%)], Train Loss: 1.85910\n",
      "Epoch: 05 [119744/244490 ( 49%)], Train Loss: 1.85905\n",
      "Epoch: 05 [120384/244490 ( 49%)], Train Loss: 1.85851\n",
      "Epoch: 05 [121024/244490 ( 50%)], Train Loss: 1.85900\n",
      "Epoch: 05 [121664/244490 ( 50%)], Train Loss: 1.85889\n",
      "Epoch: 05 [122304/244490 ( 50%)], Train Loss: 1.85885\n",
      "Epoch: 05 [122944/244490 ( 50%)], Train Loss: 1.85872\n",
      "Epoch: 05 [123584/244490 ( 51%)], Train Loss: 1.85785\n",
      "Epoch: 05 [124224/244490 ( 51%)], Train Loss: 1.85760\n",
      "Epoch: 05 [124864/244490 ( 51%)], Train Loss: 1.85835\n",
      "Epoch: 05 [125504/244490 ( 51%)], Train Loss: 1.85898\n",
      "Epoch: 05 [126144/244490 ( 52%)], Train Loss: 1.85829\n",
      "Epoch: 05 [126784/244490 ( 52%)], Train Loss: 1.85808\n",
      "Epoch: 05 [127424/244490 ( 52%)], Train Loss: 1.85840\n",
      "Epoch: 05 [128064/244490 ( 52%)], Train Loss: 1.85945\n",
      "Epoch: 05 [128704/244490 ( 53%)], Train Loss: 1.85975\n",
      "Epoch: 05 [129344/244490 ( 53%)], Train Loss: 1.86048\n",
      "Epoch: 05 [129984/244490 ( 53%)], Train Loss: 1.85963\n",
      "Epoch: 05 [130624/244490 ( 53%)], Train Loss: 1.85967\n",
      "Epoch: 05 [131264/244490 ( 54%)], Train Loss: 1.86002\n",
      "Epoch: 05 [131904/244490 ( 54%)], Train Loss: 1.85979\n",
      "Epoch: 05 [132544/244490 ( 54%)], Train Loss: 1.85966\n",
      "Epoch: 05 [133184/244490 ( 54%)], Train Loss: 1.85963\n",
      "Epoch: 05 [133824/244490 ( 55%)], Train Loss: 1.85963\n",
      "Epoch: 05 [134464/244490 ( 55%)], Train Loss: 1.85929\n",
      "Epoch: 05 [135104/244490 ( 55%)], Train Loss: 1.85915\n",
      "Epoch: 05 [135744/244490 ( 56%)], Train Loss: 1.85963\n",
      "Epoch: 05 [136384/244490 ( 56%)], Train Loss: 1.86029\n",
      "Epoch: 05 [137024/244490 ( 56%)], Train Loss: 1.86006\n",
      "Epoch: 05 [137664/244490 ( 56%)], Train Loss: 1.85989\n",
      "Epoch: 05 [138304/244490 ( 57%)], Train Loss: 1.86008\n",
      "Epoch: 05 [138944/244490 ( 57%)], Train Loss: 1.85969\n",
      "Epoch: 05 [139584/244490 ( 57%)], Train Loss: 1.85904\n",
      "Epoch: 05 [140224/244490 ( 57%)], Train Loss: 1.85889\n",
      "Epoch: 05 [140864/244490 ( 58%)], Train Loss: 1.85890\n",
      "Epoch: 05 [141504/244490 ( 58%)], Train Loss: 1.85829\n",
      "Epoch: 05 [142144/244490 ( 58%)], Train Loss: 1.85818\n",
      "Epoch: 05 [142784/244490 ( 58%)], Train Loss: 1.85794\n",
      "Epoch: 05 [143424/244490 ( 59%)], Train Loss: 1.85733\n",
      "Epoch: 05 [144064/244490 ( 59%)], Train Loss: 1.85698\n",
      "Epoch: 05 [144704/244490 ( 59%)], Train Loss: 1.85676\n",
      "Epoch: 05 [145344/244490 ( 59%)], Train Loss: 1.85656\n",
      "Epoch: 05 [145984/244490 ( 60%)], Train Loss: 1.85659\n",
      "Epoch: 05 [146624/244490 ( 60%)], Train Loss: 1.85644\n",
      "Epoch: 05 [147264/244490 ( 60%)], Train Loss: 1.85648\n",
      "Epoch: 05 [147904/244490 ( 60%)], Train Loss: 1.85691\n",
      "Epoch: 05 [148544/244490 ( 61%)], Train Loss: 1.85720\n",
      "Epoch: 05 [149184/244490 ( 61%)], Train Loss: 1.85776\n",
      "Epoch: 05 [149824/244490 ( 61%)], Train Loss: 1.85759\n",
      "Epoch: 05 [150464/244490 ( 62%)], Train Loss: 1.85689\n",
      "Epoch: 05 [151104/244490 ( 62%)], Train Loss: 1.85714\n",
      "Epoch: 05 [151744/244490 ( 62%)], Train Loss: 1.85744\n",
      "Epoch: 05 [152384/244490 ( 62%)], Train Loss: 1.85745\n",
      "Epoch: 05 [153024/244490 ( 63%)], Train Loss: 1.85682\n",
      "Epoch: 05 [153664/244490 ( 63%)], Train Loss: 1.85690\n",
      "Epoch: 05 [154304/244490 ( 63%)], Train Loss: 1.85685\n",
      "Epoch: 05 [154944/244490 ( 63%)], Train Loss: 1.85670\n",
      "Epoch: 05 [155584/244490 ( 64%)], Train Loss: 1.85682\n",
      "Epoch: 05 [156224/244490 ( 64%)], Train Loss: 1.85723\n",
      "Epoch: 05 [156864/244490 ( 64%)], Train Loss: 1.85671\n",
      "Epoch: 05 [157504/244490 ( 64%)], Train Loss: 1.85625\n",
      "Epoch: 05 [158144/244490 ( 65%)], Train Loss: 1.85611\n",
      "Epoch: 05 [158784/244490 ( 65%)], Train Loss: 1.85611\n",
      "Epoch: 05 [159424/244490 ( 65%)], Train Loss: 1.85673\n",
      "Epoch: 05 [160064/244490 ( 65%)], Train Loss: 1.85661\n",
      "Epoch: 05 [160704/244490 ( 66%)], Train Loss: 1.85628\n",
      "Epoch: 05 [161344/244490 ( 66%)], Train Loss: 1.85649\n",
      "Epoch: 05 [161984/244490 ( 66%)], Train Loss: 1.85628\n",
      "Epoch: 05 [162624/244490 ( 67%)], Train Loss: 1.85592\n",
      "Epoch: 05 [163264/244490 ( 67%)], Train Loss: 1.85583\n",
      "Epoch: 05 [163904/244490 ( 67%)], Train Loss: 1.85597\n",
      "Epoch: 05 [164544/244490 ( 67%)], Train Loss: 1.85570\n",
      "Epoch: 05 [165184/244490 ( 68%)], Train Loss: 1.85543\n",
      "Epoch: 05 [165824/244490 ( 68%)], Train Loss: 1.85525\n",
      "Epoch: 05 [166464/244490 ( 68%)], Train Loss: 1.85555\n",
      "Epoch: 05 [167104/244490 ( 68%)], Train Loss: 1.85574\n",
      "Epoch: 05 [167744/244490 ( 69%)], Train Loss: 1.85545\n",
      "Epoch: 05 [168384/244490 ( 69%)], Train Loss: 1.85518\n",
      "Epoch: 05 [169024/244490 ( 69%)], Train Loss: 1.85507\n",
      "Epoch: 05 [169664/244490 ( 69%)], Train Loss: 1.85504\n",
      "Epoch: 05 [170304/244490 ( 70%)], Train Loss: 1.85448\n",
      "Epoch: 05 [170944/244490 ( 70%)], Train Loss: 1.85433\n",
      "Epoch: 05 [171584/244490 ( 70%)], Train Loss: 1.85431\n",
      "Epoch: 05 [172224/244490 ( 70%)], Train Loss: 1.85443\n",
      "Epoch: 05 [172864/244490 ( 71%)], Train Loss: 1.85393\n",
      "Epoch: 05 [173504/244490 ( 71%)], Train Loss: 1.85393\n",
      "Epoch: 05 [174144/244490 ( 71%)], Train Loss: 1.85402\n",
      "Epoch: 05 [174784/244490 ( 71%)], Train Loss: 1.85379\n",
      "Epoch: 05 [175424/244490 ( 72%)], Train Loss: 1.85324\n",
      "Epoch: 05 [176064/244490 ( 72%)], Train Loss: 1.85322\n",
      "Epoch: 05 [176704/244490 ( 72%)], Train Loss: 1.85336\n",
      "Epoch: 05 [177344/244490 ( 73%)], Train Loss: 1.85292\n",
      "Epoch: 05 [177984/244490 ( 73%)], Train Loss: 1.85253\n",
      "Epoch: 05 [178624/244490 ( 73%)], Train Loss: 1.85247\n",
      "Epoch: 05 [179264/244490 ( 73%)], Train Loss: 1.85235\n",
      "Epoch: 05 [179904/244490 ( 74%)], Train Loss: 1.85267\n",
      "Epoch: 05 [180544/244490 ( 74%)], Train Loss: 1.85260\n",
      "Epoch: 05 [181184/244490 ( 74%)], Train Loss: 1.85224\n",
      "Epoch: 05 [181824/244490 ( 74%)], Train Loss: 1.85127\n",
      "Epoch: 05 [182464/244490 ( 75%)], Train Loss: 1.85090\n",
      "Epoch: 05 [183104/244490 ( 75%)], Train Loss: 1.85106\n",
      "Epoch: 05 [183744/244490 ( 75%)], Train Loss: 1.85043\n",
      "Epoch: 05 [184384/244490 ( 75%)], Train Loss: 1.85038\n",
      "Epoch: 05 [185024/244490 ( 76%)], Train Loss: 1.85018\n",
      "Epoch: 05 [185664/244490 ( 76%)], Train Loss: 1.84966\n",
      "Epoch: 05 [186304/244490 ( 76%)], Train Loss: 1.84935\n",
      "Epoch: 05 [186944/244490 ( 76%)], Train Loss: 1.84911\n",
      "Epoch: 05 [187584/244490 ( 77%)], Train Loss: 1.84875\n",
      "Epoch: 05 [188224/244490 ( 77%)], Train Loss: 1.84849\n",
      "Epoch: 05 [188864/244490 ( 77%)], Train Loss: 1.84811\n",
      "Epoch: 05 [189504/244490 ( 78%)], Train Loss: 1.84820\n",
      "Epoch: 05 [190144/244490 ( 78%)], Train Loss: 1.84804\n",
      "Epoch: 05 [190784/244490 ( 78%)], Train Loss: 1.84811\n",
      "Epoch: 05 [191424/244490 ( 78%)], Train Loss: 1.84812\n",
      "Epoch: 05 [192064/244490 ( 79%)], Train Loss: 1.84790\n",
      "Epoch: 05 [192704/244490 ( 79%)], Train Loss: 1.84768\n",
      "Epoch: 05 [193344/244490 ( 79%)], Train Loss: 1.84722\n",
      "Epoch: 05 [193984/244490 ( 79%)], Train Loss: 1.84726\n",
      "Epoch: 05 [194624/244490 ( 80%)], Train Loss: 1.84689\n",
      "Epoch: 05 [195264/244490 ( 80%)], Train Loss: 1.84699\n",
      "Epoch: 05 [195904/244490 ( 80%)], Train Loss: 1.84642\n",
      "Epoch: 05 [196544/244490 ( 80%)], Train Loss: 1.84609\n",
      "Epoch: 05 [197184/244490 ( 81%)], Train Loss: 1.84608\n",
      "Epoch: 05 [197824/244490 ( 81%)], Train Loss: 1.84643\n",
      "Epoch: 05 [198464/244490 ( 81%)], Train Loss: 1.84626\n",
      "Epoch: 05 [199104/244490 ( 81%)], Train Loss: 1.84613\n",
      "Epoch: 05 [199744/244490 ( 82%)], Train Loss: 1.84608\n",
      "Epoch: 05 [200384/244490 ( 82%)], Train Loss: 1.84566\n",
      "Epoch: 05 [201024/244490 ( 82%)], Train Loss: 1.84511\n",
      "Epoch: 05 [201664/244490 ( 82%)], Train Loss: 1.84489\n",
      "Epoch: 05 [202304/244490 ( 83%)], Train Loss: 1.84501\n",
      "Epoch: 05 [202944/244490 ( 83%)], Train Loss: 1.84437\n",
      "Epoch: 05 [203584/244490 ( 83%)], Train Loss: 1.84405\n",
      "Epoch: 05 [204224/244490 ( 84%)], Train Loss: 1.84414\n",
      "Epoch: 05 [204864/244490 ( 84%)], Train Loss: 1.84389\n",
      "Epoch: 05 [205504/244490 ( 84%)], Train Loss: 1.84391\n",
      "Epoch: 05 [206144/244490 ( 84%)], Train Loss: 1.84411\n",
      "Epoch: 05 [206784/244490 ( 85%)], Train Loss: 1.84393\n",
      "Epoch: 05 [207424/244490 ( 85%)], Train Loss: 1.84388\n",
      "Epoch: 05 [208064/244490 ( 85%)], Train Loss: 1.84327\n",
      "Epoch: 05 [208704/244490 ( 85%)], Train Loss: 1.84350\n",
      "Epoch: 05 [209344/244490 ( 86%)], Train Loss: 1.84313\n",
      "Epoch: 05 [209984/244490 ( 86%)], Train Loss: 1.84305\n",
      "Epoch: 05 [210624/244490 ( 86%)], Train Loss: 1.84292\n",
      "Epoch: 05 [211264/244490 ( 86%)], Train Loss: 1.84307\n",
      "Epoch: 05 [211904/244490 ( 87%)], Train Loss: 1.84281\n",
      "Epoch: 05 [212544/244490 ( 87%)], Train Loss: 1.84275\n",
      "Epoch: 05 [213184/244490 ( 87%)], Train Loss: 1.84282\n",
      "Epoch: 05 [213824/244490 ( 87%)], Train Loss: 1.84289\n",
      "Epoch: 05 [214464/244490 ( 88%)], Train Loss: 1.84279\n",
      "Epoch: 05 [215104/244490 ( 88%)], Train Loss: 1.84248\n",
      "Epoch: 05 [215744/244490 ( 88%)], Train Loss: 1.84217\n",
      "Epoch: 05 [216384/244490 ( 89%)], Train Loss: 1.84200\n",
      "Epoch: 05 [217024/244490 ( 89%)], Train Loss: 1.84198\n",
      "Epoch: 05 [217664/244490 ( 89%)], Train Loss: 1.84181\n",
      "Epoch: 05 [218304/244490 ( 89%)], Train Loss: 1.84165\n",
      "Epoch: 05 [218944/244490 ( 90%)], Train Loss: 1.84160\n",
      "Epoch: 05 [219584/244490 ( 90%)], Train Loss: 1.84145\n",
      "Epoch: 05 [220224/244490 ( 90%)], Train Loss: 1.84133\n",
      "Epoch: 05 [220864/244490 ( 90%)], Train Loss: 1.84107\n",
      "Epoch: 05 [221504/244490 ( 91%)], Train Loss: 1.84091\n",
      "Epoch: 05 [222144/244490 ( 91%)], Train Loss: 1.84075\n",
      "Epoch: 05 [222784/244490 ( 91%)], Train Loss: 1.84058\n",
      "Epoch: 05 [223424/244490 ( 91%)], Train Loss: 1.84043\n",
      "Epoch: 05 [224064/244490 ( 92%)], Train Loss: 1.83976\n",
      "Epoch: 05 [224704/244490 ( 92%)], Train Loss: 1.83982\n",
      "Epoch: 05 [225344/244490 ( 92%)], Train Loss: 1.83968\n",
      "Epoch: 05 [225984/244490 ( 92%)], Train Loss: 1.83984\n",
      "Epoch: 05 [226624/244490 ( 93%)], Train Loss: 1.83990\n",
      "Epoch: 05 [227264/244490 ( 93%)], Train Loss: 1.83971\n",
      "Epoch: 05 [227904/244490 ( 93%)], Train Loss: 1.83932\n",
      "Epoch: 05 [228544/244490 ( 93%)], Train Loss: 1.83957\n",
      "Epoch: 05 [229184/244490 ( 94%)], Train Loss: 1.83955\n",
      "Epoch: 05 [229824/244490 ( 94%)], Train Loss: 1.83989\n",
      "Epoch: 05 [230464/244490 ( 94%)], Train Loss: 1.83994\n",
      "Epoch: 05 [231104/244490 ( 95%)], Train Loss: 1.83988\n",
      "Epoch: 05 [231744/244490 ( 95%)], Train Loss: 1.83990\n",
      "Epoch: 05 [232384/244490 ( 95%)], Train Loss: 1.83979\n",
      "Epoch: 05 [233024/244490 ( 95%)], Train Loss: 1.84005\n",
      "Epoch: 05 [233664/244490 ( 96%)], Train Loss: 1.83968\n",
      "Epoch: 05 [234304/244490 ( 96%)], Train Loss: 1.83961\n",
      "Epoch: 05 [234944/244490 ( 96%)], Train Loss: 1.83969\n",
      "Epoch: 05 [235584/244490 ( 96%)], Train Loss: 1.83964\n",
      "Epoch: 05 [236224/244490 ( 97%)], Train Loss: 1.83930\n",
      "Epoch: 05 [236864/244490 ( 97%)], Train Loss: 1.83923\n",
      "Epoch: 05 [237504/244490 ( 97%)], Train Loss: 1.83946\n",
      "Epoch: 05 [238144/244490 ( 97%)], Train Loss: 1.83933\n",
      "Epoch: 05 [238784/244490 ( 98%)], Train Loss: 1.83940\n",
      "Epoch: 05 [239424/244490 ( 98%)], Train Loss: 1.83954\n",
      "Epoch: 05 [240064/244490 ( 98%)], Train Loss: 1.83949\n",
      "Epoch: 05 [240704/244490 ( 98%)], Train Loss: 1.83962\n",
      "Epoch: 05 [241344/244490 ( 99%)], Train Loss: 1.83999\n",
      "Epoch: 05 [241984/244490 ( 99%)], Train Loss: 1.83968\n",
      "Epoch: 05 [242624/244490 ( 99%)], Train Loss: 1.83956\n",
      "Epoch: 05 [243264/244490 ( 99%)], Train Loss: 1.83966\n",
      "Epoch: 05 [243904/244490 (100%)], Train Loss: 1.83981\n",
      "Epoch: 05 [244490/244490 (100%)], Train Loss: 1.83937\n",
      "----Validation Results Summary----\n",
      "Epoch: [5] Valid Loss: 1.81569\n",
      "\n",
      "----SWA Validation Results Summary----\n",
      "Epoch: [5] Valid Loss: 1.70208\n",
      "\n",
      "Total Training Time: 13133.454667806625secs, Average Training Time per Epoch: 2188.909111301104secs.\n",
      "Total Validation Time: 1858.147828578949secs, Average Validation Time per Epoch: 309.69130476315814secs.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 1\n",
      "--------------------------------------------------\n",
      "Model pushed to 2 GPU(s), type NVIDIA GeForce RTX 3090.\n",
      "Num examples Train= 244490, Num examples Valid=61123\n",
      "Total Training Steps: 22926, Total Warmup Steps: 687\n",
      "Total Training Steps: 22926, Total Warmup Steps: 687, SWA Start Step: 4\n",
      "Epoch: 00 [    64/244490 (  0%)], Train Loss: 5.94211\n",
      "Epoch: 00 [   704/244490 (  0%)], Train Loss: 5.88869\n",
      "Epoch: 00 [  1344/244490 (  1%)], Train Loss: 5.73661\n",
      "Epoch: 00 [  1984/244490 (  1%)], Train Loss: 5.57853\n",
      "Epoch: 00 [  2624/244490 (  1%)], Train Loss: 5.18397\n",
      "Epoch: 00 [  3264/244490 (  1%)], Train Loss: 4.69001\n",
      "Epoch: 00 [  3904/244490 (  2%)], Train Loss: 4.33759\n",
      "Epoch: 00 [  4544/244490 (  2%)], Train Loss: 4.06456\n",
      "Epoch: 00 [  5184/244490 (  2%)], Train Loss: 3.85711\n",
      "Epoch: 00 [  5824/244490 (  2%)], Train Loss: 3.69979\n",
      "Epoch: 00 [  6464/244490 (  3%)], Train Loss: 3.58062\n",
      "Epoch: 00 [  7104/244490 (  3%)], Train Loss: 3.47411\n",
      "Epoch: 00 [  7744/244490 (  3%)], Train Loss: 3.38902\n",
      "Epoch: 00 [  8384/244490 (  3%)], Train Loss: 3.32039\n",
      "Epoch: 00 [  9024/244490 (  4%)], Train Loss: 3.26273\n",
      "Epoch: 00 [  9664/244490 (  4%)], Train Loss: 3.20720\n",
      "Epoch: 00 [ 10304/244490 (  4%)], Train Loss: 3.16108\n",
      "Epoch: 00 [ 10944/244490 (  4%)], Train Loss: 3.11616\n",
      "Epoch: 00 [ 11584/244490 (  5%)], Train Loss: 3.07955\n",
      "Epoch: 00 [ 12224/244490 (  5%)], Train Loss: 3.04253\n",
      "Epoch: 00 [ 12864/244490 (  5%)], Train Loss: 3.01312\n",
      "Epoch: 00 [ 13504/244490 (  6%)], Train Loss: 2.98690\n",
      "Epoch: 00 [ 14144/244490 (  6%)], Train Loss: 2.96733\n",
      "Epoch: 00 [ 14784/244490 (  6%)], Train Loss: 2.94302\n",
      "Epoch: 00 [ 15424/244490 (  6%)], Train Loss: 2.91799\n",
      "Epoch: 00 [ 16064/244490 (  7%)], Train Loss: 2.89578\n",
      "Epoch: 00 [ 16704/244490 (  7%)], Train Loss: 2.87226\n",
      "Epoch: 00 [ 17344/244490 (  7%)], Train Loss: 2.85501\n",
      "Epoch: 00 [ 17984/244490 (  7%)], Train Loss: 2.84046\n",
      "Epoch: 00 [ 18624/244490 (  8%)], Train Loss: 2.82463\n",
      "Epoch: 00 [ 19264/244490 (  8%)], Train Loss: 2.80905\n",
      "Epoch: 00 [ 19904/244490 (  8%)], Train Loss: 2.79547\n",
      "Epoch: 00 [ 20544/244490 (  8%)], Train Loss: 2.78300\n",
      "Epoch: 00 [ 21184/244490 (  9%)], Train Loss: 2.76820\n",
      "Epoch: 00 [ 21824/244490 (  9%)], Train Loss: 2.75573\n",
      "Epoch: 00 [ 22464/244490 (  9%)], Train Loss: 2.74503\n",
      "Epoch: 00 [ 23104/244490 (  9%)], Train Loss: 2.73420\n",
      "Epoch: 00 [ 23744/244490 ( 10%)], Train Loss: 2.72223\n",
      "Epoch: 00 [ 24384/244490 ( 10%)], Train Loss: 2.71249\n",
      "Epoch: 00 [ 25024/244490 ( 10%)], Train Loss: 2.70363\n",
      "Epoch: 00 [ 25664/244490 ( 10%)], Train Loss: 2.69173\n",
      "Epoch: 00 [ 26304/244490 ( 11%)], Train Loss: 2.68350\n",
      "Epoch: 00 [ 26944/244490 ( 11%)], Train Loss: 2.67194\n",
      "Epoch: 00 [ 27584/244490 ( 11%)], Train Loss: 2.66281\n",
      "Epoch: 00 [ 28224/244490 ( 12%)], Train Loss: 2.65327\n",
      "Epoch: 00 [ 28864/244490 ( 12%)], Train Loss: 2.64621\n",
      "Epoch: 00 [ 29504/244490 ( 12%)], Train Loss: 2.63696\n",
      "Epoch: 00 [ 30144/244490 ( 12%)], Train Loss: 2.62809\n",
      "Epoch: 00 [ 30784/244490 ( 13%)], Train Loss: 2.61955\n",
      "Epoch: 00 [ 31424/244490 ( 13%)], Train Loss: 2.61503\n",
      "Epoch: 00 [ 32064/244490 ( 13%)], Train Loss: 2.61042\n",
      "Epoch: 00 [ 32704/244490 ( 13%)], Train Loss: 2.60292\n",
      "Epoch: 00 [ 33344/244490 ( 14%)], Train Loss: 2.59738\n",
      "Epoch: 00 [ 33984/244490 ( 14%)], Train Loss: 2.58971\n",
      "Epoch: 00 [ 34624/244490 ( 14%)], Train Loss: 2.58582\n",
      "Epoch: 00 [ 35264/244490 ( 14%)], Train Loss: 2.58086\n",
      "Epoch: 00 [ 35904/244490 ( 15%)], Train Loss: 2.57778\n",
      "Epoch: 00 [ 36544/244490 ( 15%)], Train Loss: 2.57396\n",
      "Epoch: 00 [ 37184/244490 ( 15%)], Train Loss: 2.56843\n",
      "Epoch: 00 [ 37824/244490 ( 15%)], Train Loss: 2.56398\n",
      "Epoch: 00 [ 38464/244490 ( 16%)], Train Loss: 2.55827\n",
      "Epoch: 00 [ 39104/244490 ( 16%)], Train Loss: 2.55320\n",
      "Epoch: 00 [ 39744/244490 ( 16%)], Train Loss: 2.54725\n",
      "Epoch: 00 [ 40384/244490 ( 17%)], Train Loss: 2.54270\n",
      "Epoch: 00 [ 41024/244490 ( 17%)], Train Loss: 2.53857\n",
      "Epoch: 00 [ 41664/244490 ( 17%)], Train Loss: 2.53415\n",
      "Epoch: 00 [ 42304/244490 ( 17%)], Train Loss: 2.53162\n",
      "Epoch: 00 [ 42944/244490 ( 18%)], Train Loss: 2.52728\n",
      "Epoch: 00 [ 43584/244490 ( 18%)], Train Loss: 2.52195\n",
      "Epoch: 00 [ 44224/244490 ( 18%)], Train Loss: 2.51882\n",
      "Epoch: 00 [ 44864/244490 ( 18%)], Train Loss: 2.51493\n",
      "Epoch: 00 [ 45504/244490 ( 19%)], Train Loss: 2.51216\n",
      "Epoch: 00 [ 46144/244490 ( 19%)], Train Loss: 2.50801\n",
      "Epoch: 00 [ 46784/244490 ( 19%)], Train Loss: 2.50348\n",
      "Epoch: 00 [ 47424/244490 ( 19%)], Train Loss: 2.49801\n",
      "Epoch: 00 [ 48064/244490 ( 20%)], Train Loss: 2.49377\n",
      "Epoch: 00 [ 48704/244490 ( 20%)], Train Loss: 2.49053\n",
      "Epoch: 00 [ 49344/244490 ( 20%)], Train Loss: 2.48743\n",
      "Epoch: 00 [ 49984/244490 ( 20%)], Train Loss: 2.48524\n",
      "Epoch: 00 [ 50624/244490 ( 21%)], Train Loss: 2.48206\n",
      "Epoch: 00 [ 51264/244490 ( 21%)], Train Loss: 2.47766\n",
      "Epoch: 00 [ 51904/244490 ( 21%)], Train Loss: 2.47464\n",
      "Epoch: 00 [ 52544/244490 ( 21%)], Train Loss: 2.47168\n",
      "Epoch: 00 [ 53184/244490 ( 22%)], Train Loss: 2.46873\n",
      "Epoch: 00 [ 53824/244490 ( 22%)], Train Loss: 2.46484\n",
      "Epoch: 00 [ 54464/244490 ( 22%)], Train Loss: 2.46136\n",
      "Epoch: 00 [ 55104/244490 ( 23%)], Train Loss: 2.45884\n",
      "Epoch: 00 [ 55744/244490 ( 23%)], Train Loss: 2.45652\n",
      "Epoch: 00 [ 56384/244490 ( 23%)], Train Loss: 2.45416\n",
      "Epoch: 00 [ 57024/244490 ( 23%)], Train Loss: 2.45131\n",
      "Epoch: 00 [ 57664/244490 ( 24%)], Train Loss: 2.44794\n",
      "Epoch: 00 [ 58304/244490 ( 24%)], Train Loss: 2.44534\n",
      "Epoch: 00 [ 58944/244490 ( 24%)], Train Loss: 2.44113\n",
      "Epoch: 00 [ 59584/244490 ( 24%)], Train Loss: 2.43786\n",
      "Epoch: 00 [ 60224/244490 ( 25%)], Train Loss: 2.43616\n",
      "Epoch: 00 [ 60864/244490 ( 25%)], Train Loss: 2.43317\n",
      "Epoch: 00 [ 61504/244490 ( 25%)], Train Loss: 2.43039\n",
      "Epoch: 00 [ 62144/244490 ( 25%)], Train Loss: 2.42685\n",
      "Epoch: 00 [ 62784/244490 ( 26%)], Train Loss: 2.42395\n",
      "Epoch: 00 [ 63424/244490 ( 26%)], Train Loss: 2.42161\n",
      "Epoch: 00 [ 64064/244490 ( 26%)], Train Loss: 2.41802\n",
      "Epoch: 00 [ 64704/244490 ( 26%)], Train Loss: 2.41711\n",
      "Epoch: 00 [ 65344/244490 ( 27%)], Train Loss: 2.41516\n",
      "Epoch: 00 [ 65984/244490 ( 27%)], Train Loss: 2.41312\n",
      "Epoch: 00 [ 66624/244490 ( 27%)], Train Loss: 2.41029\n",
      "Epoch: 00 [ 67264/244490 ( 28%)], Train Loss: 2.40738\n",
      "Epoch: 00 [ 67904/244490 ( 28%)], Train Loss: 2.40519\n",
      "Epoch: 00 [ 68544/244490 ( 28%)], Train Loss: 2.40117\n",
      "Epoch: 00 [ 69184/244490 ( 28%)], Train Loss: 2.39937\n",
      "Epoch: 00 [ 69824/244490 ( 29%)], Train Loss: 2.39741\n",
      "Epoch: 00 [ 70464/244490 ( 29%)], Train Loss: 2.39381\n",
      "Epoch: 00 [ 71104/244490 ( 29%)], Train Loss: 2.39175\n",
      "Epoch: 00 [ 71744/244490 ( 29%)], Train Loss: 2.38936\n",
      "Epoch: 00 [ 72384/244490 ( 30%)], Train Loss: 2.38862\n",
      "Epoch: 00 [ 73024/244490 ( 30%)], Train Loss: 2.38752\n",
      "Epoch: 00 [ 73664/244490 ( 30%)], Train Loss: 2.38521\n",
      "Epoch: 00 [ 74304/244490 ( 30%)], Train Loss: 2.38291\n",
      "Epoch: 00 [ 74944/244490 ( 31%)], Train Loss: 2.38053\n",
      "Epoch: 00 [ 75584/244490 ( 31%)], Train Loss: 2.37943\n",
      "Epoch: 00 [ 76224/244490 ( 31%)], Train Loss: 2.37882\n",
      "Epoch: 00 [ 76864/244490 ( 31%)], Train Loss: 2.37726\n",
      "Epoch: 00 [ 77504/244490 ( 32%)], Train Loss: 2.37465\n",
      "Epoch: 00 [ 78144/244490 ( 32%)], Train Loss: 2.37177\n",
      "Epoch: 00 [ 78784/244490 ( 32%)], Train Loss: 2.36960\n",
      "Epoch: 00 [ 79424/244490 ( 32%)], Train Loss: 2.36812\n",
      "Epoch: 00 [ 80064/244490 ( 33%)], Train Loss: 2.36633\n",
      "Epoch: 00 [ 80704/244490 ( 33%)], Train Loss: 2.36591\n",
      "Epoch: 00 [ 81344/244490 ( 33%)], Train Loss: 2.36485\n",
      "Epoch: 00 [ 81984/244490 ( 34%)], Train Loss: 2.36245\n",
      "Epoch: 00 [ 82624/244490 ( 34%)], Train Loss: 2.36089\n",
      "Epoch: 00 [ 83264/244490 ( 34%)], Train Loss: 2.35836\n",
      "Epoch: 00 [ 83904/244490 ( 34%)], Train Loss: 2.35622\n",
      "Epoch: 00 [ 84544/244490 ( 35%)], Train Loss: 2.35403\n",
      "Epoch: 00 [ 85184/244490 ( 35%)], Train Loss: 2.35216\n",
      "Epoch: 00 [ 85824/244490 ( 35%)], Train Loss: 2.35085\n",
      "Epoch: 00 [ 86464/244490 ( 35%)], Train Loss: 2.34924\n",
      "Epoch: 00 [ 87104/244490 ( 36%)], Train Loss: 2.34808\n",
      "Epoch: 00 [ 87744/244490 ( 36%)], Train Loss: 2.34664\n",
      "Epoch: 00 [ 88384/244490 ( 36%)], Train Loss: 2.34467\n",
      "Epoch: 00 [ 89024/244490 ( 36%)], Train Loss: 2.34264\n",
      "Epoch: 00 [ 89664/244490 ( 37%)], Train Loss: 2.34142\n",
      "Epoch: 00 [ 90304/244490 ( 37%)], Train Loss: 2.34007\n",
      "Epoch: 00 [ 90944/244490 ( 37%)], Train Loss: 2.33900\n",
      "Epoch: 00 [ 91584/244490 ( 37%)], Train Loss: 2.33806\n",
      "Epoch: 00 [ 92224/244490 ( 38%)], Train Loss: 2.33649\n",
      "Epoch: 00 [ 92864/244490 ( 38%)], Train Loss: 2.33507\n",
      "Epoch: 00 [ 93504/244490 ( 38%)], Train Loss: 2.33356\n",
      "Epoch: 00 [ 94144/244490 ( 39%)], Train Loss: 2.33217\n",
      "Epoch: 00 [ 94784/244490 ( 39%)], Train Loss: 2.33096\n",
      "Epoch: 00 [ 95424/244490 ( 39%)], Train Loss: 2.33008\n",
      "Epoch: 00 [ 96064/244490 ( 39%)], Train Loss: 2.32826\n",
      "Epoch: 00 [ 96704/244490 ( 40%)], Train Loss: 2.32665\n",
      "Epoch: 00 [ 97344/244490 ( 40%)], Train Loss: 2.32410\n",
      "Epoch: 00 [ 97984/244490 ( 40%)], Train Loss: 2.32292\n",
      "Epoch: 00 [ 98624/244490 ( 40%)], Train Loss: 2.32147\n",
      "Epoch: 00 [ 99264/244490 ( 41%)], Train Loss: 2.32001\n",
      "Epoch: 00 [ 99904/244490 ( 41%)], Train Loss: 2.31988\n",
      "Epoch: 00 [100544/244490 ( 41%)], Train Loss: 2.31882\n",
      "Epoch: 00 [101184/244490 ( 41%)], Train Loss: 2.31783\n",
      "Epoch: 00 [101824/244490 ( 42%)], Train Loss: 2.31637\n",
      "Epoch: 00 [102464/244490 ( 42%)], Train Loss: 2.31452\n",
      "Epoch: 00 [103104/244490 ( 42%)], Train Loss: 2.31400\n",
      "Epoch: 00 [103744/244490 ( 42%)], Train Loss: 2.31263\n",
      "Epoch: 00 [104384/244490 ( 43%)], Train Loss: 2.31090\n",
      "Epoch: 00 [105024/244490 ( 43%)], Train Loss: 2.30913\n",
      "Epoch: 00 [105664/244490 ( 43%)], Train Loss: 2.30874\n",
      "Epoch: 00 [106304/244490 ( 43%)], Train Loss: 2.30687\n",
      "Epoch: 00 [106944/244490 ( 44%)], Train Loss: 2.30490\n",
      "Epoch: 00 [107584/244490 ( 44%)], Train Loss: 2.30397\n",
      "Epoch: 00 [108224/244490 ( 44%)], Train Loss: 2.30315\n",
      "Epoch: 00 [108864/244490 ( 45%)], Train Loss: 2.30180\n",
      "Epoch: 00 [109504/244490 ( 45%)], Train Loss: 2.30047\n",
      "Epoch: 00 [110144/244490 ( 45%)], Train Loss: 2.29916\n",
      "Epoch: 00 [110784/244490 ( 45%)], Train Loss: 2.29721\n",
      "Epoch: 00 [111424/244490 ( 46%)], Train Loss: 2.29581\n",
      "Epoch: 00 [112064/244490 ( 46%)], Train Loss: 2.29366\n",
      "Epoch: 00 [112704/244490 ( 46%)], Train Loss: 2.29306\n",
      "Epoch: 00 [113344/244490 ( 46%)], Train Loss: 2.29182\n",
      "Epoch: 00 [113984/244490 ( 47%)], Train Loss: 2.29052\n",
      "Epoch: 00 [114624/244490 ( 47%)], Train Loss: 2.28899\n",
      "Epoch: 00 [115264/244490 ( 47%)], Train Loss: 2.28753\n",
      "Epoch: 00 [115904/244490 ( 47%)], Train Loss: 2.28627\n",
      "Epoch: 00 [116544/244490 ( 48%)], Train Loss: 2.28486\n",
      "Epoch: 00 [117184/244490 ( 48%)], Train Loss: 2.28316\n",
      "Epoch: 00 [117824/244490 ( 48%)], Train Loss: 2.28163\n",
      "Epoch: 00 [118464/244490 ( 48%)], Train Loss: 2.27985\n",
      "Epoch: 00 [119104/244490 ( 49%)], Train Loss: 2.27852\n",
      "Epoch: 00 [119744/244490 ( 49%)], Train Loss: 2.27759\n",
      "Epoch: 00 [120384/244490 ( 49%)], Train Loss: 2.27674\n",
      "Epoch: 00 [121024/244490 ( 50%)], Train Loss: 2.27589\n",
      "Epoch: 00 [121664/244490 ( 50%)], Train Loss: 2.27488\n",
      "Epoch: 00 [122304/244490 ( 50%)], Train Loss: 2.27366\n",
      "Epoch: 00 [122944/244490 ( 50%)], Train Loss: 2.27277\n",
      "Epoch: 00 [123584/244490 ( 51%)], Train Loss: 2.27133\n",
      "Epoch: 00 [124224/244490 ( 51%)], Train Loss: 2.26989\n",
      "Epoch: 00 [124864/244490 ( 51%)], Train Loss: 2.26855\n",
      "Epoch: 00 [125504/244490 ( 51%)], Train Loss: 2.26817\n",
      "Epoch: 00 [126144/244490 ( 52%)], Train Loss: 2.26751\n",
      "Epoch: 00 [126784/244490 ( 52%)], Train Loss: 2.26646\n",
      "Epoch: 00 [127424/244490 ( 52%)], Train Loss: 2.26528\n",
      "Epoch: 00 [128064/244490 ( 52%)], Train Loss: 2.26410\n",
      "Epoch: 00 [128704/244490 ( 53%)], Train Loss: 2.26310\n",
      "Epoch: 00 [129344/244490 ( 53%)], Train Loss: 2.26205\n",
      "Epoch: 00 [129984/244490 ( 53%)], Train Loss: 2.26125\n",
      "Epoch: 00 [130624/244490 ( 53%)], Train Loss: 2.25995\n",
      "Epoch: 00 [131264/244490 ( 54%)], Train Loss: 2.25875\n",
      "Epoch: 00 [131904/244490 ( 54%)], Train Loss: 2.25797\n",
      "Epoch: 00 [132544/244490 ( 54%)], Train Loss: 2.25671\n",
      "Epoch: 00 [133184/244490 ( 54%)], Train Loss: 2.25580\n",
      "Epoch: 00 [133824/244490 ( 55%)], Train Loss: 2.25448\n",
      "Epoch: 00 [134464/244490 ( 55%)], Train Loss: 2.25280\n",
      "Epoch: 00 [135104/244490 ( 55%)], Train Loss: 2.25264\n",
      "Epoch: 00 [135744/244490 ( 56%)], Train Loss: 2.25192\n",
      "Epoch: 00 [136384/244490 ( 56%)], Train Loss: 2.25095\n",
      "Epoch: 00 [137024/244490 ( 56%)], Train Loss: 2.24984\n",
      "Epoch: 00 [137664/244490 ( 56%)], Train Loss: 2.24873\n",
      "Epoch: 00 [138304/244490 ( 57%)], Train Loss: 2.24794\n",
      "Epoch: 00 [138944/244490 ( 57%)], Train Loss: 2.24669\n",
      "Epoch: 00 [139584/244490 ( 57%)], Train Loss: 2.24519\n",
      "Epoch: 00 [140224/244490 ( 57%)], Train Loss: 2.24390\n",
      "Epoch: 00 [140864/244490 ( 58%)], Train Loss: 2.24304\n",
      "Epoch: 00 [141504/244490 ( 58%)], Train Loss: 2.24233\n",
      "Epoch: 00 [142144/244490 ( 58%)], Train Loss: 2.24122\n",
      "Epoch: 00 [142784/244490 ( 58%)], Train Loss: 2.24074\n",
      "Epoch: 00 [143424/244490 ( 59%)], Train Loss: 2.23985\n",
      "Epoch: 00 [144064/244490 ( 59%)], Train Loss: 2.23831\n",
      "Epoch: 00 [144704/244490 ( 59%)], Train Loss: 2.23694\n",
      "Epoch: 00 [145344/244490 ( 59%)], Train Loss: 2.23613\n",
      "Epoch: 00 [145984/244490 ( 60%)], Train Loss: 2.23519\n",
      "Epoch: 00 [146624/244490 ( 60%)], Train Loss: 2.23454\n",
      "Epoch: 00 [147264/244490 ( 60%)], Train Loss: 2.23315\n",
      "Epoch: 00 [147904/244490 ( 60%)], Train Loss: 2.23161\n",
      "Epoch: 00 [148544/244490 ( 61%)], Train Loss: 2.23098\n",
      "Epoch: 00 [149184/244490 ( 61%)], Train Loss: 2.23005\n",
      "Epoch: 00 [149824/244490 ( 61%)], Train Loss: 2.22934\n",
      "Epoch: 00 [150464/244490 ( 62%)], Train Loss: 2.22824\n",
      "Epoch: 00 [151104/244490 ( 62%)], Train Loss: 2.22794\n",
      "Epoch: 00 [151744/244490 ( 62%)], Train Loss: 2.22709\n",
      "Epoch: 00 [152384/244490 ( 62%)], Train Loss: 2.22606\n",
      "Epoch: 00 [153024/244490 ( 63%)], Train Loss: 2.22558\n",
      "Epoch: 00 [153664/244490 ( 63%)], Train Loss: 2.22451\n",
      "Epoch: 00 [154304/244490 ( 63%)], Train Loss: 2.22374\n",
      "Epoch: 00 [154944/244490 ( 63%)], Train Loss: 2.22256\n",
      "Epoch: 00 [155584/244490 ( 64%)], Train Loss: 2.22175\n",
      "Epoch: 00 [156224/244490 ( 64%)], Train Loss: 2.22055\n",
      "Epoch: 00 [156864/244490 ( 64%)], Train Loss: 2.21937\n",
      "Epoch: 00 [157504/244490 ( 64%)], Train Loss: 2.21839\n",
      "Epoch: 00 [158144/244490 ( 65%)], Train Loss: 2.21807\n",
      "Epoch: 00 [158784/244490 ( 65%)], Train Loss: 2.21726\n",
      "Epoch: 00 [159424/244490 ( 65%)], Train Loss: 2.21661\n",
      "Epoch: 00 [160064/244490 ( 65%)], Train Loss: 2.21584\n",
      "Epoch: 00 [160704/244490 ( 66%)], Train Loss: 2.21459\n",
      "Epoch: 00 [161344/244490 ( 66%)], Train Loss: 2.21358\n",
      "Epoch: 00 [161984/244490 ( 66%)], Train Loss: 2.21285\n",
      "Epoch: 00 [162624/244490 ( 67%)], Train Loss: 2.21227\n",
      "Epoch: 00 [163264/244490 ( 67%)], Train Loss: 2.21165\n",
      "Epoch: 00 [163904/244490 ( 67%)], Train Loss: 2.21124\n",
      "Epoch: 00 [164544/244490 ( 67%)], Train Loss: 2.20956\n",
      "Epoch: 00 [165184/244490 ( 68%)], Train Loss: 2.20876\n",
      "Epoch: 00 [165824/244490 ( 68%)], Train Loss: 2.20819\n",
      "Epoch: 00 [166464/244490 ( 68%)], Train Loss: 2.20718\n",
      "Epoch: 00 [167104/244490 ( 68%)], Train Loss: 2.20663\n",
      "Epoch: 00 [167744/244490 ( 69%)], Train Loss: 2.20611\n",
      "Epoch: 00 [168384/244490 ( 69%)], Train Loss: 2.20523\n",
      "Epoch: 00 [169024/244490 ( 69%)], Train Loss: 2.20442\n",
      "Epoch: 00 [169664/244490 ( 69%)], Train Loss: 2.20352\n",
      "Epoch: 00 [170304/244490 ( 70%)], Train Loss: 2.20279\n",
      "Epoch: 00 [170944/244490 ( 70%)], Train Loss: 2.20254\n",
      "Epoch: 00 [171584/244490 ( 70%)], Train Loss: 2.20146\n",
      "Epoch: 00 [172224/244490 ( 70%)], Train Loss: 2.20057\n",
      "Epoch: 00 [172864/244490 ( 71%)], Train Loss: 2.19984\n",
      "Epoch: 00 [173504/244490 ( 71%)], Train Loss: 2.19887\n",
      "Epoch: 00 [174144/244490 ( 71%)], Train Loss: 2.19829\n",
      "Epoch: 00 [174784/244490 ( 71%)], Train Loss: 2.19768\n",
      "Epoch: 00 [175424/244490 ( 72%)], Train Loss: 2.19709\n",
      "Epoch: 00 [176064/244490 ( 72%)], Train Loss: 2.19625\n",
      "Epoch: 00 [176704/244490 ( 72%)], Train Loss: 2.19552\n",
      "Epoch: 00 [177344/244490 ( 73%)], Train Loss: 2.19533\n",
      "Epoch: 00 [177984/244490 ( 73%)], Train Loss: 2.19468\n",
      "Epoch: 00 [178624/244490 ( 73%)], Train Loss: 2.19363\n",
      "Epoch: 00 [179264/244490 ( 73%)], Train Loss: 2.19333\n",
      "Epoch: 00 [179904/244490 ( 74%)], Train Loss: 2.19246\n",
      "Epoch: 00 [180544/244490 ( 74%)], Train Loss: 2.19156\n",
      "Epoch: 00 [181184/244490 ( 74%)], Train Loss: 2.19046\n",
      "Epoch: 00 [181824/244490 ( 74%)], Train Loss: 2.18933\n",
      "Epoch: 00 [182464/244490 ( 75%)], Train Loss: 2.18862\n",
      "Epoch: 00 [183104/244490 ( 75%)], Train Loss: 2.18752\n",
      "Epoch: 00 [183744/244490 ( 75%)], Train Loss: 2.18680\n",
      "Epoch: 00 [184384/244490 ( 75%)], Train Loss: 2.18635\n",
      "Epoch: 00 [185024/244490 ( 76%)], Train Loss: 2.18569\n",
      "Epoch: 00 [185664/244490 ( 76%)], Train Loss: 2.18484\n",
      "Epoch: 00 [186304/244490 ( 76%)], Train Loss: 2.18463\n",
      "Epoch: 00 [186944/244490 ( 76%)], Train Loss: 2.18376\n",
      "Epoch: 00 [187584/244490 ( 77%)], Train Loss: 2.18314\n",
      "Epoch: 00 [188224/244490 ( 77%)], Train Loss: 2.18240\n",
      "Epoch: 00 [188864/244490 ( 77%)], Train Loss: 2.18182\n",
      "Epoch: 00 [189504/244490 ( 78%)], Train Loss: 2.18110\n",
      "Epoch: 00 [190144/244490 ( 78%)], Train Loss: 2.18016\n",
      "Epoch: 00 [190784/244490 ( 78%)], Train Loss: 2.17939\n",
      "Epoch: 00 [191424/244490 ( 78%)], Train Loss: 2.17852\n",
      "Epoch: 00 [192064/244490 ( 79%)], Train Loss: 2.17802\n",
      "Epoch: 00 [192704/244490 ( 79%)], Train Loss: 2.17738\n",
      "Epoch: 00 [193344/244490 ( 79%)], Train Loss: 2.17662\n",
      "Epoch: 00 [193984/244490 ( 79%)], Train Loss: 2.17593\n",
      "Epoch: 00 [194624/244490 ( 80%)], Train Loss: 2.17531\n",
      "Epoch: 00 [195264/244490 ( 80%)], Train Loss: 2.17454\n",
      "Epoch: 00 [195904/244490 ( 80%)], Train Loss: 2.17387\n",
      "Epoch: 00 [196544/244490 ( 80%)], Train Loss: 2.17303\n",
      "Epoch: 00 [197184/244490 ( 81%)], Train Loss: 2.17250\n",
      "Epoch: 00 [197824/244490 ( 81%)], Train Loss: 2.17201\n",
      "Epoch: 00 [198464/244490 ( 81%)], Train Loss: 2.17165\n",
      "Epoch: 00 [199104/244490 ( 81%)], Train Loss: 2.17120\n",
      "Epoch: 00 [199744/244490 ( 82%)], Train Loss: 2.17050\n",
      "Epoch: 00 [200384/244490 ( 82%)], Train Loss: 2.16970\n",
      "Epoch: 00 [201024/244490 ( 82%)], Train Loss: 2.16926\n",
      "Epoch: 00 [201664/244490 ( 82%)], Train Loss: 2.16850\n",
      "Epoch: 00 [202304/244490 ( 83%)], Train Loss: 2.16766\n",
      "Epoch: 00 [202944/244490 ( 83%)], Train Loss: 2.16705\n",
      "Epoch: 00 [203584/244490 ( 83%)], Train Loss: 2.16668\n",
      "Epoch: 00 [204224/244490 ( 84%)], Train Loss: 2.16545\n",
      "Epoch: 00 [204864/244490 ( 84%)], Train Loss: 2.16477\n",
      "Epoch: 00 [205504/244490 ( 84%)], Train Loss: 2.16463\n",
      "Epoch: 00 [206144/244490 ( 84%)], Train Loss: 2.16420\n",
      "Epoch: 00 [206784/244490 ( 85%)], Train Loss: 2.16343\n",
      "Epoch: 00 [207424/244490 ( 85%)], Train Loss: 2.16255\n",
      "Epoch: 00 [208064/244490 ( 85%)], Train Loss: 2.16190\n",
      "Epoch: 00 [208704/244490 ( 85%)], Train Loss: 2.16121\n",
      "Epoch: 00 [209344/244490 ( 86%)], Train Loss: 2.16055\n",
      "Epoch: 00 [209984/244490 ( 86%)], Train Loss: 2.15987\n",
      "Epoch: 00 [210624/244490 ( 86%)], Train Loss: 2.15922\n",
      "Epoch: 00 [211264/244490 ( 86%)], Train Loss: 2.15889\n",
      "Epoch: 00 [211904/244490 ( 87%)], Train Loss: 2.15861\n",
      "Epoch: 00 [212544/244490 ( 87%)], Train Loss: 2.15817\n",
      "Epoch: 00 [213184/244490 ( 87%)], Train Loss: 2.15770\n",
      "Epoch: 00 [213824/244490 ( 87%)], Train Loss: 2.15707\n",
      "Epoch: 00 [214464/244490 ( 88%)], Train Loss: 2.15640\n",
      "Epoch: 00 [215104/244490 ( 88%)], Train Loss: 2.15563\n",
      "Epoch: 00 [215744/244490 ( 88%)], Train Loss: 2.15510\n",
      "Epoch: 00 [216384/244490 ( 89%)], Train Loss: 2.15513\n",
      "Epoch: 00 [217024/244490 ( 89%)], Train Loss: 2.15488\n",
      "Epoch: 00 [217664/244490 ( 89%)], Train Loss: 2.15438\n",
      "Epoch: 00 [218304/244490 ( 89%)], Train Loss: 2.15383\n",
      "Epoch: 00 [218944/244490 ( 90%)], Train Loss: 2.15329\n",
      "Epoch: 00 [219584/244490 ( 90%)], Train Loss: 2.15270\n",
      "Epoch: 00 [220224/244490 ( 90%)], Train Loss: 2.15168\n",
      "Epoch: 00 [220864/244490 ( 90%)], Train Loss: 2.15126\n",
      "Epoch: 00 [221504/244490 ( 91%)], Train Loss: 2.15112\n",
      "Epoch: 00 [222144/244490 ( 91%)], Train Loss: 2.15101\n",
      "Epoch: 00 [222784/244490 ( 91%)], Train Loss: 2.15053\n",
      "Epoch: 00 [223424/244490 ( 91%)], Train Loss: 2.15018\n",
      "Epoch: 00 [224064/244490 ( 92%)], Train Loss: 2.14925\n",
      "Epoch: 00 [224704/244490 ( 92%)], Train Loss: 2.14837\n",
      "Epoch: 00 [225344/244490 ( 92%)], Train Loss: 2.14802\n",
      "Epoch: 00 [225984/244490 ( 92%)], Train Loss: 2.14734\n",
      "Epoch: 00 [226624/244490 ( 93%)], Train Loss: 2.14700\n",
      "Epoch: 00 [227264/244490 ( 93%)], Train Loss: 2.14651\n",
      "Epoch: 00 [227904/244490 ( 93%)], Train Loss: 2.14576\n",
      "Epoch: 00 [228544/244490 ( 93%)], Train Loss: 2.14503\n",
      "Epoch: 00 [229184/244490 ( 94%)], Train Loss: 2.14459\n",
      "Epoch: 00 [229824/244490 ( 94%)], Train Loss: 2.14397\n",
      "Epoch: 00 [230464/244490 ( 94%)], Train Loss: 2.14365\n",
      "Epoch: 00 [231104/244490 ( 95%)], Train Loss: 2.14304\n",
      "Epoch: 00 [231744/244490 ( 95%)], Train Loss: 2.14267\n",
      "Epoch: 00 [232384/244490 ( 95%)], Train Loss: 2.14241\n",
      "Epoch: 00 [233024/244490 ( 95%)], Train Loss: 2.14168\n",
      "Epoch: 00 [233664/244490 ( 96%)], Train Loss: 2.14105\n",
      "Epoch: 00 [234304/244490 ( 96%)], Train Loss: 2.14053\n",
      "Epoch: 00 [234944/244490 ( 96%)], Train Loss: 2.13996\n",
      "Epoch: 00 [235584/244490 ( 96%)], Train Loss: 2.13931\n",
      "Epoch: 00 [236224/244490 ( 97%)], Train Loss: 2.13854\n",
      "Epoch: 00 [236864/244490 ( 97%)], Train Loss: 2.13801\n",
      "Epoch: 00 [237504/244490 ( 97%)], Train Loss: 2.13748\n",
      "Epoch: 00 [238144/244490 ( 97%)], Train Loss: 2.13681\n",
      "Epoch: 00 [238784/244490 ( 98%)], Train Loss: 2.13656\n",
      "Epoch: 00 [239424/244490 ( 98%)], Train Loss: 2.13616\n",
      "Epoch: 00 [240064/244490 ( 98%)], Train Loss: 2.13553\n",
      "Epoch: 00 [240704/244490 ( 98%)], Train Loss: 2.13522\n",
      "Epoch: 00 [241344/244490 ( 99%)], Train Loss: 2.13471\n",
      "Epoch: 00 [241984/244490 ( 99%)], Train Loss: 2.13409\n",
      "Epoch: 00 [242624/244490 ( 99%)], Train Loss: 2.13357\n",
      "Epoch: 00 [243264/244490 ( 99%)], Train Loss: 2.13322\n",
      "Epoch: 00 [243904/244490 (100%)], Train Loss: 2.13262\n",
      "Epoch: 00 [244490/244490 (100%)], Train Loss: 2.13220\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 1.85560\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 1.85560\n",
      "Saving model checkpoint to output/checkpoint-fold-1.\n",
      "\n",
      "Epoch: 01 [    64/244490 (  0%)], Train Loss: 1.68864\n",
      "Epoch: 01 [   704/244490 (  0%)], Train Loss: 1.89035\n",
      "Epoch: 01 [  1344/244490 (  1%)], Train Loss: 1.83674\n",
      "Epoch: 01 [  1984/244490 (  1%)], Train Loss: 1.87414\n",
      "Epoch: 01 [  2624/244490 (  1%)], Train Loss: 1.88533\n",
      "Epoch: 01 [  3264/244490 (  1%)], Train Loss: 1.88888\n",
      "Epoch: 01 [  3904/244490 (  2%)], Train Loss: 1.89590\n",
      "Epoch: 01 [  4544/244490 (  2%)], Train Loss: 1.90369\n",
      "Epoch: 01 [  5184/244490 (  2%)], Train Loss: 1.90152\n",
      "Epoch: 01 [  5824/244490 (  2%)], Train Loss: 1.90463\n",
      "Epoch: 01 [  6464/244490 (  3%)], Train Loss: 1.91694\n",
      "Epoch: 01 [  7104/244490 (  3%)], Train Loss: 1.91800\n",
      "Epoch: 01 [  7744/244490 (  3%)], Train Loss: 1.91782\n",
      "Epoch: 01 [  8384/244490 (  3%)], Train Loss: 1.92591\n",
      "Epoch: 01 [  9024/244490 (  4%)], Train Loss: 1.93135\n",
      "Epoch: 01 [  9664/244490 (  4%)], Train Loss: 1.93502\n",
      "Epoch: 01 [ 10304/244490 (  4%)], Train Loss: 1.93660\n",
      "Epoch: 01 [ 10944/244490 (  4%)], Train Loss: 1.93344\n",
      "Epoch: 01 [ 11584/244490 (  5%)], Train Loss: 1.93423\n",
      "Epoch: 01 [ 12224/244490 (  5%)], Train Loss: 1.93348\n",
      "Epoch: 01 [ 12864/244490 (  5%)], Train Loss: 1.93761\n",
      "Epoch: 01 [ 13504/244490 (  6%)], Train Loss: 1.94529\n",
      "Epoch: 01 [ 14144/244490 (  6%)], Train Loss: 1.94816\n",
      "Epoch: 01 [ 14784/244490 (  6%)], Train Loss: 1.94592\n",
      "Epoch: 01 [ 15424/244490 (  6%)], Train Loss: 1.94207\n",
      "Epoch: 01 [ 16064/244490 (  7%)], Train Loss: 1.94186\n",
      "Epoch: 01 [ 16704/244490 (  7%)], Train Loss: 1.93935\n",
      "Epoch: 01 [ 17344/244490 (  7%)], Train Loss: 1.93812\n",
      "Epoch: 01 [ 17984/244490 (  7%)], Train Loss: 1.94363\n",
      "Epoch: 01 [ 18624/244490 (  8%)], Train Loss: 1.94356\n",
      "Epoch: 01 [ 19264/244490 (  8%)], Train Loss: 1.93945\n",
      "Epoch: 01 [ 19904/244490 (  8%)], Train Loss: 1.93772\n",
      "Epoch: 01 [ 20544/244490 (  8%)], Train Loss: 1.93920\n",
      "Epoch: 01 [ 21184/244490 (  9%)], Train Loss: 1.93760\n",
      "Epoch: 01 [ 21824/244490 (  9%)], Train Loss: 1.93699\n",
      "Epoch: 01 [ 22464/244490 (  9%)], Train Loss: 1.93747\n",
      "Epoch: 01 [ 23104/244490 (  9%)], Train Loss: 1.93794\n",
      "Epoch: 01 [ 23744/244490 ( 10%)], Train Loss: 1.93685\n",
      "Epoch: 01 [ 24384/244490 ( 10%)], Train Loss: 1.93430\n",
      "Epoch: 01 [ 25024/244490 ( 10%)], Train Loss: 1.93546\n",
      "Epoch: 01 [ 25664/244490 ( 10%)], Train Loss: 1.93308\n",
      "Epoch: 01 [ 26304/244490 ( 11%)], Train Loss: 1.93268\n",
      "Epoch: 01 [ 26944/244490 ( 11%)], Train Loss: 1.93036\n",
      "Epoch: 01 [ 27584/244490 ( 11%)], Train Loss: 1.92969\n",
      "Epoch: 01 [ 28224/244490 ( 12%)], Train Loss: 1.92762\n",
      "Epoch: 01 [ 28864/244490 ( 12%)], Train Loss: 1.92768\n",
      "Epoch: 01 [ 29504/244490 ( 12%)], Train Loss: 1.92518\n",
      "Epoch: 01 [ 30144/244490 ( 12%)], Train Loss: 1.92364\n",
      "Epoch: 01 [ 30784/244490 ( 13%)], Train Loss: 1.92282\n",
      "Epoch: 01 [ 31424/244490 ( 13%)], Train Loss: 1.92322\n",
      "Epoch: 01 [ 32064/244490 ( 13%)], Train Loss: 1.92486\n",
      "Epoch: 01 [ 32704/244490 ( 13%)], Train Loss: 1.92429\n",
      "Epoch: 01 [ 33344/244490 ( 14%)], Train Loss: 1.92443\n",
      "Epoch: 01 [ 33984/244490 ( 14%)], Train Loss: 1.92260\n",
      "Epoch: 01 [ 34624/244490 ( 14%)], Train Loss: 1.92397\n",
      "Epoch: 01 [ 35264/244490 ( 14%)], Train Loss: 1.92463\n",
      "Epoch: 01 [ 35904/244490 ( 15%)], Train Loss: 1.92601\n",
      "Epoch: 01 [ 36544/244490 ( 15%)], Train Loss: 1.92780\n",
      "Epoch: 01 [ 37184/244490 ( 15%)], Train Loss: 1.92726\n",
      "Epoch: 01 [ 37824/244490 ( 15%)], Train Loss: 1.92696\n",
      "Epoch: 01 [ 38464/244490 ( 16%)], Train Loss: 1.92615\n",
      "Epoch: 01 [ 39104/244490 ( 16%)], Train Loss: 1.92577\n",
      "Epoch: 01 [ 39744/244490 ( 16%)], Train Loss: 1.92560\n",
      "Epoch: 01 [ 40384/244490 ( 17%)], Train Loss: 1.92450\n",
      "Epoch: 01 [ 41024/244490 ( 17%)], Train Loss: 1.92352\n",
      "Epoch: 01 [ 41664/244490 ( 17%)], Train Loss: 1.92154\n",
      "Epoch: 01 [ 42304/244490 ( 17%)], Train Loss: 1.92247\n",
      "Epoch: 01 [ 42944/244490 ( 18%)], Train Loss: 1.92018\n",
      "Epoch: 01 [ 43584/244490 ( 18%)], Train Loss: 1.91757\n",
      "Epoch: 01 [ 44224/244490 ( 18%)], Train Loss: 1.91732\n",
      "Epoch: 01 [ 44864/244490 ( 18%)], Train Loss: 1.91643\n",
      "Epoch: 01 [ 45504/244490 ( 19%)], Train Loss: 1.91616\n",
      "Epoch: 01 [ 46144/244490 ( 19%)], Train Loss: 1.91369\n",
      "Epoch: 01 [ 46784/244490 ( 19%)], Train Loss: 1.91268\n",
      "Epoch: 01 [ 47424/244490 ( 19%)], Train Loss: 1.90992\n",
      "Epoch: 01 [ 48064/244490 ( 20%)], Train Loss: 1.90854\n",
      "Epoch: 01 [ 48704/244490 ( 20%)], Train Loss: 1.90898\n",
      "Epoch: 01 [ 49344/244490 ( 20%)], Train Loss: 1.90835\n",
      "Epoch: 01 [ 49984/244490 ( 20%)], Train Loss: 1.90885\n",
      "Epoch: 01 [ 50624/244490 ( 21%)], Train Loss: 1.90773\n",
      "Epoch: 01 [ 51264/244490 ( 21%)], Train Loss: 1.90653\n",
      "Epoch: 01 [ 51904/244490 ( 21%)], Train Loss: 1.90600\n",
      "Epoch: 01 [ 52544/244490 ( 21%)], Train Loss: 1.90512\n",
      "Epoch: 01 [ 53184/244490 ( 22%)], Train Loss: 1.90361\n",
      "Epoch: 01 [ 53824/244490 ( 22%)], Train Loss: 1.90252\n",
      "Epoch: 01 [ 54464/244490 ( 22%)], Train Loss: 1.90153\n",
      "Epoch: 01 [ 55104/244490 ( 23%)], Train Loss: 1.90119\n",
      "Epoch: 01 [ 55744/244490 ( 23%)], Train Loss: 1.90113\n",
      "Epoch: 01 [ 56384/244490 ( 23%)], Train Loss: 1.90042\n",
      "Epoch: 01 [ 57024/244490 ( 23%)], Train Loss: 1.90066\n",
      "Epoch: 01 [ 57664/244490 ( 24%)], Train Loss: 1.89977\n",
      "Epoch: 01 [ 58304/244490 ( 24%)], Train Loss: 1.89902\n",
      "Epoch: 01 [ 58944/244490 ( 24%)], Train Loss: 1.89774\n",
      "Epoch: 01 [ 59584/244490 ( 24%)], Train Loss: 1.89649\n",
      "Epoch: 01 [ 60224/244490 ( 25%)], Train Loss: 1.89723\n",
      "Epoch: 01 [ 60864/244490 ( 25%)], Train Loss: 1.89615\n",
      "Epoch: 01 [ 61504/244490 ( 25%)], Train Loss: 1.89563\n",
      "Epoch: 01 [ 62144/244490 ( 25%)], Train Loss: 1.89416\n",
      "Epoch: 01 [ 62784/244490 ( 26%)], Train Loss: 1.89333\n",
      "Epoch: 01 [ 63424/244490 ( 26%)], Train Loss: 1.89305\n",
      "Epoch: 01 [ 64064/244490 ( 26%)], Train Loss: 1.89131\n",
      "Epoch: 01 [ 64704/244490 ( 26%)], Train Loss: 1.89141\n",
      "Epoch: 01 [ 65344/244490 ( 27%)], Train Loss: 1.89107\n",
      "Epoch: 01 [ 65984/244490 ( 27%)], Train Loss: 1.89009\n",
      "Epoch: 01 [ 66624/244490 ( 27%)], Train Loss: 1.88914\n",
      "Epoch: 01 [ 67264/244490 ( 28%)], Train Loss: 1.88856\n",
      "Epoch: 01 [ 67904/244490 ( 28%)], Train Loss: 1.88775\n",
      "Epoch: 01 [ 68544/244490 ( 28%)], Train Loss: 1.88553\n",
      "Epoch: 01 [ 69184/244490 ( 28%)], Train Loss: 1.88506\n",
      "Epoch: 01 [ 69824/244490 ( 29%)], Train Loss: 1.88549\n",
      "Epoch: 01 [ 70464/244490 ( 29%)], Train Loss: 1.88338\n",
      "Epoch: 01 [ 71104/244490 ( 29%)], Train Loss: 1.88255\n",
      "Epoch: 01 [ 71744/244490 ( 29%)], Train Loss: 1.88114\n",
      "Epoch: 01 [ 72384/244490 ( 30%)], Train Loss: 1.88128\n",
      "Epoch: 01 [ 73024/244490 ( 30%)], Train Loss: 1.88121\n",
      "Epoch: 01 [ 73664/244490 ( 30%)], Train Loss: 1.88073\n",
      "Epoch: 01 [ 74304/244490 ( 30%)], Train Loss: 1.88008\n",
      "Epoch: 01 [ 74944/244490 ( 31%)], Train Loss: 1.87927\n",
      "Epoch: 01 [ 75584/244490 ( 31%)], Train Loss: 1.87964\n",
      "Epoch: 01 [ 76224/244490 ( 31%)], Train Loss: 1.88108\n",
      "Epoch: 01 [ 76864/244490 ( 31%)], Train Loss: 1.88168\n",
      "Epoch: 01 [ 77504/244490 ( 32%)], Train Loss: 1.88067\n",
      "Epoch: 01 [ 78144/244490 ( 32%)], Train Loss: 1.87912\n",
      "Epoch: 01 [ 78784/244490 ( 32%)], Train Loss: 1.87833\n",
      "Epoch: 01 [ 79424/244490 ( 32%)], Train Loss: 1.87845\n",
      "Epoch: 01 [ 80064/244490 ( 33%)], Train Loss: 1.87793\n",
      "Epoch: 01 [ 80704/244490 ( 33%)], Train Loss: 1.87811\n",
      "Epoch: 01 [ 81344/244490 ( 33%)], Train Loss: 1.87829\n",
      "Epoch: 01 [ 81984/244490 ( 34%)], Train Loss: 1.87742\n",
      "Epoch: 01 [ 82624/244490 ( 34%)], Train Loss: 1.87728\n",
      "Epoch: 01 [ 83264/244490 ( 34%)], Train Loss: 1.87601\n",
      "Epoch: 01 [ 83904/244490 ( 34%)], Train Loss: 1.87583\n",
      "Epoch: 01 [ 84544/244490 ( 35%)], Train Loss: 1.87524\n",
      "Epoch: 01 [ 85184/244490 ( 35%)], Train Loss: 1.87471\n",
      "Epoch: 01 [ 85824/244490 ( 35%)], Train Loss: 1.87408\n",
      "Epoch: 01 [ 86464/244490 ( 35%)], Train Loss: 1.87379\n",
      "Epoch: 01 [ 87104/244490 ( 36%)], Train Loss: 1.87395\n",
      "Epoch: 01 [ 87744/244490 ( 36%)], Train Loss: 1.87416\n",
      "Epoch: 01 [ 88384/244490 ( 36%)], Train Loss: 1.87377\n",
      "Epoch: 01 [ 89024/244490 ( 36%)], Train Loss: 1.87313\n",
      "Epoch: 01 [ 89664/244490 ( 37%)], Train Loss: 1.87262\n",
      "Epoch: 01 [ 90304/244490 ( 37%)], Train Loss: 1.87233\n",
      "Epoch: 01 [ 90944/244490 ( 37%)], Train Loss: 1.87197\n",
      "Epoch: 01 [ 91584/244490 ( 37%)], Train Loss: 1.87203\n",
      "Epoch: 01 [ 92224/244490 ( 38%)], Train Loss: 1.87081\n",
      "Epoch: 01 [ 92864/244490 ( 38%)], Train Loss: 1.87012\n",
      "Epoch: 01 [ 93504/244490 ( 38%)], Train Loss: 1.86955\n",
      "Epoch: 01 [ 94144/244490 ( 39%)], Train Loss: 1.86930\n",
      "Epoch: 01 [ 94784/244490 ( 39%)], Train Loss: 1.86930\n",
      "Epoch: 01 [ 95424/244490 ( 39%)], Train Loss: 1.86982\n",
      "Epoch: 01 [ 96064/244490 ( 39%)], Train Loss: 1.86878\n",
      "Epoch: 01 [ 96704/244490 ( 40%)], Train Loss: 1.86842\n",
      "Epoch: 01 [ 97344/244490 ( 40%)], Train Loss: 1.86684\n",
      "Epoch: 01 [ 97984/244490 ( 40%)], Train Loss: 1.86646\n",
      "Epoch: 01 [ 98624/244490 ( 40%)], Train Loss: 1.86618\n",
      "Epoch: 01 [ 99264/244490 ( 41%)], Train Loss: 1.86587\n",
      "Epoch: 01 [ 99904/244490 ( 41%)], Train Loss: 1.86659\n",
      "Epoch: 01 [100544/244490 ( 41%)], Train Loss: 1.86659\n",
      "Epoch: 01 [101184/244490 ( 41%)], Train Loss: 1.86636\n",
      "Epoch: 01 [101824/244490 ( 42%)], Train Loss: 1.86653\n",
      "Epoch: 01 [102464/244490 ( 42%)], Train Loss: 1.86575\n",
      "Epoch: 01 [103104/244490 ( 42%)], Train Loss: 1.86610\n",
      "Epoch: 01 [103744/244490 ( 42%)], Train Loss: 1.86580\n",
      "Epoch: 01 [104384/244490 ( 43%)], Train Loss: 1.86487\n",
      "Epoch: 01 [105024/244490 ( 43%)], Train Loss: 1.86425\n",
      "Epoch: 01 [105664/244490 ( 43%)], Train Loss: 1.86443\n",
      "Epoch: 01 [106304/244490 ( 43%)], Train Loss: 1.86335\n",
      "Epoch: 01 [106944/244490 ( 44%)], Train Loss: 1.86224\n",
      "Epoch: 01 [107584/244490 ( 44%)], Train Loss: 1.86246\n",
      "Epoch: 01 [108224/244490 ( 44%)], Train Loss: 1.86241\n",
      "Epoch: 01 [108864/244490 ( 45%)], Train Loss: 1.86149\n",
      "Epoch: 01 [109504/244490 ( 45%)], Train Loss: 1.86109\n",
      "Epoch: 01 [110144/244490 ( 45%)], Train Loss: 1.86085\n",
      "Epoch: 01 [110784/244490 ( 45%)], Train Loss: 1.85990\n",
      "Epoch: 01 [111424/244490 ( 46%)], Train Loss: 1.85930\n",
      "Epoch: 01 [112064/244490 ( 46%)], Train Loss: 1.85777\n",
      "Epoch: 01 [112704/244490 ( 46%)], Train Loss: 1.85775\n",
      "Epoch: 01 [113344/244490 ( 46%)], Train Loss: 1.85772\n",
      "Epoch: 01 [113984/244490 ( 47%)], Train Loss: 1.85720\n",
      "Epoch: 01 [114624/244490 ( 47%)], Train Loss: 1.85652\n",
      "Epoch: 01 [115264/244490 ( 47%)], Train Loss: 1.85608\n",
      "Epoch: 01 [115904/244490 ( 47%)], Train Loss: 1.85575\n",
      "Epoch: 01 [116544/244490 ( 48%)], Train Loss: 1.85480\n",
      "Epoch: 01 [117184/244490 ( 48%)], Train Loss: 1.85392\n",
      "Epoch: 01 [117824/244490 ( 48%)], Train Loss: 1.85354\n",
      "Epoch: 01 [118464/244490 ( 48%)], Train Loss: 1.85221\n",
      "Epoch: 01 [119104/244490 ( 49%)], Train Loss: 1.85138\n",
      "Epoch: 01 [119744/244490 ( 49%)], Train Loss: 1.85096\n",
      "Epoch: 01 [120384/244490 ( 49%)], Train Loss: 1.85082\n",
      "Epoch: 01 [121024/244490 ( 50%)], Train Loss: 1.85069\n",
      "Epoch: 01 [121664/244490 ( 50%)], Train Loss: 1.85008\n",
      "Epoch: 01 [122304/244490 ( 50%)], Train Loss: 1.84935\n",
      "Epoch: 01 [122944/244490 ( 50%)], Train Loss: 1.84907\n",
      "Epoch: 01 [123584/244490 ( 51%)], Train Loss: 1.84837\n",
      "Epoch: 01 [124224/244490 ( 51%)], Train Loss: 1.84758\n",
      "Epoch: 01 [124864/244490 ( 51%)], Train Loss: 1.84688\n",
      "Epoch: 01 [125504/244490 ( 51%)], Train Loss: 1.84740\n",
      "Epoch: 01 [126144/244490 ( 52%)], Train Loss: 1.84739\n",
      "Epoch: 01 [126784/244490 ( 52%)], Train Loss: 1.84703\n",
      "Epoch: 01 [127424/244490 ( 52%)], Train Loss: 1.84625\n",
      "Epoch: 01 [128064/244490 ( 52%)], Train Loss: 1.84572\n",
      "Epoch: 01 [128704/244490 ( 53%)], Train Loss: 1.84518\n",
      "Epoch: 01 [129344/244490 ( 53%)], Train Loss: 1.84498\n",
      "Epoch: 01 [129984/244490 ( 53%)], Train Loss: 1.84492\n",
      "Epoch: 01 [130624/244490 ( 53%)], Train Loss: 1.84427\n",
      "Epoch: 01 [131264/244490 ( 54%)], Train Loss: 1.84372\n",
      "Epoch: 01 [131904/244490 ( 54%)], Train Loss: 1.84326\n",
      "Epoch: 01 [132544/244490 ( 54%)], Train Loss: 1.84280\n",
      "Epoch: 01 [133184/244490 ( 54%)], Train Loss: 1.84299\n",
      "Epoch: 01 [133824/244490 ( 55%)], Train Loss: 1.84217\n",
      "Epoch: 01 [134464/244490 ( 55%)], Train Loss: 1.84135\n",
      "Epoch: 01 [135104/244490 ( 55%)], Train Loss: 1.84155\n",
      "Epoch: 01 [135744/244490 ( 56%)], Train Loss: 1.84207\n",
      "Epoch: 01 [136384/244490 ( 56%)], Train Loss: 1.84211\n",
      "Epoch: 01 [137024/244490 ( 56%)], Train Loss: 1.84174\n",
      "Epoch: 01 [137664/244490 ( 56%)], Train Loss: 1.84115\n",
      "Epoch: 01 [138304/244490 ( 57%)], Train Loss: 1.84088\n",
      "Epoch: 01 [138944/244490 ( 57%)], Train Loss: 1.84017\n",
      "Epoch: 01 [139584/244490 ( 57%)], Train Loss: 1.83918\n",
      "Epoch: 01 [140224/244490 ( 57%)], Train Loss: 1.83857\n",
      "Epoch: 01 [140864/244490 ( 58%)], Train Loss: 1.83807\n",
      "Epoch: 01 [141504/244490 ( 58%)], Train Loss: 1.83790\n",
      "Epoch: 01 [142144/244490 ( 58%)], Train Loss: 1.83716\n",
      "Epoch: 01 [142784/244490 ( 58%)], Train Loss: 1.83673\n",
      "Epoch: 01 [143424/244490 ( 59%)], Train Loss: 1.83653\n",
      "Epoch: 01 [144064/244490 ( 59%)], Train Loss: 1.83551\n",
      "Epoch: 01 [144704/244490 ( 59%)], Train Loss: 1.83477\n",
      "Epoch: 01 [145344/244490 ( 59%)], Train Loss: 1.83415\n",
      "Epoch: 01 [145984/244490 ( 60%)], Train Loss: 1.83379\n",
      "Epoch: 01 [146624/244490 ( 60%)], Train Loss: 1.83370\n",
      "Epoch: 01 [147264/244490 ( 60%)], Train Loss: 1.83277\n",
      "Epoch: 01 [147904/244490 ( 60%)], Train Loss: 1.83183\n",
      "Epoch: 01 [148544/244490 ( 61%)], Train Loss: 1.83157\n",
      "Epoch: 01 [149184/244490 ( 61%)], Train Loss: 1.83104\n",
      "Epoch: 01 [149824/244490 ( 61%)], Train Loss: 1.83081\n",
      "Epoch: 01 [150464/244490 ( 62%)], Train Loss: 1.83013\n",
      "Epoch: 01 [151104/244490 ( 62%)], Train Loss: 1.83020\n",
      "Epoch: 01 [151744/244490 ( 62%)], Train Loss: 1.83006\n",
      "Epoch: 01 [152384/244490 ( 62%)], Train Loss: 1.82960\n",
      "Epoch: 01 [153024/244490 ( 63%)], Train Loss: 1.82958\n",
      "Epoch: 01 [153664/244490 ( 63%)], Train Loss: 1.82878\n",
      "Epoch: 01 [154304/244490 ( 63%)], Train Loss: 1.82856\n",
      "Epoch: 01 [154944/244490 ( 63%)], Train Loss: 1.82763\n",
      "Epoch: 01 [155584/244490 ( 64%)], Train Loss: 1.82730\n",
      "Epoch: 01 [156224/244490 ( 64%)], Train Loss: 1.82675\n",
      "Epoch: 01 [156864/244490 ( 64%)], Train Loss: 1.82595\n",
      "Epoch: 01 [157504/244490 ( 64%)], Train Loss: 1.82533\n",
      "Epoch: 01 [158144/244490 ( 65%)], Train Loss: 1.82559\n",
      "Epoch: 01 [158784/244490 ( 65%)], Train Loss: 1.82497\n",
      "Epoch: 01 [159424/244490 ( 65%)], Train Loss: 1.82460\n",
      "Epoch: 01 [160064/244490 ( 65%)], Train Loss: 1.82429\n",
      "Epoch: 01 [160704/244490 ( 66%)], Train Loss: 1.82366\n",
      "Epoch: 01 [161344/244490 ( 66%)], Train Loss: 1.82264\n",
      "Epoch: 01 [161984/244490 ( 66%)], Train Loss: 1.82236\n",
      "Epoch: 01 [162624/244490 ( 67%)], Train Loss: 1.82200\n",
      "Epoch: 01 [163264/244490 ( 67%)], Train Loss: 1.82180\n",
      "Epoch: 01 [163904/244490 ( 67%)], Train Loss: 1.82199\n",
      "Epoch: 01 [164544/244490 ( 67%)], Train Loss: 1.82088\n",
      "Epoch: 01 [165184/244490 ( 68%)], Train Loss: 1.82033\n",
      "Epoch: 01 [165824/244490 ( 68%)], Train Loss: 1.82010\n",
      "Epoch: 01 [166464/244490 ( 68%)], Train Loss: 1.81943\n",
      "Epoch: 01 [167104/244490 ( 68%)], Train Loss: 1.81932\n",
      "Epoch: 01 [167744/244490 ( 69%)], Train Loss: 1.81897\n",
      "Epoch: 01 [168384/244490 ( 69%)], Train Loss: 1.81847\n",
      "Epoch: 01 [169024/244490 ( 69%)], Train Loss: 1.81793\n",
      "Epoch: 01 [169664/244490 ( 69%)], Train Loss: 1.81740\n",
      "Epoch: 01 [170304/244490 ( 70%)], Train Loss: 1.81688\n",
      "Epoch: 01 [170944/244490 ( 70%)], Train Loss: 1.81695\n",
      "Epoch: 01 [171584/244490 ( 70%)], Train Loss: 1.81610\n",
      "Epoch: 01 [172224/244490 ( 70%)], Train Loss: 1.81531\n",
      "Epoch: 01 [172864/244490 ( 71%)], Train Loss: 1.81490\n",
      "Epoch: 01 [173504/244490 ( 71%)], Train Loss: 1.81425\n",
      "Epoch: 01 [174144/244490 ( 71%)], Train Loss: 1.81401\n",
      "Epoch: 01 [174784/244490 ( 71%)], Train Loss: 1.81393\n",
      "Epoch: 01 [175424/244490 ( 72%)], Train Loss: 1.81387\n",
      "Epoch: 01 [176064/244490 ( 72%)], Train Loss: 1.81354\n",
      "Epoch: 01 [176704/244490 ( 72%)], Train Loss: 1.81302\n",
      "Epoch: 01 [177344/244490 ( 73%)], Train Loss: 1.81320\n",
      "Epoch: 01 [177984/244490 ( 73%)], Train Loss: 1.81304\n",
      "Epoch: 01 [178624/244490 ( 73%)], Train Loss: 1.81246\n",
      "Epoch: 01 [179264/244490 ( 73%)], Train Loss: 1.81229\n",
      "Epoch: 01 [179904/244490 ( 74%)], Train Loss: 1.81174\n",
      "Epoch: 01 [180544/244490 ( 74%)], Train Loss: 1.81113\n",
      "Epoch: 01 [181184/244490 ( 74%)], Train Loss: 1.81050\n",
      "Epoch: 01 [181824/244490 ( 74%)], Train Loss: 1.80967\n",
      "Epoch: 01 [182464/244490 ( 75%)], Train Loss: 1.80934\n",
      "Epoch: 01 [183104/244490 ( 75%)], Train Loss: 1.80858\n",
      "Epoch: 01 [183744/244490 ( 75%)], Train Loss: 1.80837\n",
      "Epoch: 01 [184384/244490 ( 75%)], Train Loss: 1.80821\n",
      "Epoch: 01 [185024/244490 ( 76%)], Train Loss: 1.80822\n",
      "Epoch: 01 [185664/244490 ( 76%)], Train Loss: 1.80777\n",
      "Epoch: 01 [186304/244490 ( 76%)], Train Loss: 1.80793\n",
      "Epoch: 01 [186944/244490 ( 76%)], Train Loss: 1.80737\n",
      "Epoch: 01 [187584/244490 ( 77%)], Train Loss: 1.80700\n",
      "Epoch: 01 [188224/244490 ( 77%)], Train Loss: 1.80657\n",
      "Epoch: 01 [188864/244490 ( 77%)], Train Loss: 1.80627\n",
      "Epoch: 01 [189504/244490 ( 78%)], Train Loss: 1.80589\n",
      "Epoch: 01 [190144/244490 ( 78%)], Train Loss: 1.80533\n",
      "Epoch: 01 [190784/244490 ( 78%)], Train Loss: 1.80501\n",
      "Epoch: 01 [191424/244490 ( 78%)], Train Loss: 1.80442\n",
      "Epoch: 01 [192064/244490 ( 79%)], Train Loss: 1.80400\n",
      "Epoch: 01 [192704/244490 ( 79%)], Train Loss: 1.80388\n",
      "Epoch: 01 [193344/244490 ( 79%)], Train Loss: 1.80336\n",
      "Epoch: 01 [193984/244490 ( 79%)], Train Loss: 1.80307\n",
      "Epoch: 01 [194624/244490 ( 80%)], Train Loss: 1.80282\n",
      "Epoch: 01 [195264/244490 ( 80%)], Train Loss: 1.80233\n",
      "Epoch: 01 [195904/244490 ( 80%)], Train Loss: 1.80185\n",
      "Epoch: 01 [196544/244490 ( 80%)], Train Loss: 1.80123\n",
      "Epoch: 01 [197184/244490 ( 81%)], Train Loss: 1.80092\n",
      "Epoch: 01 [197824/244490 ( 81%)], Train Loss: 1.80074\n",
      "Epoch: 01 [198464/244490 ( 81%)], Train Loss: 1.80078\n",
      "Epoch: 01 [199104/244490 ( 81%)], Train Loss: 1.80080\n",
      "Epoch: 01 [199744/244490 ( 82%)], Train Loss: 1.80048\n",
      "Epoch: 01 [200384/244490 ( 82%)], Train Loss: 1.79988\n",
      "Epoch: 01 [201024/244490 ( 82%)], Train Loss: 1.79974\n",
      "Epoch: 01 [201664/244490 ( 82%)], Train Loss: 1.79917\n",
      "Epoch: 01 [202304/244490 ( 83%)], Train Loss: 1.79866\n",
      "Epoch: 01 [202944/244490 ( 83%)], Train Loss: 1.79819\n",
      "Epoch: 01 [203584/244490 ( 83%)], Train Loss: 1.79799\n",
      "Epoch: 01 [204224/244490 ( 84%)], Train Loss: 1.79689\n",
      "Epoch: 01 [204864/244490 ( 84%)], Train Loss: 1.79648\n",
      "Epoch: 01 [205504/244490 ( 84%)], Train Loss: 1.79686\n",
      "Epoch: 01 [206144/244490 ( 84%)], Train Loss: 1.79659\n",
      "Epoch: 01 [206784/244490 ( 85%)], Train Loss: 1.79605\n",
      "Epoch: 01 [207424/244490 ( 85%)], Train Loss: 1.79539\n",
      "Epoch: 01 [208064/244490 ( 85%)], Train Loss: 1.79504\n",
      "Epoch: 01 [208704/244490 ( 85%)], Train Loss: 1.79470\n",
      "Epoch: 01 [209344/244490 ( 86%)], Train Loss: 1.79417\n",
      "Epoch: 01 [209984/244490 ( 86%)], Train Loss: 1.79368\n",
      "Epoch: 01 [210624/244490 ( 86%)], Train Loss: 1.79324\n",
      "Epoch: 01 [211264/244490 ( 86%)], Train Loss: 1.79294\n",
      "Epoch: 01 [211904/244490 ( 87%)], Train Loss: 1.79277\n",
      "Epoch: 01 [212544/244490 ( 87%)], Train Loss: 1.79247\n",
      "Epoch: 01 [213184/244490 ( 87%)], Train Loss: 1.79186\n",
      "Epoch: 01 [213824/244490 ( 87%)], Train Loss: 1.79136\n",
      "Epoch: 01 [214464/244490 ( 88%)], Train Loss: 1.79107\n",
      "Epoch: 01 [215104/244490 ( 88%)], Train Loss: 1.79073\n",
      "Epoch: 01 [215744/244490 ( 88%)], Train Loss: 1.79040\n",
      "Epoch: 01 [216384/244490 ( 89%)], Train Loss: 1.79079\n",
      "Epoch: 01 [217024/244490 ( 89%)], Train Loss: 1.79079\n",
      "Epoch: 01 [217664/244490 ( 89%)], Train Loss: 1.79053\n",
      "Epoch: 01 [218304/244490 ( 89%)], Train Loss: 1.79016\n",
      "Epoch: 01 [218944/244490 ( 90%)], Train Loss: 1.78980\n",
      "Epoch: 01 [219584/244490 ( 90%)], Train Loss: 1.78933\n",
      "Epoch: 01 [220224/244490 ( 90%)], Train Loss: 1.78862\n",
      "Epoch: 01 [220864/244490 ( 90%)], Train Loss: 1.78833\n",
      "Epoch: 01 [221504/244490 ( 91%)], Train Loss: 1.78829\n",
      "Epoch: 01 [222144/244490 ( 91%)], Train Loss: 1.78831\n",
      "Epoch: 01 [222784/244490 ( 91%)], Train Loss: 1.78807\n",
      "Epoch: 01 [223424/244490 ( 91%)], Train Loss: 1.78785\n",
      "Epoch: 01 [224064/244490 ( 92%)], Train Loss: 1.78720\n",
      "Epoch: 01 [224704/244490 ( 92%)], Train Loss: 1.78652\n",
      "Epoch: 01 [225344/244490 ( 92%)], Train Loss: 1.78625\n",
      "Epoch: 01 [225984/244490 ( 92%)], Train Loss: 1.78584\n",
      "Epoch: 01 [226624/244490 ( 93%)], Train Loss: 1.78558\n",
      "Epoch: 01 [227264/244490 ( 93%)], Train Loss: 1.78541\n",
      "Epoch: 01 [227904/244490 ( 93%)], Train Loss: 1.78485\n",
      "Epoch: 01 [228544/244490 ( 93%)], Train Loss: 1.78424\n",
      "Epoch: 01 [229184/244490 ( 94%)], Train Loss: 1.78399\n",
      "Epoch: 01 [229824/244490 ( 94%)], Train Loss: 1.78350\n",
      "Epoch: 01 [230464/244490 ( 94%)], Train Loss: 1.78319\n",
      "Epoch: 01 [231104/244490 ( 95%)], Train Loss: 1.78271\n",
      "Epoch: 01 [231744/244490 ( 95%)], Train Loss: 1.78248\n",
      "Epoch: 01 [232384/244490 ( 95%)], Train Loss: 1.78236\n",
      "Epoch: 01 [233024/244490 ( 95%)], Train Loss: 1.78178\n",
      "Epoch: 01 [233664/244490 ( 96%)], Train Loss: 1.78132\n",
      "Epoch: 01 [234304/244490 ( 96%)], Train Loss: 1.78102\n",
      "Epoch: 01 [234944/244490 ( 96%)], Train Loss: 1.78053\n",
      "Epoch: 01 [235584/244490 ( 96%)], Train Loss: 1.78002\n",
      "Epoch: 01 [236224/244490 ( 97%)], Train Loss: 1.77947\n",
      "Epoch: 01 [236864/244490 ( 97%)], Train Loss: 1.77903\n",
      "Epoch: 01 [237504/244490 ( 97%)], Train Loss: 1.77871\n",
      "Epoch: 01 [238144/244490 ( 97%)], Train Loss: 1.77831\n",
      "Epoch: 01 [238784/244490 ( 98%)], Train Loss: 1.77816\n",
      "Epoch: 01 [239424/244490 ( 98%)], Train Loss: 1.77800\n",
      "Epoch: 01 [240064/244490 ( 98%)], Train Loss: 1.77754\n",
      "Epoch: 01 [240704/244490 ( 98%)], Train Loss: 1.77746\n",
      "Epoch: 01 [241344/244490 ( 99%)], Train Loss: 1.77728\n",
      "Epoch: 01 [241984/244490 ( 99%)], Train Loss: 1.77704\n",
      "Epoch: 01 [242624/244490 ( 99%)], Train Loss: 1.77657\n",
      "Epoch: 01 [243264/244490 ( 99%)], Train Loss: 1.77635\n",
      "Epoch: 01 [243904/244490 (100%)], Train Loss: 1.77606\n",
      "Epoch: 01 [244490/244490 (100%)], Train Loss: 1.77566\n",
      "----Validation Results Summary----\n",
      "Epoch: [1] Valid Loss: 1.96390\n",
      "\n",
      "Epoch: 02 [    64/244490 (  0%)], Train Loss: 1.23831\n",
      "Epoch: 02 [   704/244490 (  0%)], Train Loss: 1.64438\n",
      "Epoch: 02 [  1344/244490 (  1%)], Train Loss: 1.60566\n",
      "Epoch: 02 [  1984/244490 (  1%)], Train Loss: 1.62570\n",
      "Epoch: 02 [  2624/244490 (  1%)], Train Loss: 1.61272\n",
      "Epoch: 02 [  3264/244490 (  1%)], Train Loss: 1.59880\n",
      "Epoch: 02 [  3904/244490 (  2%)], Train Loss: 1.60067\n",
      "Epoch: 02 [  4544/244490 (  2%)], Train Loss: 1.60063\n",
      "Epoch: 02 [  5184/244490 (  2%)], Train Loss: 1.59490\n",
      "Epoch: 02 [  5824/244490 (  2%)], Train Loss: 1.59636\n",
      "Epoch: 02 [  6464/244490 (  3%)], Train Loss: 1.61281\n",
      "Epoch: 02 [  7104/244490 (  3%)], Train Loss: 1.62340\n",
      "Epoch: 02 [  7744/244490 (  3%)], Train Loss: 1.62376\n",
      "Epoch: 02 [  8384/244490 (  3%)], Train Loss: 1.62848\n",
      "Epoch: 02 [  9024/244490 (  4%)], Train Loss: 1.63601\n",
      "Epoch: 02 [  9664/244490 (  4%)], Train Loss: 1.64278\n",
      "Epoch: 02 [ 10304/244490 (  4%)], Train Loss: 1.64541\n",
      "Epoch: 02 [ 10944/244490 (  4%)], Train Loss: 1.63968\n",
      "Epoch: 02 [ 11584/244490 (  5%)], Train Loss: 1.63943\n",
      "Epoch: 02 [ 12224/244490 (  5%)], Train Loss: 1.63764\n",
      "Epoch: 02 [ 12864/244490 (  5%)], Train Loss: 1.63794\n",
      "Epoch: 02 [ 13504/244490 (  6%)], Train Loss: 1.64353\n",
      "Epoch: 02 [ 14144/244490 (  6%)], Train Loss: 1.64403\n",
      "Epoch: 02 [ 14784/244490 (  6%)], Train Loss: 1.63869\n",
      "Epoch: 02 [ 15424/244490 (  6%)], Train Loss: 1.63626\n",
      "Epoch: 02 [ 16064/244490 (  7%)], Train Loss: 1.63303\n",
      "Epoch: 02 [ 16704/244490 (  7%)], Train Loss: 1.62891\n",
      "Epoch: 02 [ 17344/244490 (  7%)], Train Loss: 1.62811\n",
      "Epoch: 02 [ 17984/244490 (  7%)], Train Loss: 1.63229\n",
      "Epoch: 02 [ 18624/244490 (  8%)], Train Loss: 1.62997\n",
      "Epoch: 02 [ 19264/244490 (  8%)], Train Loss: 1.62704\n",
      "Epoch: 02 [ 19904/244490 (  8%)], Train Loss: 1.62591\n",
      "Epoch: 02 [ 20544/244490 (  8%)], Train Loss: 1.62664\n",
      "Epoch: 02 [ 21184/244490 (  9%)], Train Loss: 1.62453\n",
      "Epoch: 02 [ 21824/244490 (  9%)], Train Loss: 1.62456\n",
      "Epoch: 02 [ 22464/244490 (  9%)], Train Loss: 1.62543\n",
      "Epoch: 02 [ 23104/244490 (  9%)], Train Loss: 1.62528\n",
      "Epoch: 02 [ 23744/244490 ( 10%)], Train Loss: 1.62420\n",
      "Epoch: 02 [ 24384/244490 ( 10%)], Train Loss: 1.62376\n",
      "Epoch: 02 [ 25024/244490 ( 10%)], Train Loss: 1.62567\n",
      "Epoch: 02 [ 25664/244490 ( 10%)], Train Loss: 1.62397\n",
      "Epoch: 02 [ 26304/244490 ( 11%)], Train Loss: 1.62323\n",
      "Epoch: 02 [ 26944/244490 ( 11%)], Train Loss: 1.62048\n",
      "Epoch: 02 [ 27584/244490 ( 11%)], Train Loss: 1.61979\n",
      "Epoch: 02 [ 28224/244490 ( 12%)], Train Loss: 1.61683\n",
      "Epoch: 02 [ 28864/244490 ( 12%)], Train Loss: 1.61693\n",
      "Epoch: 02 [ 29504/244490 ( 12%)], Train Loss: 1.61428\n",
      "Epoch: 02 [ 30144/244490 ( 12%)], Train Loss: 1.61328\n",
      "Epoch: 02 [ 30784/244490 ( 13%)], Train Loss: 1.61196\n",
      "Epoch: 02 [ 31424/244490 ( 13%)], Train Loss: 1.61352\n",
      "Epoch: 02 [ 32064/244490 ( 13%)], Train Loss: 1.61415\n",
      "Epoch: 02 [ 32704/244490 ( 13%)], Train Loss: 1.61520\n",
      "Epoch: 02 [ 33344/244490 ( 14%)], Train Loss: 1.61548\n",
      "Epoch: 02 [ 33984/244490 ( 14%)], Train Loss: 1.61518\n",
      "Epoch: 02 [ 34624/244490 ( 14%)], Train Loss: 1.61554\n",
      "Epoch: 02 [ 35264/244490 ( 14%)], Train Loss: 1.61602\n",
      "Epoch: 02 [ 35904/244490 ( 15%)], Train Loss: 1.61684\n",
      "Epoch: 02 [ 36544/244490 ( 15%)], Train Loss: 1.61860\n",
      "Epoch: 02 [ 37184/244490 ( 15%)], Train Loss: 1.61884\n",
      "Epoch: 02 [ 37824/244490 ( 15%)], Train Loss: 1.61828\n",
      "Epoch: 02 [ 38464/244490 ( 16%)], Train Loss: 1.61710\n",
      "Epoch: 02 [ 39104/244490 ( 16%)], Train Loss: 1.61727\n",
      "Epoch: 02 [ 39744/244490 ( 16%)], Train Loss: 1.61629\n",
      "Epoch: 02 [ 40384/244490 ( 17%)], Train Loss: 1.61521\n",
      "Epoch: 02 [ 41024/244490 ( 17%)], Train Loss: 1.61410\n",
      "Epoch: 02 [ 41664/244490 ( 17%)], Train Loss: 1.61191\n",
      "Epoch: 02 [ 42304/244490 ( 17%)], Train Loss: 1.61212\n",
      "Epoch: 02 [ 42944/244490 ( 18%)], Train Loss: 1.61033\n",
      "Epoch: 02 [ 43584/244490 ( 18%)], Train Loss: 1.60822\n",
      "Epoch: 02 [ 44224/244490 ( 18%)], Train Loss: 1.60835\n",
      "Epoch: 02 [ 44864/244490 ( 18%)], Train Loss: 1.60759\n",
      "Epoch: 02 [ 45504/244490 ( 19%)], Train Loss: 1.60848\n",
      "Epoch: 02 [ 46144/244490 ( 19%)], Train Loss: 1.60641\n",
      "Epoch: 02 [ 46784/244490 ( 19%)], Train Loss: 1.60542\n",
      "Epoch: 02 [ 47424/244490 ( 19%)], Train Loss: 1.60333\n",
      "Epoch: 02 [ 48064/244490 ( 20%)], Train Loss: 1.60186\n",
      "Epoch: 02 [ 48704/244490 ( 20%)], Train Loss: 1.60221\n",
      "Epoch: 02 [ 49344/244490 ( 20%)], Train Loss: 1.60038\n",
      "Epoch: 02 [ 49984/244490 ( 20%)], Train Loss: 1.60099\n",
      "Epoch: 02 [ 50624/244490 ( 21%)], Train Loss: 1.59983\n",
      "Epoch: 02 [ 51264/244490 ( 21%)], Train Loss: 1.59902\n",
      "Epoch: 02 [ 51904/244490 ( 21%)], Train Loss: 1.59924\n",
      "Epoch: 02 [ 52544/244490 ( 21%)], Train Loss: 1.59716\n",
      "Epoch: 02 [ 53184/244490 ( 22%)], Train Loss: 1.59569\n",
      "Epoch: 02 [ 53824/244490 ( 22%)], Train Loss: 1.59431\n",
      "Epoch: 02 [ 54464/244490 ( 22%)], Train Loss: 1.59342\n",
      "Epoch: 02 [ 55104/244490 ( 23%)], Train Loss: 1.59279\n",
      "Epoch: 02 [ 55744/244490 ( 23%)], Train Loss: 1.59304\n",
      "Epoch: 02 [ 56384/244490 ( 23%)], Train Loss: 1.59227\n",
      "Epoch: 02 [ 57024/244490 ( 23%)], Train Loss: 1.59154\n",
      "Epoch: 02 [ 57664/244490 ( 24%)], Train Loss: 1.58990\n",
      "Epoch: 02 [ 58304/244490 ( 24%)], Train Loss: 1.58849\n",
      "Epoch: 02 [ 58944/244490 ( 24%)], Train Loss: 1.58676\n",
      "Epoch: 02 [ 59584/244490 ( 24%)], Train Loss: 1.58523\n",
      "Epoch: 02 [ 60224/244490 ( 25%)], Train Loss: 1.58582\n",
      "Epoch: 02 [ 60864/244490 ( 25%)], Train Loss: 1.58422\n",
      "Epoch: 02 [ 61504/244490 ( 25%)], Train Loss: 1.58287\n",
      "Epoch: 02 [ 62144/244490 ( 25%)], Train Loss: 1.58132\n",
      "Epoch: 02 [ 62784/244490 ( 26%)], Train Loss: 1.58049\n",
      "Epoch: 02 [ 63424/244490 ( 26%)], Train Loss: 1.57995\n",
      "Epoch: 02 [ 64064/244490 ( 26%)], Train Loss: 1.57812\n",
      "Epoch: 02 [ 64704/244490 ( 26%)], Train Loss: 1.57789\n",
      "Epoch: 02 [ 65344/244490 ( 27%)], Train Loss: 1.57752\n",
      "Epoch: 02 [ 65984/244490 ( 27%)], Train Loss: 1.57615\n",
      "Epoch: 02 [ 66624/244490 ( 27%)], Train Loss: 1.57532\n",
      "Epoch: 02 [ 67264/244490 ( 28%)], Train Loss: 1.57504\n",
      "Epoch: 02 [ 67904/244490 ( 28%)], Train Loss: 1.57431\n",
      "Epoch: 02 [ 68544/244490 ( 28%)], Train Loss: 1.57161\n",
      "Epoch: 02 [ 69184/244490 ( 28%)], Train Loss: 1.57091\n",
      "Epoch: 02 [ 69824/244490 ( 29%)], Train Loss: 1.57100\n",
      "Epoch: 02 [ 70464/244490 ( 29%)], Train Loss: 1.56915\n",
      "Epoch: 02 [ 71104/244490 ( 29%)], Train Loss: 1.56817\n",
      "Epoch: 02 [ 71744/244490 ( 29%)], Train Loss: 1.56694\n",
      "Epoch: 02 [ 72384/244490 ( 30%)], Train Loss: 1.56641\n",
      "Epoch: 02 [ 73024/244490 ( 30%)], Train Loss: 1.56640\n",
      "Epoch: 02 [ 73664/244490 ( 30%)], Train Loss: 1.56568\n",
      "Epoch: 02 [ 74304/244490 ( 30%)], Train Loss: 1.56479\n",
      "Epoch: 02 [ 74944/244490 ( 31%)], Train Loss: 1.56369\n",
      "Epoch: 02 [ 75584/244490 ( 31%)], Train Loss: 1.56409\n",
      "Epoch: 02 [ 76224/244490 ( 31%)], Train Loss: 1.56633\n",
      "Epoch: 02 [ 76864/244490 ( 31%)], Train Loss: 1.56744\n",
      "Epoch: 02 [ 77504/244490 ( 32%)], Train Loss: 1.56690\n",
      "Epoch: 02 [ 78144/244490 ( 32%)], Train Loss: 1.56561\n",
      "Epoch: 02 [ 78784/244490 ( 32%)], Train Loss: 1.56476\n",
      "Epoch: 02 [ 79424/244490 ( 32%)], Train Loss: 1.56478\n",
      "Epoch: 02 [ 80064/244490 ( 33%)], Train Loss: 1.56459\n",
      "Epoch: 02 [ 80704/244490 ( 33%)], Train Loss: 1.56476\n",
      "Epoch: 02 [ 81344/244490 ( 33%)], Train Loss: 1.56486\n",
      "Epoch: 02 [ 81984/244490 ( 34%)], Train Loss: 1.56432\n",
      "Epoch: 02 [ 82624/244490 ( 34%)], Train Loss: 1.56405\n",
      "Epoch: 02 [ 83264/244490 ( 34%)], Train Loss: 1.56235\n",
      "Epoch: 02 [ 83904/244490 ( 34%)], Train Loss: 1.56237\n",
      "Epoch: 02 [ 84544/244490 ( 35%)], Train Loss: 1.56178\n",
      "Epoch: 02 [ 85184/244490 ( 35%)], Train Loss: 1.56111\n",
      "Epoch: 02 [ 85824/244490 ( 35%)], Train Loss: 1.56034\n",
      "Epoch: 02 [ 86464/244490 ( 35%)], Train Loss: 1.55941\n",
      "Epoch: 02 [ 87104/244490 ( 36%)], Train Loss: 1.55911\n",
      "Epoch: 02 [ 87744/244490 ( 36%)], Train Loss: 1.55919\n",
      "Epoch: 02 [ 88384/244490 ( 36%)], Train Loss: 1.55875\n",
      "Epoch: 02 [ 89024/244490 ( 36%)], Train Loss: 1.55827\n",
      "Epoch: 02 [ 89664/244490 ( 37%)], Train Loss: 1.55778\n",
      "Epoch: 02 [ 90304/244490 ( 37%)], Train Loss: 1.55673\n",
      "Epoch: 02 [ 90944/244490 ( 37%)], Train Loss: 1.55607\n",
      "Epoch: 02 [ 91584/244490 ( 37%)], Train Loss: 1.55554\n",
      "Epoch: 02 [ 92224/244490 ( 38%)], Train Loss: 1.55420\n",
      "Epoch: 02 [ 92864/244490 ( 38%)], Train Loss: 1.55331\n",
      "Epoch: 02 [ 93504/244490 ( 38%)], Train Loss: 1.55308\n",
      "Epoch: 02 [ 94144/244490 ( 39%)], Train Loss: 1.55276\n",
      "Epoch: 02 [ 94784/244490 ( 39%)], Train Loss: 1.55285\n",
      "Epoch: 02 [ 95424/244490 ( 39%)], Train Loss: 1.55322\n",
      "Epoch: 02 [ 96064/244490 ( 39%)], Train Loss: 1.55202\n",
      "Epoch: 02 [ 96704/244490 ( 40%)], Train Loss: 1.55175\n",
      "Epoch: 02 [ 97344/244490 ( 40%)], Train Loss: 1.55008\n",
      "Epoch: 02 [ 97984/244490 ( 40%)], Train Loss: 1.55009\n",
      "Epoch: 02 [ 98624/244490 ( 40%)], Train Loss: 1.54964\n",
      "Epoch: 02 [ 99264/244490 ( 41%)], Train Loss: 1.54900\n",
      "Epoch: 02 [ 99904/244490 ( 41%)], Train Loss: 1.54955\n",
      "Epoch: 02 [100544/244490 ( 41%)], Train Loss: 1.54979\n",
      "Epoch: 02 [101184/244490 ( 41%)], Train Loss: 1.54964\n",
      "Epoch: 02 [101824/244490 ( 42%)], Train Loss: 1.54990\n",
      "Epoch: 02 [102464/244490 ( 42%)], Train Loss: 1.54936\n",
      "Epoch: 02 [103104/244490 ( 42%)], Train Loss: 1.54953\n",
      "Epoch: 02 [103744/244490 ( 42%)], Train Loss: 1.54910\n",
      "Epoch: 02 [104384/244490 ( 43%)], Train Loss: 1.54837\n",
      "Epoch: 02 [105024/244490 ( 43%)], Train Loss: 1.54734\n",
      "Epoch: 02 [105664/244490 ( 43%)], Train Loss: 1.54726\n",
      "Epoch: 02 [106304/244490 ( 43%)], Train Loss: 1.54605\n",
      "Epoch: 02 [106944/244490 ( 44%)], Train Loss: 1.54501\n",
      "Epoch: 02 [107584/244490 ( 44%)], Train Loss: 1.54523\n",
      "Epoch: 02 [108224/244490 ( 44%)], Train Loss: 1.54554\n",
      "Epoch: 02 [108864/244490 ( 45%)], Train Loss: 1.54455\n",
      "Epoch: 02 [109504/244490 ( 45%)], Train Loss: 1.54408\n",
      "Epoch: 02 [110144/244490 ( 45%)], Train Loss: 1.54381\n",
      "Epoch: 02 [110784/244490 ( 45%)], Train Loss: 1.54317\n",
      "Epoch: 02 [111424/244490 ( 46%)], Train Loss: 1.54232\n",
      "Epoch: 02 [112064/244490 ( 46%)], Train Loss: 1.54093\n",
      "Epoch: 02 [112704/244490 ( 46%)], Train Loss: 1.54109\n",
      "Epoch: 02 [113344/244490 ( 46%)], Train Loss: 1.54115\n",
      "Epoch: 02 [113984/244490 ( 47%)], Train Loss: 1.54044\n",
      "Epoch: 02 [114624/244490 ( 47%)], Train Loss: 1.54017\n",
      "Epoch: 02 [115264/244490 ( 47%)], Train Loss: 1.53957\n",
      "Epoch: 02 [115904/244490 ( 47%)], Train Loss: 1.53912\n",
      "Epoch: 02 [116544/244490 ( 48%)], Train Loss: 1.53813\n",
      "Epoch: 02 [117184/244490 ( 48%)], Train Loss: 1.53742\n",
      "Epoch: 02 [117824/244490 ( 48%)], Train Loss: 1.53680\n",
      "Epoch: 02 [118464/244490 ( 48%)], Train Loss: 1.53539\n",
      "Epoch: 02 [119104/244490 ( 49%)], Train Loss: 1.53448\n",
      "Epoch: 02 [119744/244490 ( 49%)], Train Loss: 1.53349\n",
      "Epoch: 02 [120384/244490 ( 49%)], Train Loss: 1.53309\n",
      "Epoch: 02 [121024/244490 ( 50%)], Train Loss: 1.53311\n",
      "Epoch: 02 [121664/244490 ( 50%)], Train Loss: 1.53231\n",
      "Epoch: 02 [122304/244490 ( 50%)], Train Loss: 1.53131\n",
      "Epoch: 02 [122944/244490 ( 50%)], Train Loss: 1.53085\n",
      "Epoch: 02 [123584/244490 ( 51%)], Train Loss: 1.53041\n",
      "Epoch: 02 [124224/244490 ( 51%)], Train Loss: 1.52990\n",
      "Epoch: 02 [124864/244490 ( 51%)], Train Loss: 1.52939\n",
      "Epoch: 02 [125504/244490 ( 51%)], Train Loss: 1.52949\n",
      "Epoch: 02 [126144/244490 ( 52%)], Train Loss: 1.52897\n",
      "Epoch: 02 [126784/244490 ( 52%)], Train Loss: 1.52866\n",
      "Epoch: 02 [127424/244490 ( 52%)], Train Loss: 1.52811\n",
      "Epoch: 02 [128064/244490 ( 52%)], Train Loss: 1.52772\n",
      "Epoch: 02 [128704/244490 ( 53%)], Train Loss: 1.52748\n",
      "Epoch: 02 [129344/244490 ( 53%)], Train Loss: 1.52723\n",
      "Epoch: 02 [129984/244490 ( 53%)], Train Loss: 1.52730\n",
      "Epoch: 02 [130624/244490 ( 53%)], Train Loss: 1.52657\n",
      "Epoch: 02 [131264/244490 ( 54%)], Train Loss: 1.52607\n",
      "Epoch: 02 [131904/244490 ( 54%)], Train Loss: 1.52583\n",
      "Epoch: 02 [132544/244490 ( 54%)], Train Loss: 1.52547\n",
      "Epoch: 02 [133184/244490 ( 54%)], Train Loss: 1.52572\n",
      "Epoch: 02 [133824/244490 ( 55%)], Train Loss: 1.52501\n",
      "Epoch: 02 [134464/244490 ( 55%)], Train Loss: 1.52429\n",
      "Epoch: 02 [135104/244490 ( 55%)], Train Loss: 1.52434\n",
      "Epoch: 02 [135744/244490 ( 56%)], Train Loss: 1.52470\n",
      "Epoch: 02 [136384/244490 ( 56%)], Train Loss: 1.52455\n",
      "Epoch: 02 [137024/244490 ( 56%)], Train Loss: 1.52414\n",
      "Epoch: 02 [137664/244490 ( 56%)], Train Loss: 1.52369\n",
      "Epoch: 02 [138304/244490 ( 57%)], Train Loss: 1.52336\n",
      "Epoch: 02 [138944/244490 ( 57%)], Train Loss: 1.52252\n",
      "Epoch: 02 [139584/244490 ( 57%)], Train Loss: 1.52161\n",
      "Epoch: 02 [140224/244490 ( 57%)], Train Loss: 1.52104\n",
      "Epoch: 02 [140864/244490 ( 58%)], Train Loss: 1.52021\n",
      "Epoch: 02 [141504/244490 ( 58%)], Train Loss: 1.52001\n",
      "Epoch: 02 [142144/244490 ( 58%)], Train Loss: 1.51911\n",
      "Epoch: 02 [142784/244490 ( 58%)], Train Loss: 1.51854\n",
      "Epoch: 02 [143424/244490 ( 59%)], Train Loss: 1.51826\n",
      "Epoch: 02 [144064/244490 ( 59%)], Train Loss: 1.51697\n",
      "Epoch: 02 [144704/244490 ( 59%)], Train Loss: 1.51645\n",
      "Epoch: 02 [145344/244490 ( 59%)], Train Loss: 1.51549\n",
      "Epoch: 02 [145984/244490 ( 60%)], Train Loss: 1.51518\n",
      "Epoch: 02 [146624/244490 ( 60%)], Train Loss: 1.51482\n",
      "Epoch: 02 [147264/244490 ( 60%)], Train Loss: 1.51401\n",
      "Epoch: 02 [147904/244490 ( 60%)], Train Loss: 1.51295\n",
      "Epoch: 02 [148544/244490 ( 61%)], Train Loss: 1.51278\n",
      "Epoch: 02 [149184/244490 ( 61%)], Train Loss: 1.51207\n",
      "Epoch: 02 [149824/244490 ( 61%)], Train Loss: 1.51168\n",
      "Epoch: 02 [150464/244490 ( 62%)], Train Loss: 1.51090\n",
      "Epoch: 02 [151104/244490 ( 62%)], Train Loss: 1.51068\n",
      "Epoch: 02 [151744/244490 ( 62%)], Train Loss: 1.51042\n",
      "Epoch: 02 [152384/244490 ( 62%)], Train Loss: 1.50986\n",
      "Epoch: 02 [153024/244490 ( 63%)], Train Loss: 1.50979\n",
      "Epoch: 02 [153664/244490 ( 63%)], Train Loss: 1.50917\n",
      "Epoch: 02 [154304/244490 ( 63%)], Train Loss: 1.50871\n",
      "Epoch: 02 [154944/244490 ( 63%)], Train Loss: 1.50786\n",
      "Epoch: 02 [155584/244490 ( 64%)], Train Loss: 1.50752\n",
      "Epoch: 02 [156224/244490 ( 64%)], Train Loss: 1.50694\n",
      "Epoch: 02 [156864/244490 ( 64%)], Train Loss: 1.50616\n",
      "Epoch: 02 [157504/244490 ( 64%)], Train Loss: 1.50542\n",
      "Epoch: 02 [158144/244490 ( 65%)], Train Loss: 1.50531\n",
      "Epoch: 02 [158784/244490 ( 65%)], Train Loss: 1.50468\n",
      "Epoch: 02 [159424/244490 ( 65%)], Train Loss: 1.50420\n",
      "Epoch: 02 [160064/244490 ( 65%)], Train Loss: 1.50391\n",
      "Epoch: 02 [160704/244490 ( 66%)], Train Loss: 1.50298\n",
      "Epoch: 02 [161344/244490 ( 66%)], Train Loss: 1.50230\n",
      "Epoch: 02 [161984/244490 ( 66%)], Train Loss: 1.50223\n",
      "Epoch: 02 [162624/244490 ( 67%)], Train Loss: 1.50201\n",
      "Epoch: 02 [163264/244490 ( 67%)], Train Loss: 1.50183\n",
      "Epoch: 02 [163904/244490 ( 67%)], Train Loss: 1.50221\n",
      "Epoch: 02 [164544/244490 ( 67%)], Train Loss: 1.50152\n",
      "Epoch: 02 [165184/244490 ( 68%)], Train Loss: 1.50103\n",
      "Epoch: 02 [165824/244490 ( 68%)], Train Loss: 1.50074\n",
      "Epoch: 02 [166464/244490 ( 68%)], Train Loss: 1.50006\n",
      "Epoch: 02 [167104/244490 ( 68%)], Train Loss: 1.49984\n",
      "Epoch: 02 [167744/244490 ( 69%)], Train Loss: 1.49959\n",
      "Epoch: 02 [168384/244490 ( 69%)], Train Loss: 1.49913\n",
      "Epoch: 02 [169024/244490 ( 69%)], Train Loss: 1.49868\n",
      "Epoch: 02 [169664/244490 ( 69%)], Train Loss: 1.49815\n",
      "Epoch: 02 [170304/244490 ( 70%)], Train Loss: 1.49765\n",
      "Epoch: 02 [170944/244490 ( 70%)], Train Loss: 1.49763\n",
      "Epoch: 02 [171584/244490 ( 70%)], Train Loss: 1.49695\n",
      "Epoch: 02 [172224/244490 ( 70%)], Train Loss: 1.49623\n",
      "Epoch: 02 [172864/244490 ( 71%)], Train Loss: 1.49577\n",
      "Epoch: 02 [173504/244490 ( 71%)], Train Loss: 1.49527\n",
      "Epoch: 02 [174144/244490 ( 71%)], Train Loss: 1.49527\n",
      "Epoch: 02 [174784/244490 ( 71%)], Train Loss: 1.49511\n",
      "Epoch: 02 [175424/244490 ( 72%)], Train Loss: 1.49512\n",
      "Epoch: 02 [176064/244490 ( 72%)], Train Loss: 1.49483\n",
      "Epoch: 02 [176704/244490 ( 72%)], Train Loss: 1.49418\n",
      "Epoch: 02 [177344/244490 ( 73%)], Train Loss: 1.49418\n",
      "Epoch: 02 [177984/244490 ( 73%)], Train Loss: 1.49372\n",
      "Epoch: 02 [178624/244490 ( 73%)], Train Loss: 1.49312\n",
      "Epoch: 02 [179264/244490 ( 73%)], Train Loss: 1.49296\n",
      "Epoch: 02 [179904/244490 ( 74%)], Train Loss: 1.49253\n",
      "Epoch: 02 [180544/244490 ( 74%)], Train Loss: 1.49199\n",
      "Epoch: 02 [181184/244490 ( 74%)], Train Loss: 1.49136\n",
      "Epoch: 02 [181824/244490 ( 74%)], Train Loss: 1.49042\n",
      "Epoch: 02 [182464/244490 ( 75%)], Train Loss: 1.48993\n",
      "Epoch: 02 [183104/244490 ( 75%)], Train Loss: 1.48906\n",
      "Epoch: 02 [183744/244490 ( 75%)], Train Loss: 1.48876\n",
      "Epoch: 02 [184384/244490 ( 75%)], Train Loss: 1.48855\n",
      "Epoch: 02 [185024/244490 ( 76%)], Train Loss: 1.48831\n",
      "Epoch: 02 [185664/244490 ( 76%)], Train Loss: 1.48780\n",
      "Epoch: 02 [186304/244490 ( 76%)], Train Loss: 1.48785\n",
      "Epoch: 02 [186944/244490 ( 76%)], Train Loss: 1.48743\n",
      "Epoch: 02 [187584/244490 ( 77%)], Train Loss: 1.48706\n",
      "Epoch: 02 [188224/244490 ( 77%)], Train Loss: 1.48652\n",
      "Epoch: 02 [188864/244490 ( 77%)], Train Loss: 1.48626\n",
      "Epoch: 02 [189504/244490 ( 78%)], Train Loss: 1.48604\n",
      "Epoch: 02 [190144/244490 ( 78%)], Train Loss: 1.48557\n",
      "Epoch: 02 [190784/244490 ( 78%)], Train Loss: 1.48510\n",
      "Epoch: 02 [191424/244490 ( 78%)], Train Loss: 1.48447\n",
      "Epoch: 02 [192064/244490 ( 79%)], Train Loss: 1.48407\n",
      "Epoch: 02 [192704/244490 ( 79%)], Train Loss: 1.48378\n",
      "Epoch: 02 [193344/244490 ( 79%)], Train Loss: 1.48305\n",
      "Epoch: 02 [193984/244490 ( 79%)], Train Loss: 1.48279\n",
      "Epoch: 02 [194624/244490 ( 80%)], Train Loss: 1.48247\n",
      "Epoch: 02 [195264/244490 ( 80%)], Train Loss: 1.48196\n",
      "Epoch: 02 [195904/244490 ( 80%)], Train Loss: 1.48170\n",
      "Epoch: 02 [196544/244490 ( 80%)], Train Loss: 1.48120\n",
      "Epoch: 02 [197184/244490 ( 81%)], Train Loss: 1.48106\n",
      "Epoch: 02 [197824/244490 ( 81%)], Train Loss: 1.48090\n",
      "Epoch: 02 [198464/244490 ( 81%)], Train Loss: 1.48138\n",
      "Epoch: 02 [199104/244490 ( 81%)], Train Loss: 1.48163\n",
      "Epoch: 02 [199744/244490 ( 82%)], Train Loss: 1.48126\n",
      "Epoch: 02 [200384/244490 ( 82%)], Train Loss: 1.48075\n",
      "Epoch: 02 [201024/244490 ( 82%)], Train Loss: 1.48052\n",
      "Epoch: 02 [201664/244490 ( 82%)], Train Loss: 1.47999\n",
      "Epoch: 02 [202304/244490 ( 83%)], Train Loss: 1.47936\n",
      "Epoch: 02 [202944/244490 ( 83%)], Train Loss: 1.47866\n",
      "Epoch: 02 [203584/244490 ( 83%)], Train Loss: 1.47831\n",
      "Epoch: 02 [204224/244490 ( 84%)], Train Loss: 1.47715\n",
      "Epoch: 02 [204864/244490 ( 84%)], Train Loss: 1.47666\n",
      "Epoch: 02 [205504/244490 ( 84%)], Train Loss: 1.47677\n",
      "Epoch: 02 [206144/244490 ( 84%)], Train Loss: 1.47660\n",
      "Epoch: 02 [206784/244490 ( 85%)], Train Loss: 1.47619\n",
      "Epoch: 02 [207424/244490 ( 85%)], Train Loss: 1.47558\n",
      "Epoch: 02 [208064/244490 ( 85%)], Train Loss: 1.47519\n",
      "Epoch: 02 [208704/244490 ( 85%)], Train Loss: 1.47477\n",
      "Epoch: 02 [209344/244490 ( 86%)], Train Loss: 1.47418\n",
      "Epoch: 02 [209984/244490 ( 86%)], Train Loss: 1.47406\n",
      "Epoch: 02 [210624/244490 ( 86%)], Train Loss: 1.47372\n",
      "Epoch: 02 [211264/244490 ( 86%)], Train Loss: 1.47331\n",
      "Epoch: 02 [211904/244490 ( 87%)], Train Loss: 1.47299\n",
      "Epoch: 02 [212544/244490 ( 87%)], Train Loss: 1.47239\n",
      "Epoch: 02 [213184/244490 ( 87%)], Train Loss: 1.47183\n",
      "Epoch: 02 [213824/244490 ( 87%)], Train Loss: 1.47129\n",
      "Epoch: 02 [214464/244490 ( 88%)], Train Loss: 1.47096\n",
      "Epoch: 02 [215104/244490 ( 88%)], Train Loss: 1.47053\n",
      "Epoch: 02 [215744/244490 ( 88%)], Train Loss: 1.47039\n",
      "Epoch: 02 [216384/244490 ( 89%)], Train Loss: 1.47070\n",
      "Epoch: 02 [217024/244490 ( 89%)], Train Loss: 1.47063\n",
      "Epoch: 02 [217664/244490 ( 89%)], Train Loss: 1.47045\n",
      "Epoch: 02 [218304/244490 ( 89%)], Train Loss: 1.47031\n",
      "Epoch: 02 [218944/244490 ( 90%)], Train Loss: 1.47009\n",
      "Epoch: 02 [219584/244490 ( 90%)], Train Loss: 1.46968\n",
      "Epoch: 02 [220224/244490 ( 90%)], Train Loss: 1.46908\n",
      "Epoch: 02 [220864/244490 ( 90%)], Train Loss: 1.46883\n",
      "Epoch: 02 [221504/244490 ( 91%)], Train Loss: 1.46866\n",
      "Epoch: 02 [222144/244490 ( 91%)], Train Loss: 1.46858\n",
      "Epoch: 02 [222784/244490 ( 91%)], Train Loss: 1.46810\n",
      "Epoch: 02 [223424/244490 ( 91%)], Train Loss: 1.46761\n",
      "Epoch: 02 [224064/244490 ( 92%)], Train Loss: 1.46683\n",
      "Epoch: 02 [224704/244490 ( 92%)], Train Loss: 1.46617\n",
      "Epoch: 02 [225344/244490 ( 92%)], Train Loss: 1.46589\n",
      "Epoch: 02 [225984/244490 ( 92%)], Train Loss: 1.46568\n",
      "Epoch: 02 [226624/244490 ( 93%)], Train Loss: 1.46574\n",
      "Epoch: 02 [227264/244490 ( 93%)], Train Loss: 1.46552\n",
      "Epoch: 02 [227904/244490 ( 93%)], Train Loss: 1.46485\n",
      "Epoch: 02 [228544/244490 ( 93%)], Train Loss: 1.46415\n",
      "Epoch: 02 [229184/244490 ( 94%)], Train Loss: 1.46407\n",
      "Epoch: 02 [229824/244490 ( 94%)], Train Loss: 1.46350\n",
      "Epoch: 02 [230464/244490 ( 94%)], Train Loss: 1.46306\n",
      "Epoch: 02 [231104/244490 ( 95%)], Train Loss: 1.46262\n",
      "Epoch: 02 [231744/244490 ( 95%)], Train Loss: 1.46233\n",
      "Epoch: 02 [232384/244490 ( 95%)], Train Loss: 1.46213\n",
      "Epoch: 02 [233024/244490 ( 95%)], Train Loss: 1.46183\n",
      "Epoch: 02 [233664/244490 ( 96%)], Train Loss: 1.46143\n",
      "Epoch: 02 [234304/244490 ( 96%)], Train Loss: 1.46116\n",
      "Epoch: 02 [234944/244490 ( 96%)], Train Loss: 1.46071\n",
      "Epoch: 02 [235584/244490 ( 96%)], Train Loss: 1.46032\n",
      "Epoch: 02 [236224/244490 ( 97%)], Train Loss: 1.45998\n",
      "Epoch: 02 [236864/244490 ( 97%)], Train Loss: 1.45959\n",
      "Epoch: 02 [237504/244490 ( 97%)], Train Loss: 1.45959\n",
      "Epoch: 02 [238144/244490 ( 97%)], Train Loss: 1.45929\n",
      "Epoch: 02 [238784/244490 ( 98%)], Train Loss: 1.45914\n",
      "Epoch: 02 [239424/244490 ( 98%)], Train Loss: 1.45895\n",
      "Epoch: 02 [240064/244490 ( 98%)], Train Loss: 1.45851\n",
      "Epoch: 02 [240704/244490 ( 98%)], Train Loss: 1.45822\n",
      "Epoch: 02 [241344/244490 ( 99%)], Train Loss: 1.45809\n",
      "Epoch: 02 [241984/244490 ( 99%)], Train Loss: 1.45806\n",
      "Epoch: 02 [242624/244490 ( 99%)], Train Loss: 1.45770\n",
      "Epoch: 02 [243264/244490 ( 99%)], Train Loss: 1.45770\n",
      "Epoch: 02 [243904/244490 (100%)], Train Loss: 1.45740\n",
      "Epoch: 02 [244490/244490 (100%)], Train Loss: 1.45698\n",
      "----Validation Results Summary----\n",
      "Epoch: [2] Valid Loss: 1.85306\n",
      "2 Epoch, Best epoch was updated! Valid Loss: 1.85306\n",
      "Saving model checkpoint to output/checkpoint-fold-1.\n",
      "\n",
      "Epoch: 03 [    64/244490 (  0%)], Train Loss: 1.12976\n",
      "Epoch: 03 [   704/244490 (  0%)], Train Loss: 1.34948\n",
      "Epoch: 03 [  1344/244490 (  1%)], Train Loss: 1.30145\n",
      "Epoch: 03 [  1984/244490 (  1%)], Train Loss: 1.32732\n",
      "Epoch: 03 [  2624/244490 (  1%)], Train Loss: 1.30660\n",
      "Epoch: 03 [  3264/244490 (  1%)], Train Loss: 1.29193\n",
      "Epoch: 03 [  3904/244490 (  2%)], Train Loss: 1.28827\n",
      "Epoch: 03 [  4544/244490 (  2%)], Train Loss: 1.28548\n",
      "Epoch: 03 [  5184/244490 (  2%)], Train Loss: 1.27232\n",
      "Epoch: 03 [  5824/244490 (  2%)], Train Loss: 1.26906\n",
      "Epoch: 03 [  6464/244490 (  3%)], Train Loss: 1.28362\n",
      "Epoch: 03 [  7104/244490 (  3%)], Train Loss: 1.30299\n",
      "Epoch: 03 [  7744/244490 (  3%)], Train Loss: 1.30812\n",
      "Epoch: 03 [  8384/244490 (  3%)], Train Loss: 1.31874\n",
      "Epoch: 03 [  9024/244490 (  4%)], Train Loss: 1.32076\n",
      "Epoch: 03 [  9664/244490 (  4%)], Train Loss: 1.32898\n",
      "Epoch: 03 [ 10304/244490 (  4%)], Train Loss: 1.32631\n",
      "Epoch: 03 [ 10944/244490 (  4%)], Train Loss: 1.32437\n",
      "Epoch: 03 [ 11584/244490 (  5%)], Train Loss: 1.32148\n",
      "Epoch: 03 [ 12224/244490 (  5%)], Train Loss: 1.31763\n",
      "Epoch: 03 [ 12864/244490 (  5%)], Train Loss: 1.31589\n",
      "Epoch: 03 [ 13504/244490 (  6%)], Train Loss: 1.31712\n",
      "Epoch: 03 [ 14144/244490 (  6%)], Train Loss: 1.31582\n",
      "Epoch: 03 [ 14784/244490 (  6%)], Train Loss: 1.30945\n",
      "Epoch: 03 [ 15424/244490 (  6%)], Train Loss: 1.30668\n",
      "Epoch: 03 [ 16064/244490 (  7%)], Train Loss: 1.30709\n",
      "Epoch: 03 [ 16704/244490 (  7%)], Train Loss: 1.30468\n",
      "Epoch: 03 [ 17344/244490 (  7%)], Train Loss: 1.30706\n",
      "Epoch: 03 [ 17984/244490 (  7%)], Train Loss: 1.31310\n",
      "Epoch: 03 [ 18624/244490 (  8%)], Train Loss: 1.30922\n",
      "Epoch: 03 [ 19264/244490 (  8%)], Train Loss: 1.30736\n",
      "Epoch: 03 [ 19904/244490 (  8%)], Train Loss: 1.30518\n",
      "Epoch: 03 [ 20544/244490 (  8%)], Train Loss: 1.30530\n",
      "Epoch: 03 [ 21184/244490 (  9%)], Train Loss: 1.30597\n",
      "Epoch: 03 [ 21824/244490 (  9%)], Train Loss: 1.30531\n",
      "Epoch: 03 [ 22464/244490 (  9%)], Train Loss: 1.30896\n",
      "Epoch: 03 [ 23104/244490 (  9%)], Train Loss: 1.30825\n",
      "Epoch: 03 [ 23744/244490 ( 10%)], Train Loss: 1.30690\n",
      "Epoch: 03 [ 24384/244490 ( 10%)], Train Loss: 1.30829\n",
      "Epoch: 03 [ 25024/244490 ( 10%)], Train Loss: 1.30926\n",
      "Epoch: 03 [ 25664/244490 ( 10%)], Train Loss: 1.30918\n",
      "Epoch: 03 [ 26304/244490 ( 11%)], Train Loss: 1.31094\n",
      "Epoch: 03 [ 26944/244490 ( 11%)], Train Loss: 1.30886\n",
      "Epoch: 03 [ 27584/244490 ( 11%)], Train Loss: 1.30880\n",
      "Epoch: 03 [ 28224/244490 ( 12%)], Train Loss: 1.30646\n",
      "Epoch: 03 [ 28864/244490 ( 12%)], Train Loss: 1.30691\n",
      "Epoch: 03 [ 29504/244490 ( 12%)], Train Loss: 1.30598\n",
      "Epoch: 03 [ 30144/244490 ( 12%)], Train Loss: 1.30514\n",
      "Epoch: 03 [ 30784/244490 ( 13%)], Train Loss: 1.30396\n",
      "Epoch: 03 [ 31424/244490 ( 13%)], Train Loss: 1.30477\n",
      "Epoch: 03 [ 32064/244490 ( 13%)], Train Loss: 1.30513\n",
      "Epoch: 03 [ 32704/244490 ( 13%)], Train Loss: 1.30478\n",
      "Epoch: 03 [ 33344/244490 ( 14%)], Train Loss: 1.30410\n",
      "Epoch: 03 [ 33984/244490 ( 14%)], Train Loss: 1.30359\n",
      "Epoch: 03 [ 34624/244490 ( 14%)], Train Loss: 1.30292\n",
      "Epoch: 03 [ 35264/244490 ( 14%)], Train Loss: 1.30177\n",
      "Epoch: 03 [ 35904/244490 ( 15%)], Train Loss: 1.30131\n",
      "Epoch: 03 [ 36544/244490 ( 15%)], Train Loss: 1.30288\n",
      "Epoch: 03 [ 37184/244490 ( 15%)], Train Loss: 1.30471\n",
      "Epoch: 03 [ 37824/244490 ( 15%)], Train Loss: 1.30435\n",
      "Epoch: 03 [ 38464/244490 ( 16%)], Train Loss: 1.30433\n",
      "Epoch: 03 [ 39104/244490 ( 16%)], Train Loss: 1.30275\n",
      "Epoch: 03 [ 39744/244490 ( 16%)], Train Loss: 1.30238\n",
      "Epoch: 03 [ 40384/244490 ( 17%)], Train Loss: 1.30147\n",
      "Epoch: 03 [ 41024/244490 ( 17%)], Train Loss: 1.30133\n",
      "Epoch: 03 [ 41664/244490 ( 17%)], Train Loss: 1.30133\n",
      "Epoch: 03 [ 42304/244490 ( 17%)], Train Loss: 1.30243\n",
      "Epoch: 03 [ 42944/244490 ( 18%)], Train Loss: 1.30334\n",
      "Epoch: 03 [ 43584/244490 ( 18%)], Train Loss: 1.30238\n",
      "Epoch: 03 [ 44224/244490 ( 18%)], Train Loss: 1.30310\n",
      "Epoch: 03 [ 44864/244490 ( 18%)], Train Loss: 1.30285\n",
      "Epoch: 03 [ 45504/244490 ( 19%)], Train Loss: 1.30453\n",
      "Epoch: 03 [ 46144/244490 ( 19%)], Train Loss: 1.30270\n",
      "Epoch: 03 [ 46784/244490 ( 19%)], Train Loss: 1.30094\n",
      "Epoch: 03 [ 47424/244490 ( 19%)], Train Loss: 1.29889\n",
      "Epoch: 03 [ 48064/244490 ( 20%)], Train Loss: 1.29745\n",
      "Epoch: 03 [ 48704/244490 ( 20%)], Train Loss: 1.29762\n",
      "Epoch: 03 [ 49344/244490 ( 20%)], Train Loss: 1.29586\n",
      "Epoch: 03 [ 49984/244490 ( 20%)], Train Loss: 1.29673\n",
      "Epoch: 03 [ 50624/244490 ( 21%)], Train Loss: 1.29596\n",
      "Epoch: 03 [ 51264/244490 ( 21%)], Train Loss: 1.29516\n",
      "Epoch: 03 [ 51904/244490 ( 21%)], Train Loss: 1.29486\n",
      "Epoch: 03 [ 52544/244490 ( 21%)], Train Loss: 1.29315\n",
      "Epoch: 03 [ 53184/244490 ( 22%)], Train Loss: 1.29232\n",
      "Epoch: 03 [ 53824/244490 ( 22%)], Train Loss: 1.28987\n",
      "Epoch: 03 [ 54464/244490 ( 22%)], Train Loss: 1.28982\n",
      "Epoch: 03 [ 55104/244490 ( 23%)], Train Loss: 1.28902\n",
      "Epoch: 03 [ 55744/244490 ( 23%)], Train Loss: 1.28878\n",
      "Epoch: 03 [ 56384/244490 ( 23%)], Train Loss: 1.28773\n",
      "Epoch: 03 [ 57024/244490 ( 23%)], Train Loss: 1.28656\n",
      "Epoch: 03 [ 57664/244490 ( 24%)], Train Loss: 1.28443\n",
      "Epoch: 03 [ 58304/244490 ( 24%)], Train Loss: 1.28312\n",
      "Epoch: 03 [ 58944/244490 ( 24%)], Train Loss: 1.28109\n",
      "Epoch: 03 [ 59584/244490 ( 24%)], Train Loss: 1.27949\n",
      "Epoch: 03 [ 60224/244490 ( 25%)], Train Loss: 1.28015\n",
      "Epoch: 03 [ 60864/244490 ( 25%)], Train Loss: 1.27992\n",
      "Epoch: 03 [ 61504/244490 ( 25%)], Train Loss: 1.27861\n",
      "Epoch: 03 [ 62144/244490 ( 25%)], Train Loss: 1.27697\n",
      "Epoch: 03 [ 62784/244490 ( 26%)], Train Loss: 1.27635\n",
      "Epoch: 03 [ 63424/244490 ( 26%)], Train Loss: 1.27603\n",
      "Epoch: 03 [ 64064/244490 ( 26%)], Train Loss: 1.27542\n",
      "Epoch: 03 [ 64704/244490 ( 26%)], Train Loss: 1.27536\n",
      "Epoch: 03 [ 65344/244490 ( 27%)], Train Loss: 1.27477\n",
      "Epoch: 03 [ 65984/244490 ( 27%)], Train Loss: 1.27450\n",
      "Epoch: 03 [ 66624/244490 ( 27%)], Train Loss: 1.27464\n",
      "Epoch: 03 [ 67264/244490 ( 28%)], Train Loss: 1.27474\n",
      "Epoch: 03 [ 67904/244490 ( 28%)], Train Loss: 1.27522\n",
      "Epoch: 03 [ 68544/244490 ( 28%)], Train Loss: 1.27368\n",
      "Epoch: 03 [ 69184/244490 ( 28%)], Train Loss: 1.27348\n",
      "Epoch: 03 [ 69824/244490 ( 29%)], Train Loss: 1.27364\n",
      "Epoch: 03 [ 70464/244490 ( 29%)], Train Loss: 1.27208\n",
      "Epoch: 03 [ 71104/244490 ( 29%)], Train Loss: 1.27143\n",
      "Epoch: 03 [ 71744/244490 ( 29%)], Train Loss: 1.27017\n",
      "Epoch: 03 [ 72384/244490 ( 30%)], Train Loss: 1.26958\n",
      "Epoch: 03 [ 73024/244490 ( 30%)], Train Loss: 1.26950\n",
      "Epoch: 03 [ 73664/244490 ( 30%)], Train Loss: 1.26918\n",
      "Epoch: 03 [ 74304/244490 ( 30%)], Train Loss: 1.26863\n",
      "Epoch: 03 [ 74944/244490 ( 31%)], Train Loss: 1.26723\n",
      "Epoch: 03 [ 75584/244490 ( 31%)], Train Loss: 1.26714\n",
      "Epoch: 03 [ 76224/244490 ( 31%)], Train Loss: 1.26940\n",
      "Epoch: 03 [ 76864/244490 ( 31%)], Train Loss: 1.27073\n",
      "Epoch: 03 [ 77504/244490 ( 32%)], Train Loss: 1.27073\n",
      "Epoch: 03 [ 78144/244490 ( 32%)], Train Loss: 1.27097\n",
      "Epoch: 03 [ 78784/244490 ( 32%)], Train Loss: 1.27071\n",
      "Epoch: 03 [ 79424/244490 ( 32%)], Train Loss: 1.27086\n",
      "Epoch: 03 [ 80064/244490 ( 33%)], Train Loss: 1.27069\n",
      "Epoch: 03 [ 80704/244490 ( 33%)], Train Loss: 1.27078\n",
      "Epoch: 03 [ 81344/244490 ( 33%)], Train Loss: 1.27046\n",
      "Epoch: 03 [ 81984/244490 ( 34%)], Train Loss: 1.26974\n",
      "Epoch: 03 [ 82624/244490 ( 34%)], Train Loss: 1.27013\n",
      "Epoch: 03 [ 83264/244490 ( 34%)], Train Loss: 1.27010\n",
      "Epoch: 03 [ 83904/244490 ( 34%)], Train Loss: 1.27034\n",
      "Epoch: 03 [ 84544/244490 ( 35%)], Train Loss: 1.27031\n",
      "Epoch: 03 [ 85184/244490 ( 35%)], Train Loss: 1.27004\n",
      "Epoch: 03 [ 85824/244490 ( 35%)], Train Loss: 1.26965\n",
      "Epoch: 03 [ 86464/244490 ( 35%)], Train Loss: 1.26896\n",
      "Epoch: 03 [ 87104/244490 ( 36%)], Train Loss: 1.26879\n",
      "Epoch: 03 [ 87744/244490 ( 36%)], Train Loss: 1.26875\n",
      "Epoch: 03 [ 88384/244490 ( 36%)], Train Loss: 1.26882\n",
      "Epoch: 03 [ 89024/244490 ( 36%)], Train Loss: 1.26801\n",
      "Epoch: 03 [ 89664/244490 ( 37%)], Train Loss: 1.26777\n",
      "Epoch: 03 [ 90304/244490 ( 37%)], Train Loss: 1.26688\n",
      "Epoch: 03 [ 90944/244490 ( 37%)], Train Loss: 1.26657\n",
      "Epoch: 03 [ 91584/244490 ( 37%)], Train Loss: 1.26541\n",
      "Epoch: 03 [ 92224/244490 ( 38%)], Train Loss: 1.26389\n",
      "Epoch: 03 [ 92864/244490 ( 38%)], Train Loss: 1.26297\n",
      "Epoch: 03 [ 93504/244490 ( 38%)], Train Loss: 1.26275\n",
      "Epoch: 03 [ 94144/244490 ( 39%)], Train Loss: 1.26260\n",
      "Epoch: 03 [ 94784/244490 ( 39%)], Train Loss: 1.26263\n",
      "Epoch: 03 [ 95424/244490 ( 39%)], Train Loss: 1.26304\n",
      "Epoch: 03 [ 96064/244490 ( 39%)], Train Loss: 1.26298\n",
      "Epoch: 03 [ 96704/244490 ( 40%)], Train Loss: 1.26337\n",
      "Epoch: 03 [ 97344/244490 ( 40%)], Train Loss: 1.26297\n",
      "Epoch: 03 [ 97984/244490 ( 40%)], Train Loss: 1.26306\n",
      "Epoch: 03 [ 98624/244490 ( 40%)], Train Loss: 1.26230\n",
      "Epoch: 03 [ 99264/244490 ( 41%)], Train Loss: 1.26175\n",
      "Epoch: 03 [ 99904/244490 ( 41%)], Train Loss: 1.26258\n",
      "Epoch: 03 [100544/244490 ( 41%)], Train Loss: 1.26329\n",
      "Epoch: 03 [101184/244490 ( 41%)], Train Loss: 1.26292\n",
      "Epoch: 03 [101824/244490 ( 42%)], Train Loss: 1.26337\n",
      "Epoch: 03 [102464/244490 ( 42%)], Train Loss: 1.26322\n",
      "Epoch: 03 [103104/244490 ( 42%)], Train Loss: 1.26376\n",
      "Epoch: 03 [103744/244490 ( 42%)], Train Loss: 1.26300\n",
      "Epoch: 03 [104384/244490 ( 43%)], Train Loss: 1.26251\n",
      "Epoch: 03 [105024/244490 ( 43%)], Train Loss: 1.26209\n",
      "Epoch: 03 [105664/244490 ( 43%)], Train Loss: 1.26294\n",
      "Epoch: 03 [106304/244490 ( 43%)], Train Loss: 1.26231\n",
      "Epoch: 03 [106944/244490 ( 44%)], Train Loss: 1.26115\n",
      "Epoch: 03 [107584/244490 ( 44%)], Train Loss: 1.26186\n",
      "Epoch: 03 [108224/244490 ( 44%)], Train Loss: 1.26215\n",
      "Epoch: 03 [108864/244490 ( 45%)], Train Loss: 1.26141\n",
      "Epoch: 03 [109504/244490 ( 45%)], Train Loss: 1.26130\n",
      "Epoch: 03 [110144/244490 ( 45%)], Train Loss: 1.26073\n",
      "Epoch: 03 [110784/244490 ( 45%)], Train Loss: 1.26046\n",
      "Epoch: 03 [111424/244490 ( 46%)], Train Loss: 1.26053\n",
      "Epoch: 03 [112064/244490 ( 46%)], Train Loss: 1.26032\n",
      "Epoch: 03 [112704/244490 ( 46%)], Train Loss: 1.26066\n",
      "Epoch: 03 [113344/244490 ( 46%)], Train Loss: 1.26084\n",
      "Epoch: 03 [113984/244490 ( 47%)], Train Loss: 1.26071\n",
      "Epoch: 03 [114624/244490 ( 47%)], Train Loss: 1.26079\n",
      "Epoch: 03 [115264/244490 ( 47%)], Train Loss: 1.26018\n",
      "Epoch: 03 [115904/244490 ( 47%)], Train Loss: 1.25967\n",
      "Epoch: 03 [116544/244490 ( 48%)], Train Loss: 1.25899\n",
      "Epoch: 03 [117184/244490 ( 48%)], Train Loss: 1.25852\n",
      "Epoch: 03 [117824/244490 ( 48%)], Train Loss: 1.25820\n",
      "Epoch: 03 [118464/244490 ( 48%)], Train Loss: 1.25733\n",
      "Epoch: 03 [119104/244490 ( 49%)], Train Loss: 1.25676\n",
      "Epoch: 03 [119744/244490 ( 49%)], Train Loss: 1.25589\n",
      "Epoch: 03 [120384/244490 ( 49%)], Train Loss: 1.25569\n",
      "Epoch: 03 [121024/244490 ( 50%)], Train Loss: 1.25593\n",
      "Epoch: 03 [121664/244490 ( 50%)], Train Loss: 1.25564\n",
      "Epoch: 03 [122304/244490 ( 50%)], Train Loss: 1.25511\n",
      "Epoch: 03 [122944/244490 ( 50%)], Train Loss: 1.25566\n",
      "Epoch: 03 [123584/244490 ( 51%)], Train Loss: 1.25586\n",
      "Epoch: 03 [124224/244490 ( 51%)], Train Loss: 1.25556\n",
      "Epoch: 03 [124864/244490 ( 51%)], Train Loss: 1.25504\n",
      "Epoch: 03 [125504/244490 ( 51%)], Train Loss: 1.25517\n",
      "Epoch: 03 [126144/244490 ( 52%)], Train Loss: 1.25489\n",
      "Epoch: 03 [126784/244490 ( 52%)], Train Loss: 1.25453\n",
      "Epoch: 03 [127424/244490 ( 52%)], Train Loss: 1.25449\n",
      "Epoch: 03 [128064/244490 ( 52%)], Train Loss: 1.25478\n",
      "Epoch: 03 [128704/244490 ( 53%)], Train Loss: 1.25511\n",
      "Epoch: 03 [129344/244490 ( 53%)], Train Loss: 1.25481\n",
      "Epoch: 03 [129984/244490 ( 53%)], Train Loss: 1.25483\n",
      "Epoch: 03 [130624/244490 ( 53%)], Train Loss: 1.25395\n",
      "Epoch: 03 [131264/244490 ( 54%)], Train Loss: 1.25373\n",
      "Epoch: 03 [131904/244490 ( 54%)], Train Loss: 1.25369\n",
      "Epoch: 03 [132544/244490 ( 54%)], Train Loss: 1.25342\n",
      "Epoch: 03 [133184/244490 ( 54%)], Train Loss: 1.25330\n",
      "Epoch: 03 [133824/244490 ( 55%)], Train Loss: 1.25245\n",
      "Epoch: 03 [134464/244490 ( 55%)], Train Loss: 1.25201\n",
      "Epoch: 03 [135104/244490 ( 55%)], Train Loss: 1.25206\n",
      "Epoch: 03 [135744/244490 ( 56%)], Train Loss: 1.25220\n",
      "Epoch: 03 [136384/244490 ( 56%)], Train Loss: 1.25188\n",
      "Epoch: 03 [137024/244490 ( 56%)], Train Loss: 1.25155\n",
      "Epoch: 03 [137664/244490 ( 56%)], Train Loss: 1.25155\n",
      "Epoch: 03 [138304/244490 ( 57%)], Train Loss: 1.25148\n",
      "Epoch: 03 [138944/244490 ( 57%)], Train Loss: 1.25095\n",
      "Epoch: 03 [139584/244490 ( 57%)], Train Loss: 1.25041\n",
      "Epoch: 03 [140224/244490 ( 57%)], Train Loss: 1.25029\n",
      "Epoch: 03 [140864/244490 ( 58%)], Train Loss: 1.24991\n",
      "Epoch: 03 [141504/244490 ( 58%)], Train Loss: 1.25024\n",
      "Epoch: 03 [142144/244490 ( 58%)], Train Loss: 1.24970\n",
      "Epoch: 03 [142784/244490 ( 58%)], Train Loss: 1.24913\n",
      "Epoch: 03 [143424/244490 ( 59%)], Train Loss: 1.24895\n",
      "Epoch: 03 [144064/244490 ( 59%)], Train Loss: 1.24811\n",
      "Epoch: 03 [144704/244490 ( 59%)], Train Loss: 1.24777\n",
      "Epoch: 03 [145344/244490 ( 59%)], Train Loss: 1.24740\n",
      "Epoch: 03 [145984/244490 ( 60%)], Train Loss: 1.24768\n",
      "Epoch: 03 [146624/244490 ( 60%)], Train Loss: 1.24793\n",
      "Epoch: 03 [147264/244490 ( 60%)], Train Loss: 1.24768\n",
      "Epoch: 03 [147904/244490 ( 60%)], Train Loss: 1.24753\n",
      "Epoch: 03 [148544/244490 ( 61%)], Train Loss: 1.24764\n",
      "Epoch: 03 [149184/244490 ( 61%)], Train Loss: 1.24728\n",
      "Epoch: 03 [149824/244490 ( 61%)], Train Loss: 1.24752\n",
      "Epoch: 03 [150464/244490 ( 62%)], Train Loss: 1.24728\n",
      "Epoch: 03 [151104/244490 ( 62%)], Train Loss: 1.24718\n",
      "Epoch: 03 [151744/244490 ( 62%)], Train Loss: 1.24703\n",
      "Epoch: 03 [152384/244490 ( 62%)], Train Loss: 1.24675\n",
      "Epoch: 03 [153024/244490 ( 63%)], Train Loss: 1.24730\n",
      "Epoch: 03 [153664/244490 ( 63%)], Train Loss: 1.24699\n",
      "Epoch: 03 [154304/244490 ( 63%)], Train Loss: 1.24668\n",
      "Epoch: 03 [154944/244490 ( 63%)], Train Loss: 1.24585\n",
      "Epoch: 03 [155584/244490 ( 64%)], Train Loss: 1.24556\n",
      "Epoch: 03 [156224/244490 ( 64%)], Train Loss: 1.24510\n",
      "Epoch: 03 [156864/244490 ( 64%)], Train Loss: 1.24448\n",
      "Epoch: 03 [157504/244490 ( 64%)], Train Loss: 1.24396\n",
      "Epoch: 03 [158144/244490 ( 65%)], Train Loss: 1.24389\n",
      "Epoch: 03 [158784/244490 ( 65%)], Train Loss: 1.24369\n",
      "Epoch: 03 [159424/244490 ( 65%)], Train Loss: 1.24328\n",
      "Epoch: 03 [160064/244490 ( 65%)], Train Loss: 1.24277\n",
      "Epoch: 03 [160704/244490 ( 66%)], Train Loss: 1.24224\n",
      "Epoch: 03 [161344/244490 ( 66%)], Train Loss: 1.24197\n",
      "Epoch: 03 [161984/244490 ( 66%)], Train Loss: 1.24214\n",
      "Epoch: 03 [162624/244490 ( 67%)], Train Loss: 1.24250\n",
      "Epoch: 03 [163264/244490 ( 67%)], Train Loss: 1.24323\n",
      "Epoch: 03 [163904/244490 ( 67%)], Train Loss: 1.24421\n",
      "Epoch: 03 [164544/244490 ( 67%)], Train Loss: 1.24444\n",
      "Epoch: 03 [165184/244490 ( 68%)], Train Loss: 1.24461\n",
      "Epoch: 03 [165824/244490 ( 68%)], Train Loss: 1.24506\n",
      "Epoch: 03 [166464/244490 ( 68%)], Train Loss: 1.24494\n",
      "Epoch: 03 [167104/244490 ( 68%)], Train Loss: 1.24520\n",
      "Epoch: 03 [167744/244490 ( 69%)], Train Loss: 1.24546\n",
      "Epoch: 03 [168384/244490 ( 69%)], Train Loss: 1.24530\n",
      "Epoch: 03 [169024/244490 ( 69%)], Train Loss: 1.24515\n",
      "Epoch: 03 [169664/244490 ( 69%)], Train Loss: 1.24493\n",
      "Epoch: 03 [170304/244490 ( 70%)], Train Loss: 1.24482\n",
      "Epoch: 03 [170944/244490 ( 70%)], Train Loss: 1.24515\n",
      "Epoch: 03 [171584/244490 ( 70%)], Train Loss: 1.24501\n",
      "Epoch: 03 [172224/244490 ( 70%)], Train Loss: 1.24480\n",
      "Epoch: 03 [172864/244490 ( 71%)], Train Loss: 1.24436\n",
      "Epoch: 03 [173504/244490 ( 71%)], Train Loss: 1.24385\n",
      "Epoch: 03 [174144/244490 ( 71%)], Train Loss: 1.24400\n",
      "Epoch: 03 [174784/244490 ( 71%)], Train Loss: 1.24366\n",
      "Epoch: 03 [175424/244490 ( 72%)], Train Loss: 1.24362\n",
      "Epoch: 03 [176064/244490 ( 72%)], Train Loss: 1.24327\n",
      "Epoch: 03 [176704/244490 ( 72%)], Train Loss: 1.24267\n",
      "Epoch: 03 [177344/244490 ( 73%)], Train Loss: 1.24283\n",
      "Epoch: 03 [177984/244490 ( 73%)], Train Loss: 1.24223\n",
      "Epoch: 03 [178624/244490 ( 73%)], Train Loss: 1.24170\n",
      "Epoch: 03 [179264/244490 ( 73%)], Train Loss: 1.24161\n",
      "Epoch: 03 [179904/244490 ( 74%)], Train Loss: 1.24144\n",
      "Epoch: 03 [180544/244490 ( 74%)], Train Loss: 1.24133\n",
      "Epoch: 03 [181184/244490 ( 74%)], Train Loss: 1.24098\n",
      "Epoch: 03 [181824/244490 ( 74%)], Train Loss: 1.24031\n",
      "Epoch: 03 [182464/244490 ( 75%)], Train Loss: 1.24018\n",
      "Epoch: 03 [183104/244490 ( 75%)], Train Loss: 1.24011\n",
      "Epoch: 03 [183744/244490 ( 75%)], Train Loss: 1.24024\n",
      "Epoch: 03 [184384/244490 ( 75%)], Train Loss: 1.24015\n",
      "Epoch: 03 [185024/244490 ( 76%)], Train Loss: 1.24010\n",
      "Epoch: 03 [185664/244490 ( 76%)], Train Loss: 1.24005\n",
      "Epoch: 03 [186304/244490 ( 76%)], Train Loss: 1.23985\n",
      "Epoch: 03 [186944/244490 ( 76%)], Train Loss: 1.23973\n",
      "Epoch: 03 [187584/244490 ( 77%)], Train Loss: 1.23942\n",
      "Epoch: 03 [188224/244490 ( 77%)], Train Loss: 1.23917\n",
      "Epoch: 03 [188864/244490 ( 77%)], Train Loss: 1.23906\n",
      "Epoch: 03 [189504/244490 ( 78%)], Train Loss: 1.23886\n",
      "Epoch: 03 [190144/244490 ( 78%)], Train Loss: 1.23859\n",
      "Epoch: 03 [190784/244490 ( 78%)], Train Loss: 1.23840\n",
      "Epoch: 03 [191424/244490 ( 78%)], Train Loss: 1.23817\n",
      "Epoch: 03 [192064/244490 ( 79%)], Train Loss: 1.23793\n",
      "Epoch: 03 [192704/244490 ( 79%)], Train Loss: 1.23751\n",
      "Epoch: 03 [193344/244490 ( 79%)], Train Loss: 1.23713\n",
      "Epoch: 03 [193984/244490 ( 79%)], Train Loss: 1.23689\n",
      "Epoch: 03 [194624/244490 ( 80%)], Train Loss: 1.23675\n",
      "Epoch: 03 [195264/244490 ( 80%)], Train Loss: 1.23651\n",
      "Epoch: 03 [195904/244490 ( 80%)], Train Loss: 1.23632\n",
      "Epoch: 03 [196544/244490 ( 80%)], Train Loss: 1.23611\n",
      "Epoch: 03 [197184/244490 ( 81%)], Train Loss: 1.23634\n",
      "Epoch: 03 [197824/244490 ( 81%)], Train Loss: 1.23659\n",
      "Epoch: 03 [198464/244490 ( 81%)], Train Loss: 1.23704\n",
      "Epoch: 03 [199104/244490 ( 81%)], Train Loss: 1.23725\n",
      "Epoch: 03 [199744/244490 ( 82%)], Train Loss: 1.23694\n",
      "Epoch: 03 [200384/244490 ( 82%)], Train Loss: 1.23651\n",
      "Epoch: 03 [201024/244490 ( 82%)], Train Loss: 1.23620\n",
      "Epoch: 03 [201664/244490 ( 82%)], Train Loss: 1.23589\n",
      "Epoch: 03 [202304/244490 ( 83%)], Train Loss: 1.23543\n",
      "Epoch: 03 [202944/244490 ( 83%)], Train Loss: 1.23482\n",
      "Epoch: 03 [203584/244490 ( 83%)], Train Loss: 1.23458\n",
      "Epoch: 03 [204224/244490 ( 84%)], Train Loss: 1.23405\n",
      "Epoch: 03 [204864/244490 ( 84%)], Train Loss: 1.23385\n",
      "Epoch: 03 [205504/244490 ( 84%)], Train Loss: 1.23400\n",
      "Epoch: 03 [206144/244490 ( 84%)], Train Loss: 1.23411\n",
      "Epoch: 03 [206784/244490 ( 85%)], Train Loss: 1.23410\n",
      "Epoch: 03 [207424/244490 ( 85%)], Train Loss: 1.23409\n",
      "Epoch: 03 [208064/244490 ( 85%)], Train Loss: 1.23394\n",
      "Epoch: 03 [208704/244490 ( 85%)], Train Loss: 1.23367\n",
      "Epoch: 03 [209344/244490 ( 86%)], Train Loss: 1.23286\n",
      "Epoch: 03 [209984/244490 ( 86%)], Train Loss: 1.23286\n",
      "Epoch: 03 [210624/244490 ( 86%)], Train Loss: 1.23288\n",
      "Epoch: 03 [211264/244490 ( 86%)], Train Loss: 1.23278\n",
      "Epoch: 03 [211904/244490 ( 87%)], Train Loss: 1.23256\n",
      "Epoch: 03 [212544/244490 ( 87%)], Train Loss: 1.23240\n",
      "Epoch: 03 [213184/244490 ( 87%)], Train Loss: 1.23219\n",
      "Epoch: 03 [213824/244490 ( 87%)], Train Loss: 1.23161\n",
      "Epoch: 03 [214464/244490 ( 88%)], Train Loss: 1.23136\n",
      "Epoch: 03 [215104/244490 ( 88%)], Train Loss: 1.23111\n",
      "Epoch: 03 [215744/244490 ( 88%)], Train Loss: 1.23128\n",
      "Epoch: 03 [216384/244490 ( 89%)], Train Loss: 1.23148\n",
      "Epoch: 03 [217024/244490 ( 89%)], Train Loss: 1.23141\n",
      "Epoch: 03 [217664/244490 ( 89%)], Train Loss: 1.23127\n",
      "Epoch: 03 [218304/244490 ( 89%)], Train Loss: 1.23107\n",
      "Epoch: 03 [218944/244490 ( 90%)], Train Loss: 1.23081\n",
      "Epoch: 03 [219584/244490 ( 90%)], Train Loss: 1.23087\n",
      "Epoch: 03 [220224/244490 ( 90%)], Train Loss: 1.23049\n",
      "Epoch: 03 [220864/244490 ( 90%)], Train Loss: 1.23033\n",
      "Epoch: 03 [221504/244490 ( 91%)], Train Loss: 1.23034\n",
      "Epoch: 03 [222144/244490 ( 91%)], Train Loss: 1.23043\n",
      "Epoch: 03 [222784/244490 ( 91%)], Train Loss: 1.23009\n",
      "Epoch: 03 [223424/244490 ( 91%)], Train Loss: 1.22965\n",
      "Epoch: 03 [224064/244490 ( 92%)], Train Loss: 1.22900\n",
      "Epoch: 03 [224704/244490 ( 92%)], Train Loss: 1.22861\n",
      "Epoch: 03 [225344/244490 ( 92%)], Train Loss: 1.22834\n",
      "Epoch: 03 [225984/244490 ( 92%)], Train Loss: 1.22804\n",
      "Epoch: 03 [226624/244490 ( 93%)], Train Loss: 1.22816\n",
      "Epoch: 03 [227264/244490 ( 93%)], Train Loss: 1.22797\n",
      "Epoch: 03 [227904/244490 ( 93%)], Train Loss: 1.22755\n",
      "Epoch: 03 [228544/244490 ( 93%)], Train Loss: 1.22712\n",
      "Epoch: 03 [229184/244490 ( 94%)], Train Loss: 1.22709\n",
      "Epoch: 03 [229824/244490 ( 94%)], Train Loss: 1.22671\n",
      "Epoch: 03 [230464/244490 ( 94%)], Train Loss: 1.22649\n",
      "Epoch: 03 [231104/244490 ( 95%)], Train Loss: 1.22616\n",
      "Epoch: 03 [231744/244490 ( 95%)], Train Loss: 1.22597\n",
      "Epoch: 03 [232384/244490 ( 95%)], Train Loss: 1.22613\n",
      "Epoch: 03 [233024/244490 ( 95%)], Train Loss: 1.22601\n",
      "Epoch: 03 [233664/244490 ( 96%)], Train Loss: 1.22582\n",
      "Epoch: 03 [234304/244490 ( 96%)], Train Loss: 1.22565\n",
      "Epoch: 03 [234944/244490 ( 96%)], Train Loss: 1.22540\n",
      "Epoch: 03 [235584/244490 ( 96%)], Train Loss: 1.22508\n",
      "Epoch: 03 [236224/244490 ( 97%)], Train Loss: 1.22489\n",
      "Epoch: 03 [236864/244490 ( 97%)], Train Loss: 1.22461\n",
      "Epoch: 03 [237504/244490 ( 97%)], Train Loss: 1.22456\n",
      "Epoch: 03 [238144/244490 ( 97%)], Train Loss: 1.22442\n",
      "Epoch: 03 [238784/244490 ( 98%)], Train Loss: 1.22445\n",
      "Epoch: 03 [239424/244490 ( 98%)], Train Loss: 1.22432\n",
      "Epoch: 03 [240064/244490 ( 98%)], Train Loss: 1.22387\n",
      "Epoch: 03 [240704/244490 ( 98%)], Train Loss: 1.22360\n",
      "Epoch: 03 [241344/244490 ( 99%)], Train Loss: 1.22333\n",
      "Epoch: 03 [241984/244490 ( 99%)], Train Loss: 1.22303\n",
      "Epoch: 03 [242624/244490 ( 99%)], Train Loss: 1.22261\n",
      "Epoch: 03 [243264/244490 ( 99%)], Train Loss: 1.22259\n",
      "Epoch: 03 [243904/244490 (100%)], Train Loss: 1.22247\n",
      "Epoch: 03 [244490/244490 (100%)], Train Loss: 1.22243\n",
      "----Validation Results Summary----\n",
      "Epoch: [3] Valid Loss: 1.88877\n",
      "\n",
      "Epoch: 04 [    64/244490 (  0%)], Train Loss: 0.92241\n",
      "Epoch: 04 [   704/244490 (  0%)], Train Loss: 1.18284\n",
      "Epoch: 04 [  1344/244490 (  1%)], Train Loss: 1.25457\n",
      "Epoch: 04 [  1984/244490 (  1%)], Train Loss: 1.37377\n",
      "Epoch: 04 [  2624/244490 (  1%)], Train Loss: 1.43760\n",
      "Epoch: 04 [  3264/244490 (  1%)], Train Loss: 1.49635\n",
      "Epoch: 04 [  3904/244490 (  2%)], Train Loss: 1.54177\n",
      "Epoch: 04 [  4544/244490 (  2%)], Train Loss: 1.58405\n",
      "Epoch: 04 [  5184/244490 (  2%)], Train Loss: 1.61584\n",
      "Epoch: 04 [  5824/244490 (  2%)], Train Loss: 1.65731\n",
      "Epoch: 04 [  6464/244490 (  3%)], Train Loss: 1.69357\n",
      "Epoch: 04 [  7104/244490 (  3%)], Train Loss: 1.71786\n",
      "Epoch: 04 [  7744/244490 (  3%)], Train Loss: 1.73654\n",
      "Epoch: 04 [  8384/244490 (  3%)], Train Loss: 1.75763\n",
      "Epoch: 04 [  9024/244490 (  4%)], Train Loss: 1.77683\n",
      "Epoch: 04 [  9664/244490 (  4%)], Train Loss: 1.78941\n",
      "Epoch: 04 [ 10304/244490 (  4%)], Train Loss: 1.79998\n",
      "Epoch: 04 [ 10944/244490 (  4%)], Train Loss: 1.80109\n",
      "Epoch: 04 [ 11584/244490 (  5%)], Train Loss: 1.80575\n",
      "Epoch: 04 [ 12224/244490 (  5%)], Train Loss: 1.81123\n",
      "Epoch: 04 [ 12864/244490 (  5%)], Train Loss: 1.81639\n",
      "Epoch: 04 [ 13504/244490 (  6%)], Train Loss: 1.82420\n",
      "Epoch: 04 [ 14144/244490 (  6%)], Train Loss: 1.83516\n",
      "Epoch: 04 [ 14784/244490 (  6%)], Train Loss: 1.84242\n",
      "Epoch: 04 [ 15424/244490 (  6%)], Train Loss: 1.84660\n",
      "Epoch: 04 [ 16064/244490 (  7%)], Train Loss: 1.85547\n",
      "Epoch: 04 [ 16704/244490 (  7%)], Train Loss: 1.86028\n",
      "Epoch: 04 [ 17344/244490 (  7%)], Train Loss: 1.86580\n",
      "Epoch: 04 [ 17984/244490 (  7%)], Train Loss: 1.87240\n",
      "Epoch: 04 [ 18624/244490 (  8%)], Train Loss: 1.87593\n",
      "Epoch: 04 [ 19264/244490 (  8%)], Train Loss: 1.87520\n",
      "Epoch: 04 [ 19904/244490 (  8%)], Train Loss: 1.87682\n",
      "Epoch: 04 [ 20544/244490 (  8%)], Train Loss: 1.88180\n",
      "Epoch: 04 [ 21184/244490 (  9%)], Train Loss: 1.88289\n",
      "Epoch: 04 [ 21824/244490 (  9%)], Train Loss: 1.88234\n",
      "Epoch: 04 [ 22464/244490 (  9%)], Train Loss: 1.88462\n",
      "Epoch: 04 [ 23104/244490 (  9%)], Train Loss: 1.88961\n",
      "Epoch: 04 [ 23744/244490 ( 10%)], Train Loss: 1.89113\n",
      "Epoch: 04 [ 24384/244490 ( 10%)], Train Loss: 1.89202\n",
      "Epoch: 04 [ 25024/244490 ( 10%)], Train Loss: 1.89426\n",
      "Epoch: 04 [ 25664/244490 ( 10%)], Train Loss: 1.89299\n",
      "Epoch: 04 [ 26304/244490 ( 11%)], Train Loss: 1.89662\n",
      "Epoch: 04 [ 26944/244490 ( 11%)], Train Loss: 1.89776\n",
      "Epoch: 04 [ 27584/244490 ( 11%)], Train Loss: 1.89890\n",
      "Epoch: 04 [ 28224/244490 ( 12%)], Train Loss: 1.89754\n",
      "Epoch: 04 [ 28864/244490 ( 12%)], Train Loss: 1.90213\n",
      "Epoch: 04 [ 29504/244490 ( 12%)], Train Loss: 1.90198\n",
      "Epoch: 04 [ 30144/244490 ( 12%)], Train Loss: 1.90157\n",
      "Epoch: 04 [ 30784/244490 ( 13%)], Train Loss: 1.90215\n",
      "Epoch: 04 [ 31424/244490 ( 13%)], Train Loss: 1.90530\n",
      "Epoch: 04 [ 32064/244490 ( 13%)], Train Loss: 1.90691\n",
      "Epoch: 04 [ 32704/244490 ( 13%)], Train Loss: 1.90716\n",
      "Epoch: 04 [ 33344/244490 ( 14%)], Train Loss: 1.90937\n",
      "Epoch: 04 [ 33984/244490 ( 14%)], Train Loss: 1.91014\n",
      "Epoch: 04 [ 34624/244490 ( 14%)], Train Loss: 1.91365\n",
      "Epoch: 04 [ 35264/244490 ( 14%)], Train Loss: 1.91574\n",
      "Epoch: 04 [ 35904/244490 ( 15%)], Train Loss: 1.91904\n",
      "Epoch: 04 [ 36544/244490 ( 15%)], Train Loss: 1.92247\n",
      "Epoch: 04 [ 37184/244490 ( 15%)], Train Loss: 1.92380\n",
      "Epoch: 04 [ 37824/244490 ( 15%)], Train Loss: 1.92509\n",
      "Epoch: 04 [ 38464/244490 ( 16%)], Train Loss: 1.92497\n",
      "Epoch: 04 [ 39104/244490 ( 16%)], Train Loss: 1.92513\n",
      "Epoch: 04 [ 39744/244490 ( 16%)], Train Loss: 1.92564\n",
      "Epoch: 04 [ 40384/244490 ( 17%)], Train Loss: 1.92566\n",
      "Epoch: 04 [ 41024/244490 ( 17%)], Train Loss: 1.92797\n",
      "Epoch: 04 [ 41664/244490 ( 17%)], Train Loss: 1.92645\n",
      "Epoch: 04 [ 42304/244490 ( 17%)], Train Loss: 1.92900\n",
      "Epoch: 04 [ 42944/244490 ( 18%)], Train Loss: 1.93058\n",
      "Epoch: 04 [ 43584/244490 ( 18%)], Train Loss: 1.93141\n",
      "Epoch: 04 [ 44224/244490 ( 18%)], Train Loss: 1.93293\n",
      "Epoch: 04 [ 44864/244490 ( 18%)], Train Loss: 1.93411\n",
      "Epoch: 04 [ 45504/244490 ( 19%)], Train Loss: 1.93587\n",
      "Epoch: 04 [ 46144/244490 ( 19%)], Train Loss: 1.93460\n",
      "Epoch: 04 [ 46784/244490 ( 19%)], Train Loss: 1.93417\n",
      "Epoch: 04 [ 47424/244490 ( 19%)], Train Loss: 1.93178\n",
      "Epoch: 04 [ 48064/244490 ( 20%)], Train Loss: 1.93117\n",
      "Epoch: 04 [ 48704/244490 ( 20%)], Train Loss: 1.93196\n",
      "Epoch: 04 [ 49344/244490 ( 20%)], Train Loss: 1.93219\n",
      "Epoch: 04 [ 49984/244490 ( 20%)], Train Loss: 1.93313\n",
      "Epoch: 04 [ 50624/244490 ( 21%)], Train Loss: 1.93281\n",
      "Epoch: 04 [ 51264/244490 ( 21%)], Train Loss: 1.93218\n",
      "Epoch: 04 [ 51904/244490 ( 21%)], Train Loss: 1.93226\n",
      "Epoch: 04 [ 52544/244490 ( 21%)], Train Loss: 1.93219\n",
      "Epoch: 04 [ 53184/244490 ( 22%)], Train Loss: 1.93218\n",
      "Epoch: 04 [ 53824/244490 ( 22%)], Train Loss: 1.93121\n",
      "Epoch: 04 [ 54464/244490 ( 22%)], Train Loss: 1.93126\n",
      "Epoch: 04 [ 55104/244490 ( 23%)], Train Loss: 1.93193\n",
      "Epoch: 04 [ 55744/244490 ( 23%)], Train Loss: 1.93233\n",
      "Epoch: 04 [ 56384/244490 ( 23%)], Train Loss: 1.93252\n",
      "Epoch: 04 [ 57024/244490 ( 23%)], Train Loss: 1.93252\n",
      "Epoch: 04 [ 57664/244490 ( 24%)], Train Loss: 1.93239\n",
      "Epoch: 04 [ 58304/244490 ( 24%)], Train Loss: 1.93271\n",
      "Epoch: 04 [ 58944/244490 ( 24%)], Train Loss: 1.93212\n",
      "Epoch: 04 [ 59584/244490 ( 24%)], Train Loss: 1.93197\n",
      "Epoch: 04 [ 60224/244490 ( 25%)], Train Loss: 1.93349\n",
      "Epoch: 04 [ 60864/244490 ( 25%)], Train Loss: 1.93323\n",
      "Epoch: 04 [ 61504/244490 ( 25%)], Train Loss: 1.93356\n",
      "Epoch: 04 [ 62144/244490 ( 25%)], Train Loss: 1.93364\n",
      "Epoch: 04 [ 62784/244490 ( 26%)], Train Loss: 1.93288\n",
      "Epoch: 04 [ 63424/244490 ( 26%)], Train Loss: 1.93291\n",
      "Epoch: 04 [ 64064/244490 ( 26%)], Train Loss: 1.93189\n",
      "Epoch: 04 [ 64704/244490 ( 26%)], Train Loss: 1.93266\n",
      "Epoch: 04 [ 65344/244490 ( 27%)], Train Loss: 1.93323\n",
      "Epoch: 04 [ 65984/244490 ( 27%)], Train Loss: 1.93328\n",
      "Epoch: 04 [ 66624/244490 ( 27%)], Train Loss: 1.93294\n",
      "Epoch: 04 [ 67264/244490 ( 28%)], Train Loss: 1.93292\n",
      "Epoch: 04 [ 67904/244490 ( 28%)], Train Loss: 1.93275\n",
      "Epoch: 04 [ 68544/244490 ( 28%)], Train Loss: 1.93131\n",
      "Epoch: 04 [ 69184/244490 ( 28%)], Train Loss: 1.93220\n",
      "Epoch: 04 [ 69824/244490 ( 29%)], Train Loss: 1.93304\n",
      "Epoch: 04 [ 70464/244490 ( 29%)], Train Loss: 1.93219\n",
      "Epoch: 04 [ 71104/244490 ( 29%)], Train Loss: 1.93238\n",
      "Epoch: 04 [ 71744/244490 ( 29%)], Train Loss: 1.93224\n",
      "Epoch: 04 [ 72384/244490 ( 30%)], Train Loss: 1.93357\n",
      "Epoch: 04 [ 73024/244490 ( 30%)], Train Loss: 1.93386\n",
      "Epoch: 04 [ 73664/244490 ( 30%)], Train Loss: 1.93382\n",
      "Epoch: 04 [ 74304/244490 ( 30%)], Train Loss: 1.93347\n",
      "Epoch: 04 [ 74944/244490 ( 31%)], Train Loss: 1.93299\n",
      "Epoch: 04 [ 75584/244490 ( 31%)], Train Loss: 1.93336\n",
      "Epoch: 04 [ 76224/244490 ( 31%)], Train Loss: 1.93464\n",
      "Epoch: 04 [ 76864/244490 ( 31%)], Train Loss: 1.93520\n",
      "Epoch: 04 [ 77504/244490 ( 32%)], Train Loss: 1.93527\n",
      "Epoch: 04 [ 78144/244490 ( 32%)], Train Loss: 1.93463\n",
      "Epoch: 04 [ 78784/244490 ( 32%)], Train Loss: 1.93440\n",
      "Epoch: 04 [ 79424/244490 ( 32%)], Train Loss: 1.93556\n",
      "Epoch: 04 [ 80064/244490 ( 33%)], Train Loss: 1.93561\n",
      "Epoch: 04 [ 80704/244490 ( 33%)], Train Loss: 1.93624\n",
      "Epoch: 04 [ 81344/244490 ( 33%)], Train Loss: 1.93695\n",
      "Epoch: 04 [ 81984/244490 ( 34%)], Train Loss: 1.93698\n",
      "Epoch: 04 [ 82624/244490 ( 34%)], Train Loss: 1.93774\n",
      "Epoch: 04 [ 83264/244490 ( 34%)], Train Loss: 1.93747\n",
      "Epoch: 04 [ 83904/244490 ( 34%)], Train Loss: 1.93791\n",
      "Epoch: 04 [ 84544/244490 ( 35%)], Train Loss: 1.93762\n",
      "Epoch: 04 [ 85184/244490 ( 35%)], Train Loss: 1.93724\n",
      "Epoch: 04 [ 85824/244490 ( 35%)], Train Loss: 1.93697\n",
      "Epoch: 04 [ 86464/244490 ( 35%)], Train Loss: 1.93677\n",
      "Epoch: 04 [ 87104/244490 ( 36%)], Train Loss: 1.93743\n",
      "Epoch: 04 [ 87744/244490 ( 36%)], Train Loss: 1.93757\n",
      "Epoch: 04 [ 88384/244490 ( 36%)], Train Loss: 1.93746\n",
      "Epoch: 04 [ 89024/244490 ( 36%)], Train Loss: 1.93669\n",
      "Epoch: 04 [ 89664/244490 ( 37%)], Train Loss: 1.93675\n",
      "Epoch: 04 [ 90304/244490 ( 37%)], Train Loss: 1.93727\n",
      "Epoch: 04 [ 90944/244490 ( 37%)], Train Loss: 1.93716\n",
      "Epoch: 04 [ 91584/244490 ( 37%)], Train Loss: 1.93733\n",
      "Epoch: 04 [ 92224/244490 ( 38%)], Train Loss: 1.93661\n",
      "Epoch: 04 [ 92864/244490 ( 38%)], Train Loss: 1.93625\n",
      "Epoch: 04 [ 93504/244490 ( 38%)], Train Loss: 1.93577\n",
      "Epoch: 04 [ 94144/244490 ( 39%)], Train Loss: 1.93563\n",
      "Epoch: 04 [ 94784/244490 ( 39%)], Train Loss: 1.93590\n",
      "Epoch: 04 [ 95424/244490 ( 39%)], Train Loss: 1.93654\n",
      "Epoch: 04 [ 96064/244490 ( 39%)], Train Loss: 1.93613\n",
      "Epoch: 04 [ 96704/244490 ( 40%)], Train Loss: 1.93623\n",
      "Epoch: 04 [ 97344/244490 ( 40%)], Train Loss: 1.93532\n",
      "Epoch: 04 [ 97984/244490 ( 40%)], Train Loss: 1.93533\n",
      "Epoch: 04 [ 98624/244490 ( 40%)], Train Loss: 1.93531\n",
      "Epoch: 04 [ 99264/244490 ( 41%)], Train Loss: 1.93524\n",
      "Epoch: 04 [ 99904/244490 ( 41%)], Train Loss: 1.93643\n",
      "Epoch: 04 [100544/244490 ( 41%)], Train Loss: 1.93639\n",
      "Epoch: 04 [101184/244490 ( 41%)], Train Loss: 1.93611\n",
      "Epoch: 04 [101824/244490 ( 42%)], Train Loss: 1.93614\n",
      "Epoch: 04 [102464/244490 ( 42%)], Train Loss: 1.93542\n",
      "Epoch: 04 [103104/244490 ( 42%)], Train Loss: 1.93616\n",
      "Epoch: 04 [103744/244490 ( 42%)], Train Loss: 1.93633\n",
      "Epoch: 04 [104384/244490 ( 43%)], Train Loss: 1.93567\n",
      "Epoch: 04 [105024/244490 ( 43%)], Train Loss: 1.93547\n",
      "Epoch: 04 [105664/244490 ( 43%)], Train Loss: 1.93697\n",
      "Epoch: 04 [106304/244490 ( 43%)], Train Loss: 1.93645\n",
      "Epoch: 04 [106944/244490 ( 44%)], Train Loss: 1.93545\n",
      "Epoch: 04 [107584/244490 ( 44%)], Train Loss: 1.93562\n",
      "Epoch: 04 [108224/244490 ( 44%)], Train Loss: 1.93582\n",
      "Epoch: 04 [108864/244490 ( 45%)], Train Loss: 1.93564\n",
      "Epoch: 04 [109504/244490 ( 45%)], Train Loss: 1.93502\n",
      "Epoch: 04 [110144/244490 ( 45%)], Train Loss: 1.93518\n",
      "Epoch: 04 [110784/244490 ( 45%)], Train Loss: 1.93485\n",
      "Epoch: 04 [111424/244490 ( 46%)], Train Loss: 1.93491\n",
      "Epoch: 04 [112064/244490 ( 46%)], Train Loss: 1.93424\n",
      "Epoch: 04 [112704/244490 ( 46%)], Train Loss: 1.93493\n",
      "Epoch: 04 [113344/244490 ( 46%)], Train Loss: 1.93540\n",
      "Epoch: 04 [113984/244490 ( 47%)], Train Loss: 1.93527\n",
      "Epoch: 04 [114624/244490 ( 47%)], Train Loss: 1.93480\n",
      "Epoch: 04 [115264/244490 ( 47%)], Train Loss: 1.93468\n",
      "Epoch: 04 [115904/244490 ( 47%)], Train Loss: 1.93459\n",
      "Epoch: 04 [116544/244490 ( 48%)], Train Loss: 1.93419\n",
      "Epoch: 04 [117184/244490 ( 48%)], Train Loss: 1.93331\n",
      "Epoch: 04 [117824/244490 ( 48%)], Train Loss: 1.93299\n",
      "Epoch: 04 [118464/244490 ( 48%)], Train Loss: 1.93237\n",
      "Epoch: 04 [119104/244490 ( 49%)], Train Loss: 1.93196\n",
      "Epoch: 04 [119744/244490 ( 49%)], Train Loss: 1.93168\n",
      "Epoch: 04 [120384/244490 ( 49%)], Train Loss: 1.93200\n",
      "Epoch: 04 [121024/244490 ( 50%)], Train Loss: 1.93224\n",
      "Epoch: 04 [121664/244490 ( 50%)], Train Loss: 1.93204\n",
      "Epoch: 04 [122304/244490 ( 50%)], Train Loss: 1.93216\n",
      "Epoch: 04 [122944/244490 ( 50%)], Train Loss: 1.93267\n",
      "Epoch: 04 [123584/244490 ( 51%)], Train Loss: 1.93259\n",
      "Epoch: 04 [124224/244490 ( 51%)], Train Loss: 1.93192\n",
      "Epoch: 04 [124864/244490 ( 51%)], Train Loss: 1.93157\n",
      "Epoch: 04 [125504/244490 ( 51%)], Train Loss: 1.93250\n",
      "Epoch: 04 [126144/244490 ( 52%)], Train Loss: 1.93267\n",
      "Epoch: 04 [126784/244490 ( 52%)], Train Loss: 1.93254\n",
      "Epoch: 04 [127424/244490 ( 52%)], Train Loss: 1.93232\n",
      "Epoch: 04 [128064/244490 ( 52%)], Train Loss: 1.93183\n",
      "Epoch: 04 [128704/244490 ( 53%)], Train Loss: 1.93170\n",
      "Epoch: 04 [129344/244490 ( 53%)], Train Loss: 1.93159\n",
      "Epoch: 04 [129984/244490 ( 53%)], Train Loss: 1.93153\n",
      "Epoch: 04 [130624/244490 ( 53%)], Train Loss: 1.93134\n",
      "Epoch: 04 [131264/244490 ( 54%)], Train Loss: 1.93127\n",
      "Epoch: 04 [131904/244490 ( 54%)], Train Loss: 1.93151\n",
      "Epoch: 04 [132544/244490 ( 54%)], Train Loss: 1.93081\n",
      "Epoch: 04 [133184/244490 ( 54%)], Train Loss: 1.93082\n",
      "Epoch: 04 [133824/244490 ( 55%)], Train Loss: 1.93022\n",
      "Epoch: 04 [134464/244490 ( 55%)], Train Loss: 1.92941\n",
      "Epoch: 04 [135104/244490 ( 55%)], Train Loss: 1.92983\n",
      "Epoch: 04 [135744/244490 ( 56%)], Train Loss: 1.93021\n",
      "Epoch: 04 [136384/244490 ( 56%)], Train Loss: 1.93031\n",
      "Epoch: 04 [137024/244490 ( 56%)], Train Loss: 1.93017\n",
      "Epoch: 04 [137664/244490 ( 56%)], Train Loss: 1.93021\n",
      "Epoch: 04 [138304/244490 ( 57%)], Train Loss: 1.93044\n",
      "Epoch: 04 [138944/244490 ( 57%)], Train Loss: 1.92996\n",
      "Epoch: 04 [139584/244490 ( 57%)], Train Loss: 1.92950\n",
      "Epoch: 04 [140224/244490 ( 57%)], Train Loss: 1.92921\n",
      "Epoch: 04 [140864/244490 ( 58%)], Train Loss: 1.92944\n",
      "Epoch: 04 [141504/244490 ( 58%)], Train Loss: 1.92937\n",
      "Epoch: 04 [142144/244490 ( 58%)], Train Loss: 1.92894\n",
      "Epoch: 04 [142784/244490 ( 58%)], Train Loss: 1.92894\n",
      "Epoch: 04 [143424/244490 ( 59%)], Train Loss: 1.92894\n",
      "Epoch: 04 [144064/244490 ( 59%)], Train Loss: 1.92812\n",
      "Epoch: 04 [144704/244490 ( 59%)], Train Loss: 1.92785\n",
      "Epoch: 04 [145344/244490 ( 59%)], Train Loss: 1.92749\n",
      "Epoch: 04 [145984/244490 ( 60%)], Train Loss: 1.92725\n",
      "Epoch: 04 [146624/244490 ( 60%)], Train Loss: 1.92762\n",
      "Epoch: 04 [147264/244490 ( 60%)], Train Loss: 1.92706\n",
      "Epoch: 04 [147904/244490 ( 60%)], Train Loss: 1.92625\n",
      "Epoch: 04 [148544/244490 ( 61%)], Train Loss: 1.92646\n",
      "Epoch: 04 [149184/244490 ( 61%)], Train Loss: 1.92604\n",
      "Epoch: 04 [149824/244490 ( 61%)], Train Loss: 1.92563\n",
      "Epoch: 04 [150464/244490 ( 62%)], Train Loss: 1.92517\n",
      "Epoch: 04 [151104/244490 ( 62%)], Train Loss: 1.92561\n",
      "Epoch: 04 [151744/244490 ( 62%)], Train Loss: 1.92569\n",
      "Epoch: 04 [152384/244490 ( 62%)], Train Loss: 1.92556\n",
      "Epoch: 04 [153024/244490 ( 63%)], Train Loss: 1.92604\n",
      "Epoch: 04 [153664/244490 ( 63%)], Train Loss: 1.92558\n",
      "Epoch: 04 [154304/244490 ( 63%)], Train Loss: 1.92555\n",
      "Epoch: 04 [154944/244490 ( 63%)], Train Loss: 1.92513\n",
      "Epoch: 04 [155584/244490 ( 64%)], Train Loss: 1.92492\n",
      "Epoch: 04 [156224/244490 ( 64%)], Train Loss: 1.92467\n",
      "Epoch: 04 [156864/244490 ( 64%)], Train Loss: 1.92438\n",
      "Epoch: 04 [157504/244490 ( 64%)], Train Loss: 1.92411\n",
      "Epoch: 04 [158144/244490 ( 65%)], Train Loss: 1.92433\n",
      "Epoch: 04 [158784/244490 ( 65%)], Train Loss: 1.92401\n",
      "Epoch: 04 [159424/244490 ( 65%)], Train Loss: 1.92429\n",
      "Epoch: 04 [160064/244490 ( 65%)], Train Loss: 1.92399\n",
      "Epoch: 04 [160704/244490 ( 66%)], Train Loss: 1.92338\n",
      "Epoch: 04 [161344/244490 ( 66%)], Train Loss: 1.92302\n",
      "Epoch: 04 [161984/244490 ( 66%)], Train Loss: 1.92282\n",
      "Epoch: 04 [162624/244490 ( 67%)], Train Loss: 1.92271\n",
      "Epoch: 04 [163264/244490 ( 67%)], Train Loss: 1.92295\n",
      "Epoch: 04 [163904/244490 ( 67%)], Train Loss: 1.92343\n",
      "Epoch: 04 [164544/244490 ( 67%)], Train Loss: 1.92269\n",
      "Epoch: 04 [165184/244490 ( 68%)], Train Loss: 1.92251\n",
      "Epoch: 04 [165824/244490 ( 68%)], Train Loss: 1.92257\n",
      "Epoch: 04 [166464/244490 ( 68%)], Train Loss: 1.92220\n",
      "Epoch: 04 [167104/244490 ( 68%)], Train Loss: 1.92226\n",
      "Epoch: 04 [167744/244490 ( 69%)], Train Loss: 1.92213\n",
      "Epoch: 04 [168384/244490 ( 69%)], Train Loss: 1.92192\n",
      "Epoch: 04 [169024/244490 ( 69%)], Train Loss: 1.92168\n",
      "Epoch: 04 [169664/244490 ( 69%)], Train Loss: 1.92109\n",
      "Epoch: 04 [170304/244490 ( 70%)], Train Loss: 1.92096\n",
      "Epoch: 04 [170944/244490 ( 70%)], Train Loss: 1.92125\n",
      "Epoch: 04 [171584/244490 ( 70%)], Train Loss: 1.92085\n",
      "Epoch: 04 [172224/244490 ( 70%)], Train Loss: 1.92050\n",
      "Epoch: 04 [172864/244490 ( 71%)], Train Loss: 1.92068\n",
      "Epoch: 04 [173504/244490 ( 71%)], Train Loss: 1.92056\n",
      "Epoch: 04 [174144/244490 ( 71%)], Train Loss: 1.92077\n",
      "Epoch: 04 [174784/244490 ( 71%)], Train Loss: 1.92078\n",
      "Epoch: 04 [175424/244490 ( 72%)], Train Loss: 1.92064\n",
      "Epoch: 04 [176064/244490 ( 72%)], Train Loss: 1.92025\n",
      "Epoch: 04 [176704/244490 ( 72%)], Train Loss: 1.91987\n",
      "Epoch: 04 [177344/244490 ( 73%)], Train Loss: 1.92015\n",
      "Epoch: 04 [177984/244490 ( 73%)], Train Loss: 1.92011\n",
      "Epoch: 04 [178624/244490 ( 73%)], Train Loss: 1.91962\n",
      "Epoch: 04 [179264/244490 ( 73%)], Train Loss: 1.92016\n",
      "Epoch: 04 [179904/244490 ( 74%)], Train Loss: 1.92001\n",
      "Epoch: 04 [180544/244490 ( 74%)], Train Loss: 1.91969\n",
      "Epoch: 04 [181184/244490 ( 74%)], Train Loss: 1.91923\n",
      "Epoch: 04 [181824/244490 ( 74%)], Train Loss: 1.91879\n",
      "Epoch: 04 [182464/244490 ( 75%)], Train Loss: 1.91844\n",
      "Epoch: 04 [183104/244490 ( 75%)], Train Loss: 1.91796\n",
      "Epoch: 04 [183744/244490 ( 75%)], Train Loss: 1.91786\n",
      "Epoch: 04 [184384/244490 ( 75%)], Train Loss: 1.91771\n",
      "Epoch: 04 [185024/244490 ( 76%)], Train Loss: 1.91755\n",
      "Epoch: 04 [185664/244490 ( 76%)], Train Loss: 1.91680\n",
      "Epoch: 04 [186304/244490 ( 76%)], Train Loss: 1.91688\n",
      "Epoch: 04 [186944/244490 ( 76%)], Train Loss: 1.91666\n",
      "Epoch: 04 [187584/244490 ( 77%)], Train Loss: 1.91654\n",
      "Epoch: 04 [188224/244490 ( 77%)], Train Loss: 1.91639\n",
      "Epoch: 04 [188864/244490 ( 77%)], Train Loss: 1.91641\n",
      "Epoch: 04 [189504/244490 ( 78%)], Train Loss: 1.91625\n",
      "Epoch: 04 [190144/244490 ( 78%)], Train Loss: 1.91600\n",
      "Epoch: 04 [190784/244490 ( 78%)], Train Loss: 1.91589\n",
      "Epoch: 04 [191424/244490 ( 78%)], Train Loss: 1.91565\n",
      "Epoch: 04 [192064/244490 ( 79%)], Train Loss: 1.91549\n",
      "Epoch: 04 [192704/244490 ( 79%)], Train Loss: 1.91544\n",
      "Epoch: 04 [193344/244490 ( 79%)], Train Loss: 1.91535\n",
      "Epoch: 04 [193984/244490 ( 79%)], Train Loss: 1.91517\n",
      "Epoch: 04 [194624/244490 ( 80%)], Train Loss: 1.91508\n",
      "Epoch: 04 [195264/244490 ( 80%)], Train Loss: 1.91469\n",
      "Epoch: 04 [195904/244490 ( 80%)], Train Loss: 1.91440\n",
      "Epoch: 04 [196544/244490 ( 80%)], Train Loss: 1.91397\n",
      "Epoch: 04 [197184/244490 ( 81%)], Train Loss: 1.91377\n",
      "Epoch: 04 [197824/244490 ( 81%)], Train Loss: 1.91386\n",
      "Epoch: 04 [198464/244490 ( 81%)], Train Loss: 1.91388\n",
      "Epoch: 04 [199104/244490 ( 81%)], Train Loss: 1.91405\n",
      "Epoch: 04 [199744/244490 ( 82%)], Train Loss: 1.91404\n",
      "Epoch: 04 [200384/244490 ( 82%)], Train Loss: 1.91351\n",
      "Epoch: 04 [201024/244490 ( 82%)], Train Loss: 1.91363\n",
      "Epoch: 04 [201664/244490 ( 82%)], Train Loss: 1.91330\n",
      "Epoch: 04 [202304/244490 ( 83%)], Train Loss: 1.91298\n",
      "Epoch: 04 [202944/244490 ( 83%)], Train Loss: 1.91268\n",
      "Epoch: 04 [203584/244490 ( 83%)], Train Loss: 1.91276\n",
      "Epoch: 04 [204224/244490 ( 84%)], Train Loss: 1.91197\n",
      "Epoch: 04 [204864/244490 ( 84%)], Train Loss: 1.91165\n",
      "Epoch: 04 [205504/244490 ( 84%)], Train Loss: 1.91200\n",
      "Epoch: 04 [206144/244490 ( 84%)], Train Loss: 1.91195\n",
      "Epoch: 04 [206784/244490 ( 85%)], Train Loss: 1.91168\n",
      "Epoch: 04 [207424/244490 ( 85%)], Train Loss: 1.91117\n",
      "Epoch: 04 [208064/244490 ( 85%)], Train Loss: 1.91103\n",
      "Epoch: 04 [208704/244490 ( 85%)], Train Loss: 1.91080\n",
      "Epoch: 04 [209344/244490 ( 86%)], Train Loss: 1.91035\n",
      "Epoch: 04 [209984/244490 ( 86%)], Train Loss: 1.91005\n",
      "Epoch: 04 [210624/244490 ( 86%)], Train Loss: 1.90982\n",
      "Epoch: 04 [211264/244490 ( 86%)], Train Loss: 1.90973\n",
      "Epoch: 04 [211904/244490 ( 87%)], Train Loss: 1.90971\n",
      "Epoch: 04 [212544/244490 ( 87%)], Train Loss: 1.90967\n",
      "Epoch: 04 [213184/244490 ( 87%)], Train Loss: 1.90957\n",
      "Epoch: 04 [213824/244490 ( 87%)], Train Loss: 1.90926\n",
      "Epoch: 04 [214464/244490 ( 88%)], Train Loss: 1.90908\n",
      "Epoch: 04 [215104/244490 ( 88%)], Train Loss: 1.90876\n",
      "Epoch: 04 [215744/244490 ( 88%)], Train Loss: 1.90844\n",
      "Epoch: 04 [216384/244490 ( 89%)], Train Loss: 1.90885\n",
      "Epoch: 04 [217024/244490 ( 89%)], Train Loss: 1.90879\n",
      "Epoch: 04 [217664/244490 ( 89%)], Train Loss: 1.90859\n",
      "Epoch: 04 [218304/244490 ( 89%)], Train Loss: 1.90853\n",
      "Epoch: 04 [218944/244490 ( 90%)], Train Loss: 1.90845\n",
      "Epoch: 04 [219584/244490 ( 90%)], Train Loss: 1.90839\n",
      "Epoch: 04 [220224/244490 ( 90%)], Train Loss: 1.90793\n",
      "Epoch: 04 [220864/244490 ( 90%)], Train Loss: 1.90782\n",
      "Epoch: 04 [221504/244490 ( 91%)], Train Loss: 1.90807\n",
      "Epoch: 04 [222144/244490 ( 91%)], Train Loss: 1.90836\n",
      "Epoch: 04 [222784/244490 ( 91%)], Train Loss: 1.90815\n",
      "Epoch: 04 [223424/244490 ( 91%)], Train Loss: 1.90810\n",
      "Epoch: 04 [224064/244490 ( 92%)], Train Loss: 1.90762\n",
      "Epoch: 04 [224704/244490 ( 92%)], Train Loss: 1.90710\n",
      "Epoch: 04 [225344/244490 ( 92%)], Train Loss: 1.90718\n",
      "Epoch: 04 [225984/244490 ( 92%)], Train Loss: 1.90703\n",
      "Epoch: 04 [226624/244490 ( 93%)], Train Loss: 1.90712\n",
      "Epoch: 04 [227264/244490 ( 93%)], Train Loss: 1.90707\n",
      "Epoch: 04 [227904/244490 ( 93%)], Train Loss: 1.90669\n",
      "Epoch: 04 [228544/244490 ( 93%)], Train Loss: 1.90645\n",
      "Epoch: 04 [229184/244490 ( 94%)], Train Loss: 1.90631\n",
      "Epoch: 04 [229824/244490 ( 94%)], Train Loss: 1.90608\n",
      "Epoch: 04 [230464/244490 ( 94%)], Train Loss: 1.90608\n",
      "Epoch: 04 [231104/244490 ( 95%)], Train Loss: 1.90574\n",
      "Epoch: 04 [231744/244490 ( 95%)], Train Loss: 1.90575\n",
      "Epoch: 04 [232384/244490 ( 95%)], Train Loss: 1.90590\n",
      "Epoch: 04 [233024/244490 ( 95%)], Train Loss: 1.90571\n",
      "Epoch: 04 [233664/244490 ( 96%)], Train Loss: 1.90553\n",
      "Epoch: 04 [234304/244490 ( 96%)], Train Loss: 1.90526\n",
      "Epoch: 04 [234944/244490 ( 96%)], Train Loss: 1.90502\n",
      "Epoch: 04 [235584/244490 ( 96%)], Train Loss: 1.90483\n",
      "Epoch: 04 [236224/244490 ( 97%)], Train Loss: 1.90438\n",
      "Epoch: 04 [236864/244490 ( 97%)], Train Loss: 1.90416\n",
      "Epoch: 04 [237504/244490 ( 97%)], Train Loss: 1.90402\n",
      "Epoch: 04 [238144/244490 ( 97%)], Train Loss: 1.90352\n",
      "Epoch: 04 [238784/244490 ( 98%)], Train Loss: 1.90361\n",
      "Epoch: 04 [239424/244490 ( 98%)], Train Loss: 1.90385\n",
      "Epoch: 04 [240064/244490 ( 98%)], Train Loss: 1.90346\n",
      "Epoch: 04 [240704/244490 ( 98%)], Train Loss: 1.90358\n",
      "Epoch: 04 [241344/244490 ( 99%)], Train Loss: 1.90351\n",
      "Epoch: 04 [241984/244490 ( 99%)], Train Loss: 1.90324\n",
      "Epoch: 04 [242624/244490 ( 99%)], Train Loss: 1.90301\n",
      "Epoch: 04 [243264/244490 ( 99%)], Train Loss: 1.90305\n",
      "Epoch: 04 [243904/244490 (100%)], Train Loss: 1.90299\n",
      "Epoch: 04 [244490/244490 (100%)], Train Loss: 1.90290\n",
      "----Validation Results Summary----\n",
      "Epoch: [4] Valid Loss: 1.86619\n",
      "\n",
      "Epoch: 05 [    64/244490 (  0%)], Train Loss: 1.47769\n",
      "Epoch: 05 [   704/244490 (  0%)], Train Loss: 1.77752\n",
      "Epoch: 05 [  1344/244490 (  1%)], Train Loss: 1.76898\n",
      "Epoch: 05 [  1984/244490 (  1%)], Train Loss: 1.81207\n",
      "Epoch: 05 [  2624/244490 (  1%)], Train Loss: 1.80294\n",
      "Epoch: 05 [  3264/244490 (  1%)], Train Loss: 1.81971\n",
      "Epoch: 05 [  3904/244490 (  2%)], Train Loss: 1.82789\n",
      "Epoch: 05 [  4544/244490 (  2%)], Train Loss: 1.83482\n",
      "Epoch: 05 [  5184/244490 (  2%)], Train Loss: 1.84132\n",
      "Epoch: 05 [  5824/244490 (  2%)], Train Loss: 1.85396\n",
      "Epoch: 05 [  6464/244490 (  3%)], Train Loss: 1.87430\n",
      "Epoch: 05 [  7104/244490 (  3%)], Train Loss: 1.87590\n",
      "Epoch: 05 [  7744/244490 (  3%)], Train Loss: 1.87406\n",
      "Epoch: 05 [  8384/244490 (  3%)], Train Loss: 1.87687\n",
      "Epoch: 05 [  9024/244490 (  4%)], Train Loss: 1.88164\n",
      "Epoch: 05 [  9664/244490 (  4%)], Train Loss: 1.88871\n",
      "Epoch: 05 [ 10304/244490 (  4%)], Train Loss: 1.89345\n",
      "Epoch: 05 [ 10944/244490 (  4%)], Train Loss: 1.88796\n",
      "Epoch: 05 [ 11584/244490 (  5%)], Train Loss: 1.89095\n",
      "Epoch: 05 [ 12224/244490 (  5%)], Train Loss: 1.89091\n",
      "Epoch: 05 [ 12864/244490 (  5%)], Train Loss: 1.89426\n",
      "Epoch: 05 [ 13504/244490 (  6%)], Train Loss: 1.90056\n",
      "Epoch: 05 [ 14144/244490 (  6%)], Train Loss: 1.90886\n",
      "Epoch: 05 [ 14784/244490 (  6%)], Train Loss: 1.90945\n",
      "Epoch: 05 [ 15424/244490 (  6%)], Train Loss: 1.91221\n",
      "Epoch: 05 [ 16064/244490 (  7%)], Train Loss: 1.91065\n",
      "Epoch: 05 [ 16704/244490 (  7%)], Train Loss: 1.90941\n",
      "Epoch: 05 [ 17344/244490 (  7%)], Train Loss: 1.90964\n",
      "Epoch: 05 [ 17984/244490 (  7%)], Train Loss: 1.91499\n",
      "Epoch: 05 [ 18624/244490 (  8%)], Train Loss: 1.91255\n",
      "Epoch: 05 [ 19264/244490 (  8%)], Train Loss: 1.90970\n",
      "Epoch: 05 [ 19904/244490 (  8%)], Train Loss: 1.91119\n",
      "Epoch: 05 [ 20544/244490 (  8%)], Train Loss: 1.91233\n",
      "Epoch: 05 [ 21184/244490 (  9%)], Train Loss: 1.90976\n",
      "Epoch: 05 [ 21824/244490 (  9%)], Train Loss: 1.90905\n",
      "Epoch: 05 [ 22464/244490 (  9%)], Train Loss: 1.91028\n",
      "Epoch: 05 [ 23104/244490 (  9%)], Train Loss: 1.91119\n",
      "Epoch: 05 [ 23744/244490 ( 10%)], Train Loss: 1.90833\n",
      "Epoch: 05 [ 24384/244490 ( 10%)], Train Loss: 1.90688\n",
      "Epoch: 05 [ 25024/244490 ( 10%)], Train Loss: 1.90691\n",
      "Epoch: 05 [ 25664/244490 ( 10%)], Train Loss: 1.90450\n",
      "Epoch: 05 [ 26304/244490 ( 11%)], Train Loss: 1.90420\n",
      "Epoch: 05 [ 26944/244490 ( 11%)], Train Loss: 1.90255\n",
      "Epoch: 05 [ 27584/244490 ( 11%)], Train Loss: 1.90089\n",
      "Epoch: 05 [ 28224/244490 ( 12%)], Train Loss: 1.89896\n",
      "Epoch: 05 [ 28864/244490 ( 12%)], Train Loss: 1.89994\n",
      "Epoch: 05 [ 29504/244490 ( 12%)], Train Loss: 1.89702\n",
      "Epoch: 05 [ 30144/244490 ( 12%)], Train Loss: 1.89683\n",
      "Epoch: 05 [ 30784/244490 ( 13%)], Train Loss: 1.89705\n",
      "Epoch: 05 [ 31424/244490 ( 13%)], Train Loss: 1.89998\n",
      "Epoch: 05 [ 32064/244490 ( 13%)], Train Loss: 1.90160\n",
      "Epoch: 05 [ 32704/244490 ( 13%)], Train Loss: 1.90070\n",
      "Epoch: 05 [ 33344/244490 ( 14%)], Train Loss: 1.90058\n",
      "Epoch: 05 [ 33984/244490 ( 14%)], Train Loss: 1.90000\n",
      "Epoch: 05 [ 34624/244490 ( 14%)], Train Loss: 1.90092\n",
      "Epoch: 05 [ 35264/244490 ( 14%)], Train Loss: 1.90169\n",
      "Epoch: 05 [ 35904/244490 ( 15%)], Train Loss: 1.90349\n",
      "Epoch: 05 [ 36544/244490 ( 15%)], Train Loss: 1.90527\n",
      "Epoch: 05 [ 37184/244490 ( 15%)], Train Loss: 1.90610\n",
      "Epoch: 05 [ 37824/244490 ( 15%)], Train Loss: 1.90664\n",
      "Epoch: 05 [ 38464/244490 ( 16%)], Train Loss: 1.90683\n",
      "Epoch: 05 [ 39104/244490 ( 16%)], Train Loss: 1.90628\n",
      "Epoch: 05 [ 39744/244490 ( 16%)], Train Loss: 1.90548\n",
      "Epoch: 05 [ 40384/244490 ( 17%)], Train Loss: 1.90504\n",
      "Epoch: 05 [ 41024/244490 ( 17%)], Train Loss: 1.90449\n",
      "Epoch: 05 [ 41664/244490 ( 17%)], Train Loss: 1.90289\n",
      "Epoch: 05 [ 42304/244490 ( 17%)], Train Loss: 1.90402\n",
      "Epoch: 05 [ 42944/244490 ( 18%)], Train Loss: 1.90333\n",
      "Epoch: 05 [ 43584/244490 ( 18%)], Train Loss: 1.90242\n",
      "Epoch: 05 [ 44224/244490 ( 18%)], Train Loss: 1.90342\n",
      "Epoch: 05 [ 44864/244490 ( 18%)], Train Loss: 1.90410\n",
      "Epoch: 05 [ 45504/244490 ( 19%)], Train Loss: 1.90499\n",
      "Epoch: 05 [ 46144/244490 ( 19%)], Train Loss: 1.90313\n",
      "Epoch: 05 [ 46784/244490 ( 19%)], Train Loss: 1.90287\n",
      "Epoch: 05 [ 47424/244490 ( 19%)], Train Loss: 1.90085\n",
      "Epoch: 05 [ 48064/244490 ( 20%)], Train Loss: 1.89987\n",
      "Epoch: 05 [ 48704/244490 ( 20%)], Train Loss: 1.90070\n",
      "Epoch: 05 [ 49344/244490 ( 20%)], Train Loss: 1.90000\n",
      "Epoch: 05 [ 49984/244490 ( 20%)], Train Loss: 1.90010\n",
      "Epoch: 05 [ 50624/244490 ( 21%)], Train Loss: 1.89953\n",
      "Epoch: 05 [ 51264/244490 ( 21%)], Train Loss: 1.89808\n",
      "Epoch: 05 [ 51904/244490 ( 21%)], Train Loss: 1.89864\n",
      "Epoch: 05 [ 52544/244490 ( 21%)], Train Loss: 1.89806\n",
      "Epoch: 05 [ 53184/244490 ( 22%)], Train Loss: 1.89742\n",
      "Epoch: 05 [ 53824/244490 ( 22%)], Train Loss: 1.89646\n",
      "Epoch: 05 [ 54464/244490 ( 22%)], Train Loss: 1.89649\n",
      "Epoch: 05 [ 55104/244490 ( 23%)], Train Loss: 1.89604\n",
      "Epoch: 05 [ 55744/244490 ( 23%)], Train Loss: 1.89623\n",
      "Epoch: 05 [ 56384/244490 ( 23%)], Train Loss: 1.89664\n",
      "Epoch: 05 [ 57024/244490 ( 23%)], Train Loss: 1.89735\n",
      "Epoch: 05 [ 57664/244490 ( 24%)], Train Loss: 1.89750\n",
      "Epoch: 05 [ 58304/244490 ( 24%)], Train Loss: 1.89778\n",
      "Epoch: 05 [ 58944/244490 ( 24%)], Train Loss: 1.89710\n",
      "Epoch: 05 [ 59584/244490 ( 24%)], Train Loss: 1.89762\n",
      "Epoch: 05 [ 60224/244490 ( 25%)], Train Loss: 1.89936\n",
      "Epoch: 05 [ 60864/244490 ( 25%)], Train Loss: 1.89849\n",
      "Epoch: 05 [ 61504/244490 ( 25%)], Train Loss: 1.89788\n",
      "Epoch: 05 [ 62144/244490 ( 25%)], Train Loss: 1.89745\n",
      "Epoch: 05 [ 62784/244490 ( 26%)], Train Loss: 1.89624\n",
      "Epoch: 05 [ 63424/244490 ( 26%)], Train Loss: 1.89597\n",
      "Epoch: 05 [ 64064/244490 ( 26%)], Train Loss: 1.89571\n",
      "Epoch: 05 [ 64704/244490 ( 26%)], Train Loss: 1.89586\n",
      "Epoch: 05 [ 65344/244490 ( 27%)], Train Loss: 1.89637\n",
      "Epoch: 05 [ 65984/244490 ( 27%)], Train Loss: 1.89650\n",
      "Epoch: 05 [ 66624/244490 ( 27%)], Train Loss: 1.89594\n",
      "Epoch: 05 [ 67264/244490 ( 28%)], Train Loss: 1.89531\n",
      "Epoch: 05 [ 67904/244490 ( 28%)], Train Loss: 1.89485\n",
      "Epoch: 05 [ 68544/244490 ( 28%)], Train Loss: 1.89305\n",
      "Epoch: 05 [ 69184/244490 ( 28%)], Train Loss: 1.89325\n",
      "Epoch: 05 [ 69824/244490 ( 29%)], Train Loss: 1.89381\n",
      "Epoch: 05 [ 70464/244490 ( 29%)], Train Loss: 1.89309\n",
      "Epoch: 05 [ 71104/244490 ( 29%)], Train Loss: 1.89360\n",
      "Epoch: 05 [ 71744/244490 ( 29%)], Train Loss: 1.89248\n",
      "Epoch: 05 [ 72384/244490 ( 30%)], Train Loss: 1.89302\n",
      "Epoch: 05 [ 73024/244490 ( 30%)], Train Loss: 1.89332\n",
      "Epoch: 05 [ 73664/244490 ( 30%)], Train Loss: 1.89318\n",
      "Epoch: 05 [ 74304/244490 ( 30%)], Train Loss: 1.89260\n",
      "Epoch: 05 [ 74944/244490 ( 31%)], Train Loss: 1.89267\n",
      "Epoch: 05 [ 75584/244490 ( 31%)], Train Loss: 1.89279\n",
      "Epoch: 05 [ 76224/244490 ( 31%)], Train Loss: 1.89419\n",
      "Epoch: 05 [ 76864/244490 ( 31%)], Train Loss: 1.89474\n",
      "Epoch: 05 [ 77504/244490 ( 32%)], Train Loss: 1.89422\n",
      "Epoch: 05 [ 78144/244490 ( 32%)], Train Loss: 1.89324\n",
      "Epoch: 05 [ 78784/244490 ( 32%)], Train Loss: 1.89260\n",
      "Epoch: 05 [ 79424/244490 ( 32%)], Train Loss: 1.89286\n",
      "Epoch: 05 [ 80064/244490 ( 33%)], Train Loss: 1.89219\n",
      "Epoch: 05 [ 80704/244490 ( 33%)], Train Loss: 1.89307\n",
      "Epoch: 05 [ 81344/244490 ( 33%)], Train Loss: 1.89331\n",
      "Epoch: 05 [ 81984/244490 ( 34%)], Train Loss: 1.89265\n",
      "Epoch: 05 [ 82624/244490 ( 34%)], Train Loss: 1.89345\n",
      "Epoch: 05 [ 83264/244490 ( 34%)], Train Loss: 1.89292\n",
      "Epoch: 05 [ 83904/244490 ( 34%)], Train Loss: 1.89329\n",
      "Epoch: 05 [ 84544/244490 ( 35%)], Train Loss: 1.89331\n",
      "Epoch: 05 [ 85184/244490 ( 35%)], Train Loss: 1.89377\n",
      "Epoch: 05 [ 85824/244490 ( 35%)], Train Loss: 1.89401\n",
      "Epoch: 05 [ 86464/244490 ( 35%)], Train Loss: 1.89434\n",
      "Epoch: 05 [ 87104/244490 ( 36%)], Train Loss: 1.89487\n",
      "Epoch: 05 [ 87744/244490 ( 36%)], Train Loss: 1.89497\n",
      "Epoch: 05 [ 88384/244490 ( 36%)], Train Loss: 1.89500\n",
      "Epoch: 05 [ 89024/244490 ( 36%)], Train Loss: 1.89462\n",
      "Epoch: 05 [ 89664/244490 ( 37%)], Train Loss: 1.89459\n",
      "Epoch: 05 [ 90304/244490 ( 37%)], Train Loss: 1.89502\n",
      "Epoch: 05 [ 90944/244490 ( 37%)], Train Loss: 1.89495\n",
      "Epoch: 05 [ 91584/244490 ( 37%)], Train Loss: 1.89500\n",
      "Epoch: 05 [ 92224/244490 ( 38%)], Train Loss: 1.89428\n",
      "Epoch: 05 [ 92864/244490 ( 38%)], Train Loss: 1.89418\n",
      "Epoch: 05 [ 93504/244490 ( 38%)], Train Loss: 1.89357\n",
      "Epoch: 05 [ 94144/244490 ( 39%)], Train Loss: 1.89357\n",
      "Epoch: 05 [ 94784/244490 ( 39%)], Train Loss: 1.89355\n",
      "Epoch: 05 [ 95424/244490 ( 39%)], Train Loss: 1.89404\n",
      "Epoch: 05 [ 96064/244490 ( 39%)], Train Loss: 1.89346\n",
      "Epoch: 05 [ 96704/244490 ( 40%)], Train Loss: 1.89293\n",
      "Epoch: 05 [ 97344/244490 ( 40%)], Train Loss: 1.89192\n",
      "Epoch: 05 [ 97984/244490 ( 40%)], Train Loss: 1.89165\n",
      "Epoch: 05 [ 98624/244490 ( 40%)], Train Loss: 1.89113\n",
      "Epoch: 05 [ 99264/244490 ( 41%)], Train Loss: 1.89077\n",
      "Epoch: 05 [ 99904/244490 ( 41%)], Train Loss: 1.89190\n",
      "Epoch: 05 [100544/244490 ( 41%)], Train Loss: 1.89224\n",
      "Epoch: 05 [101184/244490 ( 41%)], Train Loss: 1.89207\n",
      "Epoch: 05 [101824/244490 ( 42%)], Train Loss: 1.89234\n",
      "Epoch: 05 [102464/244490 ( 42%)], Train Loss: 1.89188\n",
      "Epoch: 05 [103104/244490 ( 42%)], Train Loss: 1.89248\n",
      "Epoch: 05 [103744/244490 ( 42%)], Train Loss: 1.89230\n",
      "Epoch: 05 [104384/244490 ( 43%)], Train Loss: 1.89143\n",
      "Epoch: 05 [105024/244490 ( 43%)], Train Loss: 1.89103\n",
      "Epoch: 05 [105664/244490 ( 43%)], Train Loss: 1.89220\n",
      "Epoch: 05 [106304/244490 ( 43%)], Train Loss: 1.89153\n",
      "Epoch: 05 [106944/244490 ( 44%)], Train Loss: 1.89108\n",
      "Epoch: 05 [107584/244490 ( 44%)], Train Loss: 1.89141\n",
      "Epoch: 05 [108224/244490 ( 44%)], Train Loss: 1.89193\n",
      "Epoch: 05 [108864/244490 ( 45%)], Train Loss: 1.89184\n",
      "Epoch: 05 [109504/244490 ( 45%)], Train Loss: 1.89190\n",
      "Epoch: 05 [110144/244490 ( 45%)], Train Loss: 1.89183\n",
      "Epoch: 05 [110784/244490 ( 45%)], Train Loss: 1.89158\n",
      "Epoch: 05 [111424/244490 ( 46%)], Train Loss: 1.89172\n",
      "Epoch: 05 [112064/244490 ( 46%)], Train Loss: 1.89056\n",
      "Epoch: 05 [112704/244490 ( 46%)], Train Loss: 1.89117\n",
      "Epoch: 05 [113344/244490 ( 46%)], Train Loss: 1.89142\n",
      "Epoch: 05 [113984/244490 ( 47%)], Train Loss: 1.89141\n",
      "Epoch: 05 [114624/244490 ( 47%)], Train Loss: 1.89131\n",
      "Epoch: 05 [115264/244490 ( 47%)], Train Loss: 1.89135\n",
      "Epoch: 05 [115904/244490 ( 47%)], Train Loss: 1.89094\n",
      "Epoch: 05 [116544/244490 ( 48%)], Train Loss: 1.89068\n",
      "Epoch: 05 [117184/244490 ( 48%)], Train Loss: 1.89001\n",
      "Epoch: 05 [117824/244490 ( 48%)], Train Loss: 1.88957\n",
      "Epoch: 05 [118464/244490 ( 48%)], Train Loss: 1.88903\n",
      "Epoch: 05 [119104/244490 ( 49%)], Train Loss: 1.88874\n",
      "Epoch: 05 [119744/244490 ( 49%)], Train Loss: 1.88855\n",
      "Epoch: 05 [120384/244490 ( 49%)], Train Loss: 1.88895\n",
      "Epoch: 05 [121024/244490 ( 50%)], Train Loss: 1.88908\n",
      "Epoch: 05 [121664/244490 ( 50%)], Train Loss: 1.88906\n",
      "Epoch: 05 [122304/244490 ( 50%)], Train Loss: 1.88901\n",
      "Epoch: 05 [122944/244490 ( 50%)], Train Loss: 1.88969\n",
      "Epoch: 05 [123584/244490 ( 51%)], Train Loss: 1.88914\n",
      "Epoch: 05 [124224/244490 ( 51%)], Train Loss: 1.88857\n",
      "Epoch: 05 [124864/244490 ( 51%)], Train Loss: 1.88806\n",
      "Epoch: 05 [125504/244490 ( 51%)], Train Loss: 1.88878\n",
      "Epoch: 05 [126144/244490 ( 52%)], Train Loss: 1.88880\n",
      "Epoch: 05 [126784/244490 ( 52%)], Train Loss: 1.88864\n",
      "Epoch: 05 [127424/244490 ( 52%)], Train Loss: 1.88834\n",
      "Epoch: 05 [128064/244490 ( 52%)], Train Loss: 1.88806\n",
      "Epoch: 05 [128704/244490 ( 53%)], Train Loss: 1.88748\n",
      "Epoch: 05 [129344/244490 ( 53%)], Train Loss: 1.88741\n",
      "Epoch: 05 [129984/244490 ( 53%)], Train Loss: 1.88723\n",
      "Epoch: 05 [130624/244490 ( 53%)], Train Loss: 1.88658\n",
      "Epoch: 05 [131264/244490 ( 54%)], Train Loss: 1.88643\n",
      "Epoch: 05 [131904/244490 ( 54%)], Train Loss: 1.88692\n",
      "Epoch: 05 [132544/244490 ( 54%)], Train Loss: 1.88657\n",
      "Epoch: 05 [133184/244490 ( 54%)], Train Loss: 1.88635\n",
      "Epoch: 05 [133824/244490 ( 55%)], Train Loss: 1.88560\n",
      "Epoch: 05 [134464/244490 ( 55%)], Train Loss: 1.88487\n",
      "Epoch: 05 [135104/244490 ( 55%)], Train Loss: 1.88534\n",
      "Epoch: 05 [135744/244490 ( 56%)], Train Loss: 1.88573\n",
      "Epoch: 05 [136384/244490 ( 56%)], Train Loss: 1.88582\n",
      "Epoch: 05 [137024/244490 ( 56%)], Train Loss: 1.88541\n",
      "Epoch: 05 [137664/244490 ( 56%)], Train Loss: 1.88500\n",
      "Epoch: 05 [138304/244490 ( 57%)], Train Loss: 1.88521\n",
      "Epoch: 05 [138944/244490 ( 57%)], Train Loss: 1.88478\n",
      "Epoch: 05 [139584/244490 ( 57%)], Train Loss: 1.88427\n",
      "Epoch: 05 [140224/244490 ( 57%)], Train Loss: 1.88396\n",
      "Epoch: 05 [140864/244490 ( 58%)], Train Loss: 1.88407\n",
      "Epoch: 05 [141504/244490 ( 58%)], Train Loss: 1.88397\n",
      "Epoch: 05 [142144/244490 ( 58%)], Train Loss: 1.88353\n",
      "Epoch: 05 [142784/244490 ( 58%)], Train Loss: 1.88342\n",
      "Epoch: 05 [143424/244490 ( 59%)], Train Loss: 1.88328\n",
      "Epoch: 05 [144064/244490 ( 59%)], Train Loss: 1.88263\n",
      "Epoch: 05 [144704/244490 ( 59%)], Train Loss: 1.88231\n",
      "Epoch: 05 [145344/244490 ( 59%)], Train Loss: 1.88209\n",
      "Epoch: 05 [145984/244490 ( 60%)], Train Loss: 1.88211\n",
      "Epoch: 05 [146624/244490 ( 60%)], Train Loss: 1.88214\n",
      "Epoch: 05 [147264/244490 ( 60%)], Train Loss: 1.88130\n",
      "Epoch: 05 [147904/244490 ( 60%)], Train Loss: 1.88062\n",
      "Epoch: 05 [148544/244490 ( 61%)], Train Loss: 1.88081\n",
      "Epoch: 05 [149184/244490 ( 61%)], Train Loss: 1.88034\n",
      "Epoch: 05 [149824/244490 ( 61%)], Train Loss: 1.88004\n",
      "Epoch: 05 [150464/244490 ( 62%)], Train Loss: 1.87958\n",
      "Epoch: 05 [151104/244490 ( 62%)], Train Loss: 1.87996\n",
      "Epoch: 05 [151744/244490 ( 62%)], Train Loss: 1.87999\n",
      "Epoch: 05 [152384/244490 ( 62%)], Train Loss: 1.87995\n",
      "Epoch: 05 [153024/244490 ( 63%)], Train Loss: 1.88034\n",
      "Epoch: 05 [153664/244490 ( 63%)], Train Loss: 1.88000\n",
      "Epoch: 05 [154304/244490 ( 63%)], Train Loss: 1.87995\n",
      "Epoch: 05 [154944/244490 ( 63%)], Train Loss: 1.87923\n",
      "Epoch: 05 [155584/244490 ( 64%)], Train Loss: 1.87895\n",
      "Epoch: 05 [156224/244490 ( 64%)], Train Loss: 1.87862\n",
      "Epoch: 05 [156864/244490 ( 64%)], Train Loss: 1.87836\n",
      "Epoch: 05 [157504/244490 ( 64%)], Train Loss: 1.87792\n",
      "Epoch: 05 [158144/244490 ( 65%)], Train Loss: 1.87822\n",
      "Epoch: 05 [158784/244490 ( 65%)], Train Loss: 1.87787\n",
      "Epoch: 05 [159424/244490 ( 65%)], Train Loss: 1.87803\n",
      "Epoch: 05 [160064/244490 ( 65%)], Train Loss: 1.87782\n",
      "Epoch: 05 [160704/244490 ( 66%)], Train Loss: 1.87725\n",
      "Epoch: 05 [161344/244490 ( 66%)], Train Loss: 1.87689\n",
      "Epoch: 05 [161984/244490 ( 66%)], Train Loss: 1.87677\n",
      "Epoch: 05 [162624/244490 ( 67%)], Train Loss: 1.87635\n",
      "Epoch: 05 [163264/244490 ( 67%)], Train Loss: 1.87659\n",
      "Epoch: 05 [163904/244490 ( 67%)], Train Loss: 1.87731\n",
      "Epoch: 05 [164544/244490 ( 67%)], Train Loss: 1.87654\n",
      "Epoch: 05 [165184/244490 ( 68%)], Train Loss: 1.87606\n",
      "Epoch: 05 [165824/244490 ( 68%)], Train Loss: 1.87624\n",
      "Epoch: 05 [166464/244490 ( 68%)], Train Loss: 1.87577\n",
      "Epoch: 05 [167104/244490 ( 68%)], Train Loss: 1.87569\n",
      "Epoch: 05 [167744/244490 ( 69%)], Train Loss: 1.87542\n",
      "Epoch: 05 [168384/244490 ( 69%)], Train Loss: 1.87516\n",
      "Epoch: 05 [169024/244490 ( 69%)], Train Loss: 1.87481\n",
      "Epoch: 05 [169664/244490 ( 69%)], Train Loss: 1.87428\n",
      "Epoch: 05 [170304/244490 ( 70%)], Train Loss: 1.87402\n",
      "Epoch: 05 [170944/244490 ( 70%)], Train Loss: 1.87420\n",
      "Epoch: 05 [171584/244490 ( 70%)], Train Loss: 1.87379\n",
      "Epoch: 05 [172224/244490 ( 70%)], Train Loss: 1.87329\n",
      "Epoch: 05 [172864/244490 ( 71%)], Train Loss: 1.87368\n",
      "Epoch: 05 [173504/244490 ( 71%)], Train Loss: 1.87343\n",
      "Epoch: 05 [174144/244490 ( 71%)], Train Loss: 1.87348\n",
      "Epoch: 05 [174784/244490 ( 71%)], Train Loss: 1.87365\n",
      "Epoch: 05 [175424/244490 ( 72%)], Train Loss: 1.87344\n",
      "Epoch: 05 [176064/244490 ( 72%)], Train Loss: 1.87306\n",
      "Epoch: 05 [176704/244490 ( 72%)], Train Loss: 1.87268\n",
      "Epoch: 05 [177344/244490 ( 73%)], Train Loss: 1.87301\n",
      "Epoch: 05 [177984/244490 ( 73%)], Train Loss: 1.87285\n",
      "Epoch: 05 [178624/244490 ( 73%)], Train Loss: 1.87214\n",
      "Epoch: 05 [179264/244490 ( 73%)], Train Loss: 1.87272\n",
      "Epoch: 05 [179904/244490 ( 74%)], Train Loss: 1.87221\n",
      "Epoch: 05 [180544/244490 ( 74%)], Train Loss: 1.87152\n",
      "Epoch: 05 [181184/244490 ( 74%)], Train Loss: 1.87100\n",
      "Epoch: 05 [181824/244490 ( 74%)], Train Loss: 1.87046\n",
      "Epoch: 05 [182464/244490 ( 75%)], Train Loss: 1.87008\n",
      "Epoch: 05 [183104/244490 ( 75%)], Train Loss: 1.86964\n",
      "Epoch: 05 [183744/244490 ( 75%)], Train Loss: 1.86959\n",
      "Epoch: 05 [184384/244490 ( 75%)], Train Loss: 1.86950\n",
      "Epoch: 05 [185024/244490 ( 76%)], Train Loss: 1.86952\n",
      "Epoch: 05 [185664/244490 ( 76%)], Train Loss: 1.86893\n",
      "Epoch: 05 [186304/244490 ( 76%)], Train Loss: 1.86889\n",
      "Epoch: 05 [186944/244490 ( 76%)], Train Loss: 1.86870\n",
      "Epoch: 05 [187584/244490 ( 77%)], Train Loss: 1.86878\n",
      "Epoch: 05 [188224/244490 ( 77%)], Train Loss: 1.86875\n",
      "Epoch: 05 [188864/244490 ( 77%)], Train Loss: 1.86884\n",
      "Epoch: 05 [189504/244490 ( 78%)], Train Loss: 1.86849\n",
      "Epoch: 05 [190144/244490 ( 78%)], Train Loss: 1.86821\n",
      "Epoch: 05 [190784/244490 ( 78%)], Train Loss: 1.86786\n",
      "Epoch: 05 [191424/244490 ( 78%)], Train Loss: 1.86738\n",
      "Epoch: 05 [192064/244490 ( 79%)], Train Loss: 1.86706\n",
      "Epoch: 05 [192704/244490 ( 79%)], Train Loss: 1.86684\n",
      "Epoch: 05 [193344/244490 ( 79%)], Train Loss: 1.86630\n",
      "Epoch: 05 [193984/244490 ( 79%)], Train Loss: 1.86608\n",
      "Epoch: 05 [194624/244490 ( 80%)], Train Loss: 1.86598\n",
      "Epoch: 05 [195264/244490 ( 80%)], Train Loss: 1.86552\n",
      "Epoch: 05 [195904/244490 ( 80%)], Train Loss: 1.86527\n",
      "Epoch: 05 [196544/244490 ( 80%)], Train Loss: 1.86482\n",
      "Epoch: 05 [197184/244490 ( 81%)], Train Loss: 1.86438\n",
      "Epoch: 05 [197824/244490 ( 81%)], Train Loss: 1.86433\n",
      "Epoch: 05 [198464/244490 ( 81%)], Train Loss: 1.86422\n",
      "Epoch: 05 [199104/244490 ( 81%)], Train Loss: 1.86432\n",
      "Epoch: 05 [199744/244490 ( 82%)], Train Loss: 1.86424\n",
      "Epoch: 05 [200384/244490 ( 82%)], Train Loss: 1.86374\n",
      "Epoch: 05 [201024/244490 ( 82%)], Train Loss: 1.86384\n",
      "Epoch: 05 [201664/244490 ( 82%)], Train Loss: 1.86362\n",
      "Epoch: 05 [202304/244490 ( 83%)], Train Loss: 1.86324\n",
      "Epoch: 05 [202944/244490 ( 83%)], Train Loss: 1.86283\n",
      "Epoch: 05 [203584/244490 ( 83%)], Train Loss: 1.86289\n",
      "Epoch: 05 [204224/244490 ( 84%)], Train Loss: 1.86217\n",
      "Epoch: 05 [204864/244490 ( 84%)], Train Loss: 1.86177\n",
      "Epoch: 05 [205504/244490 ( 84%)], Train Loss: 1.86219\n",
      "Epoch: 05 [206144/244490 ( 84%)], Train Loss: 1.86202\n",
      "Epoch: 05 [206784/244490 ( 85%)], Train Loss: 1.86182\n",
      "Epoch: 05 [207424/244490 ( 85%)], Train Loss: 1.86129\n",
      "Epoch: 05 [208064/244490 ( 85%)], Train Loss: 1.86112\n",
      "Epoch: 05 [208704/244490 ( 85%)], Train Loss: 1.86094\n",
      "Epoch: 05 [209344/244490 ( 86%)], Train Loss: 1.86051\n",
      "Epoch: 05 [209984/244490 ( 86%)], Train Loss: 1.86018\n",
      "Epoch: 05 [210624/244490 ( 86%)], Train Loss: 1.85990\n",
      "Epoch: 05 [211264/244490 ( 86%)], Train Loss: 1.85971\n",
      "Epoch: 05 [211904/244490 ( 87%)], Train Loss: 1.85991\n",
      "Epoch: 05 [212544/244490 ( 87%)], Train Loss: 1.85960\n",
      "Epoch: 05 [213184/244490 ( 87%)], Train Loss: 1.85977\n",
      "Epoch: 05 [213824/244490 ( 87%)], Train Loss: 1.85954\n",
      "Epoch: 05 [214464/244490 ( 88%)], Train Loss: 1.85923\n",
      "Epoch: 05 [215104/244490 ( 88%)], Train Loss: 1.85896\n",
      "Epoch: 05 [215744/244490 ( 88%)], Train Loss: 1.85885\n",
      "Epoch: 05 [216384/244490 ( 89%)], Train Loss: 1.85915\n",
      "Epoch: 05 [217024/244490 ( 89%)], Train Loss: 1.85895\n",
      "Epoch: 05 [217664/244490 ( 89%)], Train Loss: 1.85872\n",
      "Epoch: 05 [218304/244490 ( 89%)], Train Loss: 1.85861\n",
      "Epoch: 05 [218944/244490 ( 90%)], Train Loss: 1.85848\n",
      "Epoch: 05 [219584/244490 ( 90%)], Train Loss: 1.85834\n",
      "Epoch: 05 [220224/244490 ( 90%)], Train Loss: 1.85795\n",
      "Epoch: 05 [220864/244490 ( 90%)], Train Loss: 1.85803\n",
      "Epoch: 05 [221504/244490 ( 91%)], Train Loss: 1.85825\n",
      "Epoch: 05 [222144/244490 ( 91%)], Train Loss: 1.85846\n",
      "Epoch: 05 [222784/244490 ( 91%)], Train Loss: 1.85843\n",
      "Epoch: 05 [223424/244490 ( 91%)], Train Loss: 1.85867\n",
      "Epoch: 05 [224064/244490 ( 92%)], Train Loss: 1.85816\n",
      "Epoch: 05 [224704/244490 ( 92%)], Train Loss: 1.85763\n",
      "Epoch: 05 [225344/244490 ( 92%)], Train Loss: 1.85783\n",
      "Epoch: 05 [225984/244490 ( 92%)], Train Loss: 1.85751\n",
      "Epoch: 05 [226624/244490 ( 93%)], Train Loss: 1.85758\n",
      "Epoch: 05 [227264/244490 ( 93%)], Train Loss: 1.85756\n",
      "Epoch: 05 [227904/244490 ( 93%)], Train Loss: 1.85725\n",
      "Epoch: 05 [228544/244490 ( 93%)], Train Loss: 1.85688\n",
      "Epoch: 05 [229184/244490 ( 94%)], Train Loss: 1.85690\n",
      "Epoch: 05 [229824/244490 ( 94%)], Train Loss: 1.85679\n",
      "Epoch: 05 [230464/244490 ( 94%)], Train Loss: 1.85662\n",
      "Epoch: 05 [231104/244490 ( 95%)], Train Loss: 1.85630\n",
      "Epoch: 05 [231744/244490 ( 95%)], Train Loss: 1.85616\n",
      "Epoch: 05 [232384/244490 ( 95%)], Train Loss: 1.85621\n",
      "Epoch: 05 [233024/244490 ( 95%)], Train Loss: 1.85599\n",
      "Epoch: 05 [233664/244490 ( 96%)], Train Loss: 1.85576\n",
      "Epoch: 05 [234304/244490 ( 96%)], Train Loss: 1.85553\n",
      "Epoch: 05 [234944/244490 ( 96%)], Train Loss: 1.85521\n",
      "Epoch: 05 [235584/244490 ( 96%)], Train Loss: 1.85494\n",
      "Epoch: 05 [236224/244490 ( 97%)], Train Loss: 1.85440\n",
      "Epoch: 05 [236864/244490 ( 97%)], Train Loss: 1.85418\n",
      "Epoch: 05 [237504/244490 ( 97%)], Train Loss: 1.85408\n",
      "Epoch: 05 [238144/244490 ( 97%)], Train Loss: 1.85364\n",
      "Epoch: 05 [238784/244490 ( 98%)], Train Loss: 1.85362\n",
      "Epoch: 05 [239424/244490 ( 98%)], Train Loss: 1.85379\n",
      "Epoch: 05 [240064/244490 ( 98%)], Train Loss: 1.85350\n",
      "Epoch: 05 [240704/244490 ( 98%)], Train Loss: 1.85364\n",
      "Epoch: 05 [241344/244490 ( 99%)], Train Loss: 1.85344\n",
      "Epoch: 05 [241984/244490 ( 99%)], Train Loss: 1.85324\n",
      "Epoch: 05 [242624/244490 ( 99%)], Train Loss: 1.85283\n",
      "Epoch: 05 [243264/244490 ( 99%)], Train Loss: 1.85276\n",
      "Epoch: 05 [243904/244490 (100%)], Train Loss: 1.85257\n",
      "Epoch: 05 [244490/244490 (100%)], Train Loss: 1.85235\n",
      "----Validation Results Summary----\n",
      "Epoch: [5] Valid Loss: 1.80308\n",
      "5 Epoch, Best epoch was updated! Valid Loss: 1.80308\n",
      "Saving model checkpoint to output/checkpoint-fold-1.\n",
      "\n",
      "----SWA Validation Results Summary----\n",
      "Epoch: [5] Valid Loss: 1.77482\n",
      "\n",
      "Total Training Time: 13124.360096216202secs, Average Training Time per Epoch: 2187.393349369367secs.\n",
      "Total Validation Time: 1849.02254986763secs, Average Validation Time per Epoch: 308.17042497793835secs.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 2\n",
      "--------------------------------------------------\n",
      "Model pushed to 2 GPU(s), type NVIDIA GeForce RTX 3090.\n",
      "Num examples Train= 244490, Num examples Valid=61123\n",
      "Total Training Steps: 22926, Total Warmup Steps: 687\n",
      "Total Training Steps: 22926, Total Warmup Steps: 687, SWA Start Step: 4\n",
      "Epoch: 00 [    64/244490 (  0%)], Train Loss: 5.84667\n",
      "Epoch: 00 [   704/244490 (  0%)], Train Loss: 5.72341\n",
      "Epoch: 00 [  1344/244490 (  1%)], Train Loss: 5.60819\n",
      "Epoch: 00 [  1984/244490 (  1%)], Train Loss: 5.47265\n",
      "Epoch: 00 [  2624/244490 (  1%)], Train Loss: 5.16578\n",
      "Epoch: 00 [  3264/244490 (  1%)], Train Loss: 4.65701\n",
      "Epoch: 00 [  3904/244490 (  2%)], Train Loss: 4.29961\n",
      "Epoch: 00 [  4544/244490 (  2%)], Train Loss: 4.05588\n",
      "Epoch: 00 [  5184/244490 (  2%)], Train Loss: 3.86740\n",
      "Epoch: 00 [  5824/244490 (  2%)], Train Loss: 3.71274\n",
      "Epoch: 00 [  6464/244490 (  3%)], Train Loss: 3.57773\n",
      "Epoch: 00 [  7104/244490 (  3%)], Train Loss: 3.47682\n",
      "Epoch: 00 [  7744/244490 (  3%)], Train Loss: 3.39570\n",
      "Epoch: 00 [  8384/244490 (  3%)], Train Loss: 3.31710\n",
      "Epoch: 00 [  9024/244490 (  4%)], Train Loss: 3.24947\n",
      "Epoch: 00 [  9664/244490 (  4%)], Train Loss: 3.19129\n",
      "Epoch: 00 [ 10304/244490 (  4%)], Train Loss: 3.14187\n",
      "Epoch: 00 [ 10944/244490 (  4%)], Train Loss: 3.10284\n",
      "Epoch: 00 [ 11584/244490 (  5%)], Train Loss: 3.06854\n",
      "Epoch: 00 [ 12224/244490 (  5%)], Train Loss: 3.02690\n",
      "Epoch: 00 [ 12864/244490 (  5%)], Train Loss: 2.99368\n",
      "Epoch: 00 [ 13504/244490 (  6%)], Train Loss: 2.96104\n",
      "Epoch: 00 [ 14144/244490 (  6%)], Train Loss: 2.93430\n",
      "Epoch: 00 [ 14784/244490 (  6%)], Train Loss: 2.90798\n",
      "Epoch: 00 [ 15424/244490 (  6%)], Train Loss: 2.87892\n",
      "Epoch: 00 [ 16064/244490 (  7%)], Train Loss: 2.85635\n",
      "Epoch: 00 [ 16704/244490 (  7%)], Train Loss: 2.84033\n",
      "Epoch: 00 [ 17344/244490 (  7%)], Train Loss: 2.82661\n",
      "Epoch: 00 [ 17984/244490 (  7%)], Train Loss: 2.80747\n",
      "Epoch: 00 [ 18624/244490 (  8%)], Train Loss: 2.79243\n",
      "Epoch: 00 [ 19264/244490 (  8%)], Train Loss: 2.78025\n",
      "Epoch: 00 [ 19904/244490 (  8%)], Train Loss: 2.76847\n",
      "Epoch: 00 [ 20544/244490 (  8%)], Train Loss: 2.75698\n",
      "Epoch: 00 [ 21184/244490 (  9%)], Train Loss: 2.74266\n",
      "Epoch: 00 [ 21824/244490 (  9%)], Train Loss: 2.72985\n",
      "Epoch: 00 [ 22464/244490 (  9%)], Train Loss: 2.71958\n",
      "Epoch: 00 [ 23104/244490 (  9%)], Train Loss: 2.70973\n",
      "Epoch: 00 [ 23744/244490 ( 10%)], Train Loss: 2.69791\n",
      "Epoch: 00 [ 24384/244490 ( 10%)], Train Loss: 2.68562\n",
      "Epoch: 00 [ 25024/244490 ( 10%)], Train Loss: 2.67382\n",
      "Epoch: 00 [ 25664/244490 ( 10%)], Train Loss: 2.66407\n",
      "Epoch: 00 [ 26304/244490 ( 11%)], Train Loss: 2.65436\n",
      "Epoch: 00 [ 26944/244490 ( 11%)], Train Loss: 2.65061\n",
      "Epoch: 00 [ 27584/244490 ( 11%)], Train Loss: 2.64082\n",
      "Epoch: 00 [ 28224/244490 ( 12%)], Train Loss: 2.63156\n",
      "Epoch: 00 [ 28864/244490 ( 12%)], Train Loss: 2.62302\n",
      "Epoch: 00 [ 29504/244490 ( 12%)], Train Loss: 2.61886\n",
      "Epoch: 00 [ 30144/244490 ( 12%)], Train Loss: 2.61521\n",
      "Epoch: 00 [ 30784/244490 ( 13%)], Train Loss: 2.60648\n",
      "Epoch: 00 [ 31424/244490 ( 13%)], Train Loss: 2.60260\n",
      "Epoch: 00 [ 32064/244490 ( 13%)], Train Loss: 2.59765\n",
      "Epoch: 00 [ 32704/244490 ( 13%)], Train Loss: 2.59157\n",
      "Epoch: 00 [ 33344/244490 ( 14%)], Train Loss: 2.58575\n",
      "Epoch: 00 [ 33984/244490 ( 14%)], Train Loss: 2.57808\n",
      "Epoch: 00 [ 34624/244490 ( 14%)], Train Loss: 2.57258\n",
      "Epoch: 00 [ 35264/244490 ( 14%)], Train Loss: 2.56812\n",
      "Epoch: 00 [ 35904/244490 ( 15%)], Train Loss: 2.56124\n",
      "Epoch: 00 [ 36544/244490 ( 15%)], Train Loss: 2.55655\n",
      "Epoch: 00 [ 37184/244490 ( 15%)], Train Loss: 2.55208\n",
      "Epoch: 00 [ 37824/244490 ( 15%)], Train Loss: 2.54807\n",
      "Epoch: 00 [ 38464/244490 ( 16%)], Train Loss: 2.54181\n",
      "Epoch: 00 [ 39104/244490 ( 16%)], Train Loss: 2.53789\n",
      "Epoch: 00 [ 39744/244490 ( 16%)], Train Loss: 2.53454\n",
      "Epoch: 00 [ 40384/244490 ( 17%)], Train Loss: 2.52928\n",
      "Epoch: 00 [ 41024/244490 ( 17%)], Train Loss: 2.52412\n",
      "Epoch: 00 [ 41664/244490 ( 17%)], Train Loss: 2.52081\n",
      "Epoch: 00 [ 42304/244490 ( 17%)], Train Loss: 2.51601\n",
      "Epoch: 00 [ 42944/244490 ( 18%)], Train Loss: 2.51220\n",
      "Epoch: 00 [ 43584/244490 ( 18%)], Train Loss: 2.50738\n",
      "Epoch: 00 [ 44224/244490 ( 18%)], Train Loss: 2.50467\n",
      "Epoch: 00 [ 44864/244490 ( 18%)], Train Loss: 2.50186\n",
      "Epoch: 00 [ 45504/244490 ( 19%)], Train Loss: 2.49993\n",
      "Epoch: 00 [ 46144/244490 ( 19%)], Train Loss: 2.49828\n",
      "Epoch: 00 [ 46784/244490 ( 19%)], Train Loss: 2.49371\n",
      "Epoch: 00 [ 47424/244490 ( 19%)], Train Loss: 2.49004\n",
      "Epoch: 00 [ 48064/244490 ( 20%)], Train Loss: 2.48918\n",
      "Epoch: 00 [ 48704/244490 ( 20%)], Train Loss: 2.48758\n",
      "Epoch: 00 [ 49344/244490 ( 20%)], Train Loss: 2.48336\n",
      "Epoch: 00 [ 49984/244490 ( 20%)], Train Loss: 2.48047\n",
      "Epoch: 00 [ 50624/244490 ( 21%)], Train Loss: 2.47887\n",
      "Epoch: 00 [ 51264/244490 ( 21%)], Train Loss: 2.47503\n",
      "Epoch: 00 [ 51904/244490 ( 21%)], Train Loss: 2.47263\n",
      "Epoch: 00 [ 52544/244490 ( 21%)], Train Loss: 2.46886\n",
      "Epoch: 00 [ 53184/244490 ( 22%)], Train Loss: 2.46484\n",
      "Epoch: 00 [ 53824/244490 ( 22%)], Train Loss: 2.46005\n",
      "Epoch: 00 [ 54464/244490 ( 22%)], Train Loss: 2.45543\n",
      "Epoch: 00 [ 55104/244490 ( 23%)], Train Loss: 2.45243\n",
      "Epoch: 00 [ 55744/244490 ( 23%)], Train Loss: 2.44922\n",
      "Epoch: 00 [ 56384/244490 ( 23%)], Train Loss: 2.44593\n",
      "Epoch: 00 [ 57024/244490 ( 23%)], Train Loss: 2.44398\n",
      "Epoch: 00 [ 57664/244490 ( 24%)], Train Loss: 2.44105\n",
      "Epoch: 00 [ 58304/244490 ( 24%)], Train Loss: 2.43827\n",
      "Epoch: 00 [ 58944/244490 ( 24%)], Train Loss: 2.43421\n",
      "Epoch: 00 [ 59584/244490 ( 24%)], Train Loss: 2.43271\n",
      "Epoch: 00 [ 60224/244490 ( 25%)], Train Loss: 2.43064\n",
      "Epoch: 00 [ 60864/244490 ( 25%)], Train Loss: 2.42661\n",
      "Epoch: 00 [ 61504/244490 ( 25%)], Train Loss: 2.42354\n",
      "Epoch: 00 [ 62144/244490 ( 25%)], Train Loss: 2.42170\n",
      "Epoch: 00 [ 62784/244490 ( 26%)], Train Loss: 2.41931\n",
      "Epoch: 00 [ 63424/244490 ( 26%)], Train Loss: 2.41744\n",
      "Epoch: 00 [ 64064/244490 ( 26%)], Train Loss: 2.41351\n",
      "Epoch: 00 [ 64704/244490 ( 26%)], Train Loss: 2.41217\n",
      "Epoch: 00 [ 65344/244490 ( 27%)], Train Loss: 2.41049\n",
      "Epoch: 00 [ 65984/244490 ( 27%)], Train Loss: 2.40818\n",
      "Epoch: 00 [ 66624/244490 ( 27%)], Train Loss: 2.40630\n",
      "Epoch: 00 [ 67264/244490 ( 28%)], Train Loss: 2.40493\n",
      "Epoch: 00 [ 67904/244490 ( 28%)], Train Loss: 2.40119\n",
      "Epoch: 00 [ 68544/244490 ( 28%)], Train Loss: 2.39783\n",
      "Epoch: 00 [ 69184/244490 ( 28%)], Train Loss: 2.39632\n",
      "Epoch: 00 [ 69824/244490 ( 29%)], Train Loss: 2.39397\n",
      "Epoch: 00 [ 70464/244490 ( 29%)], Train Loss: 2.39281\n",
      "Epoch: 00 [ 71104/244490 ( 29%)], Train Loss: 2.39146\n",
      "Epoch: 00 [ 71744/244490 ( 29%)], Train Loss: 2.38967\n",
      "Epoch: 00 [ 72384/244490 ( 30%)], Train Loss: 2.38805\n",
      "Epoch: 00 [ 73024/244490 ( 30%)], Train Loss: 2.38566\n",
      "Epoch: 00 [ 73664/244490 ( 30%)], Train Loss: 2.38287\n",
      "Epoch: 00 [ 74304/244490 ( 30%)], Train Loss: 2.38085\n",
      "Epoch: 00 [ 74944/244490 ( 31%)], Train Loss: 2.37869\n",
      "Epoch: 00 [ 75584/244490 ( 31%)], Train Loss: 2.37604\n",
      "Epoch: 00 [ 76224/244490 ( 31%)], Train Loss: 2.37335\n",
      "Epoch: 00 [ 76864/244490 ( 31%)], Train Loss: 2.37232\n",
      "Epoch: 00 [ 77504/244490 ( 32%)], Train Loss: 2.37060\n",
      "Epoch: 00 [ 78144/244490 ( 32%)], Train Loss: 2.36928\n",
      "Epoch: 00 [ 78784/244490 ( 32%)], Train Loss: 2.36704\n",
      "Epoch: 00 [ 79424/244490 ( 32%)], Train Loss: 2.36428\n",
      "Epoch: 00 [ 80064/244490 ( 33%)], Train Loss: 2.36261\n",
      "Epoch: 00 [ 80704/244490 ( 33%)], Train Loss: 2.36119\n",
      "Epoch: 00 [ 81344/244490 ( 33%)], Train Loss: 2.35951\n",
      "Epoch: 00 [ 81984/244490 ( 34%)], Train Loss: 2.35780\n",
      "Epoch: 00 [ 82624/244490 ( 34%)], Train Loss: 2.35568\n",
      "Epoch: 00 [ 83264/244490 ( 34%)], Train Loss: 2.35394\n",
      "Epoch: 00 [ 83904/244490 ( 34%)], Train Loss: 2.35432\n",
      "Epoch: 00 [ 84544/244490 ( 35%)], Train Loss: 2.35222\n",
      "Epoch: 00 [ 85184/244490 ( 35%)], Train Loss: 2.35046\n",
      "Epoch: 00 [ 85824/244490 ( 35%)], Train Loss: 2.34832\n",
      "Epoch: 00 [ 86464/244490 ( 35%)], Train Loss: 2.34610\n",
      "Epoch: 00 [ 87104/244490 ( 36%)], Train Loss: 2.34473\n",
      "Epoch: 00 [ 87744/244490 ( 36%)], Train Loss: 2.34258\n",
      "Epoch: 00 [ 88384/244490 ( 36%)], Train Loss: 2.34094\n",
      "Epoch: 00 [ 89024/244490 ( 36%)], Train Loss: 2.33940\n",
      "Epoch: 00 [ 89664/244490 ( 37%)], Train Loss: 2.33749\n",
      "Epoch: 00 [ 90304/244490 ( 37%)], Train Loss: 2.33569\n",
      "Epoch: 00 [ 90944/244490 ( 37%)], Train Loss: 2.33514\n",
      "Epoch: 00 [ 91584/244490 ( 37%)], Train Loss: 2.33386\n",
      "Epoch: 00 [ 92224/244490 ( 38%)], Train Loss: 2.33225\n",
      "Epoch: 00 [ 92864/244490 ( 38%)], Train Loss: 2.33021\n",
      "Epoch: 00 [ 93504/244490 ( 38%)], Train Loss: 2.32824\n",
      "Epoch: 00 [ 94144/244490 ( 39%)], Train Loss: 2.32646\n",
      "Epoch: 00 [ 94784/244490 ( 39%)], Train Loss: 2.32546\n",
      "Epoch: 00 [ 95424/244490 ( 39%)], Train Loss: 2.32357\n",
      "Epoch: 00 [ 96064/244490 ( 39%)], Train Loss: 2.32138\n",
      "Epoch: 00 [ 96704/244490 ( 40%)], Train Loss: 2.32012\n",
      "Epoch: 00 [ 97344/244490 ( 40%)], Train Loss: 2.31861\n",
      "Epoch: 00 [ 97984/244490 ( 40%)], Train Loss: 2.31664\n",
      "Epoch: 00 [ 98624/244490 ( 40%)], Train Loss: 2.31502\n",
      "Epoch: 00 [ 99264/244490 ( 41%)], Train Loss: 2.31399\n",
      "Epoch: 00 [ 99904/244490 ( 41%)], Train Loss: 2.31275\n",
      "Epoch: 00 [100544/244490 ( 41%)], Train Loss: 2.31122\n",
      "Epoch: 00 [101184/244490 ( 41%)], Train Loss: 2.31018\n",
      "Epoch: 00 [101824/244490 ( 42%)], Train Loss: 2.30793\n",
      "Epoch: 00 [102464/244490 ( 42%)], Train Loss: 2.30651\n",
      "Epoch: 00 [103104/244490 ( 42%)], Train Loss: 2.30499\n",
      "Epoch: 00 [103744/244490 ( 42%)], Train Loss: 2.30415\n",
      "Epoch: 00 [104384/244490 ( 43%)], Train Loss: 2.30287\n",
      "Epoch: 00 [105024/244490 ( 43%)], Train Loss: 2.30121\n",
      "Epoch: 00 [105664/244490 ( 43%)], Train Loss: 2.30020\n",
      "Epoch: 00 [106304/244490 ( 43%)], Train Loss: 2.29883\n",
      "Epoch: 00 [106944/244490 ( 44%)], Train Loss: 2.29729\n",
      "Epoch: 00 [107584/244490 ( 44%)], Train Loss: 2.29584\n",
      "Epoch: 00 [108224/244490 ( 44%)], Train Loss: 2.29453\n",
      "Epoch: 00 [108864/244490 ( 45%)], Train Loss: 2.29328\n",
      "Epoch: 00 [109504/244490 ( 45%)], Train Loss: 2.29204\n",
      "Epoch: 00 [110144/244490 ( 45%)], Train Loss: 2.29088\n",
      "Epoch: 00 [110784/244490 ( 45%)], Train Loss: 2.28908\n",
      "Epoch: 00 [111424/244490 ( 46%)], Train Loss: 2.28785\n",
      "Epoch: 00 [112064/244490 ( 46%)], Train Loss: 2.28636\n",
      "Epoch: 00 [112704/244490 ( 46%)], Train Loss: 2.28554\n",
      "Epoch: 00 [113344/244490 ( 46%)], Train Loss: 2.28440\n",
      "Epoch: 00 [113984/244490 ( 47%)], Train Loss: 2.28330\n",
      "Epoch: 00 [114624/244490 ( 47%)], Train Loss: 2.28137\n",
      "Epoch: 00 [115264/244490 ( 47%)], Train Loss: 2.28001\n",
      "Epoch: 00 [115904/244490 ( 47%)], Train Loss: 2.27804\n",
      "Epoch: 00 [116544/244490 ( 48%)], Train Loss: 2.27663\n",
      "Epoch: 00 [117184/244490 ( 48%)], Train Loss: 2.27530\n",
      "Epoch: 00 [117824/244490 ( 48%)], Train Loss: 2.27361\n",
      "Epoch: 00 [118464/244490 ( 48%)], Train Loss: 2.27314\n",
      "Epoch: 00 [119104/244490 ( 49%)], Train Loss: 2.27248\n",
      "Epoch: 00 [119744/244490 ( 49%)], Train Loss: 2.27071\n",
      "Epoch: 00 [120384/244490 ( 49%)], Train Loss: 2.26959\n",
      "Epoch: 00 [121024/244490 ( 50%)], Train Loss: 2.26950\n",
      "Epoch: 00 [121664/244490 ( 50%)], Train Loss: 2.26771\n",
      "Epoch: 00 [122304/244490 ( 50%)], Train Loss: 2.26683\n",
      "Epoch: 00 [122944/244490 ( 50%)], Train Loss: 2.26617\n",
      "Epoch: 00 [123584/244490 ( 51%)], Train Loss: 2.26491\n",
      "Epoch: 00 [124224/244490 ( 51%)], Train Loss: 2.26422\n",
      "Epoch: 00 [124864/244490 ( 51%)], Train Loss: 2.26338\n",
      "Epoch: 00 [125504/244490 ( 51%)], Train Loss: 2.26185\n",
      "Epoch: 00 [126144/244490 ( 52%)], Train Loss: 2.26004\n",
      "Epoch: 00 [126784/244490 ( 52%)], Train Loss: 2.25935\n",
      "Epoch: 00 [127424/244490 ( 52%)], Train Loss: 2.25815\n",
      "Epoch: 00 [128064/244490 ( 52%)], Train Loss: 2.25733\n",
      "Epoch: 00 [128704/244490 ( 53%)], Train Loss: 2.25669\n",
      "Epoch: 00 [129344/244490 ( 53%)], Train Loss: 2.25639\n",
      "Epoch: 00 [129984/244490 ( 53%)], Train Loss: 2.25591\n",
      "Epoch: 00 [130624/244490 ( 53%)], Train Loss: 2.25510\n",
      "Epoch: 00 [131264/244490 ( 54%)], Train Loss: 2.25423\n",
      "Epoch: 00 [131904/244490 ( 54%)], Train Loss: 2.25401\n",
      "Epoch: 00 [132544/244490 ( 54%)], Train Loss: 2.25291\n",
      "Epoch: 00 [133184/244490 ( 54%)], Train Loss: 2.25120\n",
      "Epoch: 00 [133824/244490 ( 55%)], Train Loss: 2.25047\n",
      "Epoch: 00 [134464/244490 ( 55%)], Train Loss: 2.24904\n",
      "Epoch: 00 [135104/244490 ( 55%)], Train Loss: 2.24785\n",
      "Epoch: 00 [135744/244490 ( 56%)], Train Loss: 2.24641\n",
      "Epoch: 00 [136384/244490 ( 56%)], Train Loss: 2.24543\n",
      "Epoch: 00 [137024/244490 ( 56%)], Train Loss: 2.24537\n",
      "Epoch: 00 [137664/244490 ( 56%)], Train Loss: 2.24423\n",
      "Epoch: 00 [138304/244490 ( 57%)], Train Loss: 2.24261\n",
      "Epoch: 00 [138944/244490 ( 57%)], Train Loss: 2.24153\n",
      "Epoch: 00 [139584/244490 ( 57%)], Train Loss: 2.24115\n",
      "Epoch: 00 [140224/244490 ( 57%)], Train Loss: 2.23974\n",
      "Epoch: 00 [140864/244490 ( 58%)], Train Loss: 2.23848\n",
      "Epoch: 00 [141504/244490 ( 58%)], Train Loss: 2.23789\n",
      "Epoch: 00 [142144/244490 ( 58%)], Train Loss: 2.23654\n",
      "Epoch: 00 [142784/244490 ( 58%)], Train Loss: 2.23538\n",
      "Epoch: 00 [143424/244490 ( 59%)], Train Loss: 2.23443\n",
      "Epoch: 00 [144064/244490 ( 59%)], Train Loss: 2.23302\n",
      "Epoch: 00 [144704/244490 ( 59%)], Train Loss: 2.23274\n",
      "Epoch: 00 [145344/244490 ( 59%)], Train Loss: 2.23159\n",
      "Epoch: 00 [145984/244490 ( 60%)], Train Loss: 2.23010\n",
      "Epoch: 00 [146624/244490 ( 60%)], Train Loss: 2.22877\n",
      "Epoch: 00 [147264/244490 ( 60%)], Train Loss: 2.22794\n",
      "Epoch: 00 [147904/244490 ( 60%)], Train Loss: 2.22678\n",
      "Epoch: 00 [148544/244490 ( 61%)], Train Loss: 2.22606\n",
      "Epoch: 00 [149184/244490 ( 61%)], Train Loss: 2.22511\n",
      "Epoch: 00 [149824/244490 ( 61%)], Train Loss: 2.22437\n",
      "Epoch: 00 [150464/244490 ( 62%)], Train Loss: 2.22397\n",
      "Epoch: 00 [151104/244490 ( 62%)], Train Loss: 2.22297\n",
      "Epoch: 00 [151744/244490 ( 62%)], Train Loss: 2.22230\n",
      "Epoch: 00 [152384/244490 ( 62%)], Train Loss: 2.22175\n",
      "Epoch: 00 [153024/244490 ( 63%)], Train Loss: 2.22158\n",
      "Epoch: 00 [153664/244490 ( 63%)], Train Loss: 2.22063\n",
      "Epoch: 00 [154304/244490 ( 63%)], Train Loss: 2.21988\n",
      "Epoch: 00 [154944/244490 ( 63%)], Train Loss: 2.21914\n",
      "Epoch: 00 [155584/244490 ( 64%)], Train Loss: 2.21862\n",
      "Epoch: 00 [156224/244490 ( 64%)], Train Loss: 2.21792\n",
      "Epoch: 00 [156864/244490 ( 64%)], Train Loss: 2.21689\n",
      "Epoch: 00 [157504/244490 ( 64%)], Train Loss: 2.21594\n",
      "Epoch: 00 [158144/244490 ( 65%)], Train Loss: 2.21569\n",
      "Epoch: 00 [158784/244490 ( 65%)], Train Loss: 2.21461\n",
      "Epoch: 00 [159424/244490 ( 65%)], Train Loss: 2.21393\n",
      "Epoch: 00 [160064/244490 ( 65%)], Train Loss: 2.21317\n",
      "Epoch: 00 [160704/244490 ( 66%)], Train Loss: 2.21210\n",
      "Epoch: 00 [161344/244490 ( 66%)], Train Loss: 2.21122\n",
      "Epoch: 00 [161984/244490 ( 66%)], Train Loss: 2.21045\n",
      "Epoch: 00 [162624/244490 ( 67%)], Train Loss: 2.21010\n",
      "Epoch: 00 [163264/244490 ( 67%)], Train Loss: 2.20912\n",
      "Epoch: 00 [163904/244490 ( 67%)], Train Loss: 2.20840\n",
      "Epoch: 00 [164544/244490 ( 67%)], Train Loss: 2.20739\n",
      "Epoch: 00 [165184/244490 ( 68%)], Train Loss: 2.20706\n",
      "Epoch: 00 [165824/244490 ( 68%)], Train Loss: 2.20661\n",
      "Epoch: 00 [166464/244490 ( 68%)], Train Loss: 2.20560\n",
      "Epoch: 00 [167104/244490 ( 68%)], Train Loss: 2.20435\n",
      "Epoch: 00 [167744/244490 ( 69%)], Train Loss: 2.20327\n",
      "Epoch: 00 [168384/244490 ( 69%)], Train Loss: 2.20263\n",
      "Epoch: 00 [169024/244490 ( 69%)], Train Loss: 2.20214\n",
      "Epoch: 00 [169664/244490 ( 69%)], Train Loss: 2.20102\n",
      "Epoch: 00 [170304/244490 ( 70%)], Train Loss: 2.20015\n",
      "Epoch: 00 [170944/244490 ( 70%)], Train Loss: 2.19956\n",
      "Epoch: 00 [171584/244490 ( 70%)], Train Loss: 2.19909\n",
      "Epoch: 00 [172224/244490 ( 70%)], Train Loss: 2.19836\n",
      "Epoch: 00 [172864/244490 ( 71%)], Train Loss: 2.19807\n",
      "Epoch: 00 [173504/244490 ( 71%)], Train Loss: 2.19696\n",
      "Epoch: 00 [174144/244490 ( 71%)], Train Loss: 2.19614\n",
      "Epoch: 00 [174784/244490 ( 71%)], Train Loss: 2.19549\n",
      "Epoch: 00 [175424/244490 ( 72%)], Train Loss: 2.19485\n",
      "Epoch: 00 [176064/244490 ( 72%)], Train Loss: 2.19432\n",
      "Epoch: 00 [176704/244490 ( 72%)], Train Loss: 2.19406\n",
      "Epoch: 00 [177344/244490 ( 73%)], Train Loss: 2.19296\n",
      "Epoch: 00 [177984/244490 ( 73%)], Train Loss: 2.19158\n",
      "Epoch: 00 [178624/244490 ( 73%)], Train Loss: 2.19126\n",
      "Epoch: 00 [179264/244490 ( 73%)], Train Loss: 2.19065\n",
      "Epoch: 00 [179904/244490 ( 74%)], Train Loss: 2.19004\n",
      "Epoch: 00 [180544/244490 ( 74%)], Train Loss: 2.18918\n",
      "Epoch: 00 [181184/244490 ( 74%)], Train Loss: 2.18870\n",
      "Epoch: 00 [181824/244490 ( 74%)], Train Loss: 2.18775\n",
      "Epoch: 00 [182464/244490 ( 75%)], Train Loss: 2.18696\n",
      "Epoch: 00 [183104/244490 ( 75%)], Train Loss: 2.18598\n",
      "Epoch: 00 [183744/244490 ( 75%)], Train Loss: 2.18498\n",
      "Epoch: 00 [184384/244490 ( 75%)], Train Loss: 2.18448\n",
      "Epoch: 00 [185024/244490 ( 76%)], Train Loss: 2.18402\n",
      "Epoch: 00 [185664/244490 ( 76%)], Train Loss: 2.18286\n",
      "Epoch: 00 [186304/244490 ( 76%)], Train Loss: 2.18205\n",
      "Epoch: 00 [186944/244490 ( 76%)], Train Loss: 2.18149\n",
      "Epoch: 00 [187584/244490 ( 77%)], Train Loss: 2.18090\n",
      "Epoch: 00 [188224/244490 ( 77%)], Train Loss: 2.18001\n",
      "Epoch: 00 [188864/244490 ( 77%)], Train Loss: 2.17925\n",
      "Epoch: 00 [189504/244490 ( 78%)], Train Loss: 2.17843\n",
      "Epoch: 00 [190144/244490 ( 78%)], Train Loss: 2.17798\n",
      "Epoch: 00 [190784/244490 ( 78%)], Train Loss: 2.17725\n",
      "Epoch: 00 [191424/244490 ( 78%)], Train Loss: 2.17655\n",
      "Epoch: 00 [192064/244490 ( 79%)], Train Loss: 2.17594\n",
      "Epoch: 00 [192704/244490 ( 79%)], Train Loss: 2.17524\n",
      "Epoch: 00 [193344/244490 ( 79%)], Train Loss: 2.17449\n",
      "Epoch: 00 [193984/244490 ( 79%)], Train Loss: 2.17367\n",
      "Epoch: 00 [194624/244490 ( 80%)], Train Loss: 2.17291\n",
      "Epoch: 00 [195264/244490 ( 80%)], Train Loss: 2.17189\n",
      "Epoch: 00 [195904/244490 ( 80%)], Train Loss: 2.17118\n",
      "Epoch: 00 [196544/244490 ( 80%)], Train Loss: 2.17054\n",
      "Epoch: 00 [197184/244490 ( 81%)], Train Loss: 2.17013\n",
      "Epoch: 00 [197824/244490 ( 81%)], Train Loss: 2.16935\n",
      "Epoch: 00 [198464/244490 ( 81%)], Train Loss: 2.16865\n",
      "Epoch: 00 [199104/244490 ( 81%)], Train Loss: 2.16807\n",
      "Epoch: 00 [199744/244490 ( 82%)], Train Loss: 2.16732\n",
      "Epoch: 00 [200384/244490 ( 82%)], Train Loss: 2.16656\n",
      "Epoch: 00 [201024/244490 ( 82%)], Train Loss: 2.16569\n",
      "Epoch: 00 [201664/244490 ( 82%)], Train Loss: 2.16465\n",
      "Epoch: 00 [202304/244490 ( 83%)], Train Loss: 2.16392\n",
      "Epoch: 00 [202944/244490 ( 83%)], Train Loss: 2.16322\n",
      "Epoch: 00 [203584/244490 ( 83%)], Train Loss: 2.16285\n",
      "Epoch: 00 [204224/244490 ( 84%)], Train Loss: 2.16157\n",
      "Epoch: 00 [204864/244490 ( 84%)], Train Loss: 2.16086\n",
      "Epoch: 00 [205504/244490 ( 84%)], Train Loss: 2.15995\n",
      "Epoch: 00 [206144/244490 ( 84%)], Train Loss: 2.15921\n",
      "Epoch: 00 [206784/244490 ( 85%)], Train Loss: 2.15860\n",
      "Epoch: 00 [207424/244490 ( 85%)], Train Loss: 2.15774\n",
      "Epoch: 00 [208064/244490 ( 85%)], Train Loss: 2.15731\n",
      "Epoch: 00 [208704/244490 ( 85%)], Train Loss: 2.15696\n",
      "Epoch: 00 [209344/244490 ( 86%)], Train Loss: 2.15684\n",
      "Epoch: 00 [209984/244490 ( 86%)], Train Loss: 2.15617\n",
      "Epoch: 00 [210624/244490 ( 86%)], Train Loss: 2.15550\n",
      "Epoch: 00 [211264/244490 ( 86%)], Train Loss: 2.15472\n",
      "Epoch: 00 [211904/244490 ( 87%)], Train Loss: 2.15414\n",
      "Epoch: 00 [212544/244490 ( 87%)], Train Loss: 2.15347\n",
      "Epoch: 00 [213184/244490 ( 87%)], Train Loss: 2.15298\n",
      "Epoch: 00 [213824/244490 ( 87%)], Train Loss: 2.15237\n",
      "Epoch: 00 [214464/244490 ( 88%)], Train Loss: 2.15208\n",
      "Epoch: 00 [215104/244490 ( 88%)], Train Loss: 2.15144\n",
      "Epoch: 00 [215744/244490 ( 88%)], Train Loss: 2.15057\n",
      "Epoch: 00 [216384/244490 ( 89%)], Train Loss: 2.14993\n",
      "Epoch: 00 [217024/244490 ( 89%)], Train Loss: 2.14937\n",
      "Epoch: 00 [217664/244490 ( 89%)], Train Loss: 2.14856\n",
      "Epoch: 00 [218304/244490 ( 89%)], Train Loss: 2.14766\n",
      "Epoch: 00 [218944/244490 ( 90%)], Train Loss: 2.14708\n",
      "Epoch: 00 [219584/244490 ( 90%)], Train Loss: 2.14647\n",
      "Epoch: 00 [220224/244490 ( 90%)], Train Loss: 2.14607\n",
      "Epoch: 00 [220864/244490 ( 90%)], Train Loss: 2.14557\n",
      "Epoch: 00 [221504/244490 ( 91%)], Train Loss: 2.14489\n",
      "Epoch: 00 [222144/244490 ( 91%)], Train Loss: 2.14421\n",
      "Epoch: 00 [222784/244490 ( 91%)], Train Loss: 2.14365\n",
      "Epoch: 00 [223424/244490 ( 91%)], Train Loss: 2.14305\n",
      "Epoch: 00 [224064/244490 ( 92%)], Train Loss: 2.14240\n",
      "Epoch: 00 [224704/244490 ( 92%)], Train Loss: 2.14156\n",
      "Epoch: 00 [225344/244490 ( 92%)], Train Loss: 2.14092\n",
      "Epoch: 00 [225984/244490 ( 92%)], Train Loss: 2.14023\n",
      "Epoch: 00 [226624/244490 ( 93%)], Train Loss: 2.13960\n",
      "Epoch: 00 [227264/244490 ( 93%)], Train Loss: 2.13916\n",
      "Epoch: 00 [227904/244490 ( 93%)], Train Loss: 2.13866\n",
      "Epoch: 00 [228544/244490 ( 93%)], Train Loss: 2.13807\n",
      "Epoch: 00 [229184/244490 ( 94%)], Train Loss: 2.13767\n",
      "Epoch: 00 [229824/244490 ( 94%)], Train Loss: 2.13702\n",
      "Epoch: 00 [230464/244490 ( 94%)], Train Loss: 2.13640\n",
      "Epoch: 00 [231104/244490 ( 95%)], Train Loss: 2.13553\n",
      "Epoch: 00 [231744/244490 ( 95%)], Train Loss: 2.13483\n",
      "Epoch: 00 [232384/244490 ( 95%)], Train Loss: 2.13432\n",
      "Epoch: 00 [233024/244490 ( 95%)], Train Loss: 2.13439\n",
      "Epoch: 00 [233664/244490 ( 96%)], Train Loss: 2.13409\n",
      "Epoch: 00 [234304/244490 ( 96%)], Train Loss: 2.13378\n",
      "Epoch: 00 [234944/244490 ( 96%)], Train Loss: 2.13328\n",
      "Epoch: 00 [235584/244490 ( 96%)], Train Loss: 2.13273\n",
      "Epoch: 00 [236224/244490 ( 97%)], Train Loss: 2.13238\n",
      "Epoch: 00 [236864/244490 ( 97%)], Train Loss: 2.13220\n",
      "Epoch: 00 [237504/244490 ( 97%)], Train Loss: 2.13164\n",
      "Epoch: 00 [238144/244490 ( 97%)], Train Loss: 2.13117\n",
      "Epoch: 00 [238784/244490 ( 98%)], Train Loss: 2.13094\n",
      "Epoch: 00 [239424/244490 ( 98%)], Train Loss: 2.13033\n",
      "Epoch: 00 [240064/244490 ( 98%)], Train Loss: 2.12986\n",
      "Epoch: 00 [240704/244490 ( 98%)], Train Loss: 2.12970\n",
      "Epoch: 00 [241344/244490 ( 99%)], Train Loss: 2.12911\n",
      "Epoch: 00 [241984/244490 ( 99%)], Train Loss: 2.12874\n",
      "Epoch: 00 [242624/244490 ( 99%)], Train Loss: 2.12810\n",
      "Epoch: 00 [243264/244490 ( 99%)], Train Loss: 2.12796\n",
      "Epoch: 00 [243904/244490 (100%)], Train Loss: 2.12746\n",
      "Epoch: 00 [244490/244490 (100%)], Train Loss: 2.12671\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 1.86223\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 1.86223\n",
      "Saving model checkpoint to output/checkpoint-fold-2.\n",
      "\n",
      "Epoch: 01 [    64/244490 (  0%)], Train Loss: 1.83994\n",
      "Epoch: 01 [   704/244490 (  0%)], Train Loss: 1.89321\n",
      "Epoch: 01 [  1344/244490 (  1%)], Train Loss: 1.85605\n",
      "Epoch: 01 [  1984/244490 (  1%)], Train Loss: 1.84410\n",
      "Epoch: 01 [  2624/244490 (  1%)], Train Loss: 1.87072\n",
      "Epoch: 01 [  3264/244490 (  1%)], Train Loss: 1.88272\n",
      "Epoch: 01 [  3904/244490 (  2%)], Train Loss: 1.88660\n",
      "Epoch: 01 [  4544/244490 (  2%)], Train Loss: 1.90302\n",
      "Epoch: 01 [  5184/244490 (  2%)], Train Loss: 1.90560\n",
      "Epoch: 01 [  5824/244490 (  2%)], Train Loss: 1.90944\n",
      "Epoch: 01 [  6464/244490 (  3%)], Train Loss: 1.90878\n",
      "Epoch: 01 [  7104/244490 (  3%)], Train Loss: 1.91398\n",
      "Epoch: 01 [  7744/244490 (  3%)], Train Loss: 1.93236\n",
      "Epoch: 01 [  8384/244490 (  3%)], Train Loss: 1.93670\n",
      "Epoch: 01 [  9024/244490 (  4%)], Train Loss: 1.93726\n",
      "Epoch: 01 [  9664/244490 (  4%)], Train Loss: 1.93699\n",
      "Epoch: 01 [ 10304/244490 (  4%)], Train Loss: 1.93278\n",
      "Epoch: 01 [ 10944/244490 (  4%)], Train Loss: 1.93661\n",
      "Epoch: 01 [ 11584/244490 (  5%)], Train Loss: 1.93421\n",
      "Epoch: 01 [ 12224/244490 (  5%)], Train Loss: 1.93070\n",
      "Epoch: 01 [ 12864/244490 (  5%)], Train Loss: 1.92972\n",
      "Epoch: 01 [ 13504/244490 (  6%)], Train Loss: 1.92623\n",
      "Epoch: 01 [ 14144/244490 (  6%)], Train Loss: 1.92483\n",
      "Epoch: 01 [ 14784/244490 (  6%)], Train Loss: 1.91810\n",
      "Epoch: 01 [ 15424/244490 (  6%)], Train Loss: 1.91243\n",
      "Epoch: 01 [ 16064/244490 (  7%)], Train Loss: 1.91284\n",
      "Epoch: 01 [ 16704/244490 (  7%)], Train Loss: 1.91629\n",
      "Epoch: 01 [ 17344/244490 (  7%)], Train Loss: 1.91916\n",
      "Epoch: 01 [ 17984/244490 (  7%)], Train Loss: 1.92074\n",
      "Epoch: 01 [ 18624/244490 (  8%)], Train Loss: 1.92464\n",
      "Epoch: 01 [ 19264/244490 (  8%)], Train Loss: 1.92497\n",
      "Epoch: 01 [ 19904/244490 (  8%)], Train Loss: 1.92607\n",
      "Epoch: 01 [ 20544/244490 (  8%)], Train Loss: 1.92854\n",
      "Epoch: 01 [ 21184/244490 (  9%)], Train Loss: 1.92841\n",
      "Epoch: 01 [ 21824/244490 (  9%)], Train Loss: 1.92486\n",
      "Epoch: 01 [ 22464/244490 (  9%)], Train Loss: 1.92630\n",
      "Epoch: 01 [ 23104/244490 (  9%)], Train Loss: 1.92587\n",
      "Epoch: 01 [ 23744/244490 ( 10%)], Train Loss: 1.92262\n",
      "Epoch: 01 [ 24384/244490 ( 10%)], Train Loss: 1.91886\n",
      "Epoch: 01 [ 25024/244490 ( 10%)], Train Loss: 1.91715\n",
      "Epoch: 01 [ 25664/244490 ( 10%)], Train Loss: 1.91650\n",
      "Epoch: 01 [ 26304/244490 ( 11%)], Train Loss: 1.91351\n",
      "Epoch: 01 [ 26944/244490 ( 11%)], Train Loss: 1.91716\n",
      "Epoch: 01 [ 27584/244490 ( 11%)], Train Loss: 1.91662\n",
      "Epoch: 01 [ 28224/244490 ( 12%)], Train Loss: 1.91573\n",
      "Epoch: 01 [ 28864/244490 ( 12%)], Train Loss: 1.91558\n",
      "Epoch: 01 [ 29504/244490 ( 12%)], Train Loss: 1.91686\n",
      "Epoch: 01 [ 30144/244490 ( 12%)], Train Loss: 1.92030\n",
      "Epoch: 01 [ 30784/244490 ( 13%)], Train Loss: 1.91815\n",
      "Epoch: 01 [ 31424/244490 ( 13%)], Train Loss: 1.92017\n",
      "Epoch: 01 [ 32064/244490 ( 13%)], Train Loss: 1.92134\n",
      "Epoch: 01 [ 32704/244490 ( 13%)], Train Loss: 1.92075\n",
      "Epoch: 01 [ 33344/244490 ( 14%)], Train Loss: 1.91907\n",
      "Epoch: 01 [ 33984/244490 ( 14%)], Train Loss: 1.91750\n",
      "Epoch: 01 [ 34624/244490 ( 14%)], Train Loss: 1.91574\n",
      "Epoch: 01 [ 35264/244490 ( 14%)], Train Loss: 1.91567\n",
      "Epoch: 01 [ 35904/244490 ( 15%)], Train Loss: 1.91364\n",
      "Epoch: 01 [ 36544/244490 ( 15%)], Train Loss: 1.91509\n",
      "Epoch: 01 [ 37184/244490 ( 15%)], Train Loss: 1.91431\n",
      "Epoch: 01 [ 37824/244490 ( 15%)], Train Loss: 1.91440\n",
      "Epoch: 01 [ 38464/244490 ( 16%)], Train Loss: 1.91227\n",
      "Epoch: 01 [ 39104/244490 ( 16%)], Train Loss: 1.91268\n",
      "Epoch: 01 [ 39744/244490 ( 16%)], Train Loss: 1.91265\n",
      "Epoch: 01 [ 40384/244490 ( 17%)], Train Loss: 1.91071\n",
      "Epoch: 01 [ 41024/244490 ( 17%)], Train Loss: 1.90944\n",
      "Epoch: 01 [ 41664/244490 ( 17%)], Train Loss: 1.91080\n",
      "Epoch: 01 [ 42304/244490 ( 17%)], Train Loss: 1.90987\n",
      "Epoch: 01 [ 42944/244490 ( 18%)], Train Loss: 1.90908\n",
      "Epoch: 01 [ 43584/244490 ( 18%)], Train Loss: 1.90810\n",
      "Epoch: 01 [ 44224/244490 ( 18%)], Train Loss: 1.90660\n",
      "Epoch: 01 [ 44864/244490 ( 18%)], Train Loss: 1.90600\n",
      "Epoch: 01 [ 45504/244490 ( 19%)], Train Loss: 1.90712\n",
      "Epoch: 01 [ 46144/244490 ( 19%)], Train Loss: 1.90815\n",
      "Epoch: 01 [ 46784/244490 ( 19%)], Train Loss: 1.90699\n",
      "Epoch: 01 [ 47424/244490 ( 19%)], Train Loss: 1.90636\n",
      "Epoch: 01 [ 48064/244490 ( 20%)], Train Loss: 1.90665\n",
      "Epoch: 01 [ 48704/244490 ( 20%)], Train Loss: 1.90742\n",
      "Epoch: 01 [ 49344/244490 ( 20%)], Train Loss: 1.90677\n",
      "Epoch: 01 [ 49984/244490 ( 20%)], Train Loss: 1.90558\n",
      "Epoch: 01 [ 50624/244490 ( 21%)], Train Loss: 1.90526\n",
      "Epoch: 01 [ 51264/244490 ( 21%)], Train Loss: 1.90358\n",
      "Epoch: 01 [ 51904/244490 ( 21%)], Train Loss: 1.90371\n",
      "Epoch: 01 [ 52544/244490 ( 21%)], Train Loss: 1.90299\n",
      "Epoch: 01 [ 53184/244490 ( 22%)], Train Loss: 1.90194\n",
      "Epoch: 01 [ 53824/244490 ( 22%)], Train Loss: 1.90050\n",
      "Epoch: 01 [ 54464/244490 ( 22%)], Train Loss: 1.89820\n",
      "Epoch: 01 [ 55104/244490 ( 23%)], Train Loss: 1.89744\n",
      "Epoch: 01 [ 55744/244490 ( 23%)], Train Loss: 1.89764\n",
      "Epoch: 01 [ 56384/244490 ( 23%)], Train Loss: 1.89568\n",
      "Epoch: 01 [ 57024/244490 ( 23%)], Train Loss: 1.89586\n",
      "Epoch: 01 [ 57664/244490 ( 24%)], Train Loss: 1.89554\n",
      "Epoch: 01 [ 58304/244490 ( 24%)], Train Loss: 1.89521\n",
      "Epoch: 01 [ 58944/244490 ( 24%)], Train Loss: 1.89349\n",
      "Epoch: 01 [ 59584/244490 ( 24%)], Train Loss: 1.89375\n",
      "Epoch: 01 [ 60224/244490 ( 25%)], Train Loss: 1.89369\n",
      "Epoch: 01 [ 60864/244490 ( 25%)], Train Loss: 1.89186\n",
      "Epoch: 01 [ 61504/244490 ( 25%)], Train Loss: 1.89116\n",
      "Epoch: 01 [ 62144/244490 ( 25%)], Train Loss: 1.89105\n",
      "Epoch: 01 [ 62784/244490 ( 26%)], Train Loss: 1.89036\n",
      "Epoch: 01 [ 63424/244490 ( 26%)], Train Loss: 1.88972\n",
      "Epoch: 01 [ 64064/244490 ( 26%)], Train Loss: 1.88765\n",
      "Epoch: 01 [ 64704/244490 ( 26%)], Train Loss: 1.88798\n",
      "Epoch: 01 [ 65344/244490 ( 27%)], Train Loss: 1.88835\n",
      "Epoch: 01 [ 65984/244490 ( 27%)], Train Loss: 1.88769\n",
      "Epoch: 01 [ 66624/244490 ( 27%)], Train Loss: 1.88741\n",
      "Epoch: 01 [ 67264/244490 ( 28%)], Train Loss: 1.88745\n",
      "Epoch: 01 [ 67904/244490 ( 28%)], Train Loss: 1.88595\n",
      "Epoch: 01 [ 68544/244490 ( 28%)], Train Loss: 1.88477\n",
      "Epoch: 01 [ 69184/244490 ( 28%)], Train Loss: 1.88507\n",
      "Epoch: 01 [ 69824/244490 ( 29%)], Train Loss: 1.88480\n",
      "Epoch: 01 [ 70464/244490 ( 29%)], Train Loss: 1.88520\n",
      "Epoch: 01 [ 71104/244490 ( 29%)], Train Loss: 1.88455\n",
      "Epoch: 01 [ 71744/244490 ( 29%)], Train Loss: 1.88382\n",
      "Epoch: 01 [ 72384/244490 ( 30%)], Train Loss: 1.88339\n",
      "Epoch: 01 [ 73024/244490 ( 30%)], Train Loss: 1.88256\n",
      "Epoch: 01 [ 73664/244490 ( 30%)], Train Loss: 1.88129\n",
      "Epoch: 01 [ 74304/244490 ( 30%)], Train Loss: 1.88154\n",
      "Epoch: 01 [ 74944/244490 ( 31%)], Train Loss: 1.88119\n",
      "Epoch: 01 [ 75584/244490 ( 31%)], Train Loss: 1.87990\n",
      "Epoch: 01 [ 76224/244490 ( 31%)], Train Loss: 1.87851\n",
      "Epoch: 01 [ 76864/244490 ( 31%)], Train Loss: 1.87885\n",
      "Epoch: 01 [ 77504/244490 ( 32%)], Train Loss: 1.87863\n",
      "Epoch: 01 [ 78144/244490 ( 32%)], Train Loss: 1.87876\n",
      "Epoch: 01 [ 78784/244490 ( 32%)], Train Loss: 1.87818\n",
      "Epoch: 01 [ 79424/244490 ( 32%)], Train Loss: 1.87667\n",
      "Epoch: 01 [ 80064/244490 ( 33%)], Train Loss: 1.87623\n",
      "Epoch: 01 [ 80704/244490 ( 33%)], Train Loss: 1.87615\n",
      "Epoch: 01 [ 81344/244490 ( 33%)], Train Loss: 1.87645\n",
      "Epoch: 01 [ 81984/244490 ( 34%)], Train Loss: 1.87683\n",
      "Epoch: 01 [ 82624/244490 ( 34%)], Train Loss: 1.87659\n",
      "Epoch: 01 [ 83264/244490 ( 34%)], Train Loss: 1.87653\n",
      "Epoch: 01 [ 83904/244490 ( 34%)], Train Loss: 1.87719\n",
      "Epoch: 01 [ 84544/244490 ( 35%)], Train Loss: 1.87610\n",
      "Epoch: 01 [ 85184/244490 ( 35%)], Train Loss: 1.87568\n",
      "Epoch: 01 [ 85824/244490 ( 35%)], Train Loss: 1.87466\n",
      "Epoch: 01 [ 86464/244490 ( 35%)], Train Loss: 1.87402\n",
      "Epoch: 01 [ 87104/244490 ( 36%)], Train Loss: 1.87363\n",
      "Epoch: 01 [ 87744/244490 ( 36%)], Train Loss: 1.87253\n",
      "Epoch: 01 [ 88384/244490 ( 36%)], Train Loss: 1.87191\n",
      "Epoch: 01 [ 89024/244490 ( 36%)], Train Loss: 1.87164\n",
      "Epoch: 01 [ 89664/244490 ( 37%)], Train Loss: 1.87100\n",
      "Epoch: 01 [ 90304/244490 ( 37%)], Train Loss: 1.87082\n",
      "Epoch: 01 [ 90944/244490 ( 37%)], Train Loss: 1.87106\n",
      "Epoch: 01 [ 91584/244490 ( 37%)], Train Loss: 1.87106\n",
      "Epoch: 01 [ 92224/244490 ( 38%)], Train Loss: 1.87068\n",
      "Epoch: 01 [ 92864/244490 ( 38%)], Train Loss: 1.86970\n",
      "Epoch: 01 [ 93504/244490 ( 38%)], Train Loss: 1.86904\n",
      "Epoch: 01 [ 94144/244490 ( 39%)], Train Loss: 1.86843\n",
      "Epoch: 01 [ 94784/244490 ( 39%)], Train Loss: 1.86808\n",
      "Epoch: 01 [ 95424/244490 ( 39%)], Train Loss: 1.86738\n",
      "Epoch: 01 [ 96064/244490 ( 39%)], Train Loss: 1.86602\n",
      "Epoch: 01 [ 96704/244490 ( 40%)], Train Loss: 1.86579\n",
      "Epoch: 01 [ 97344/244490 ( 40%)], Train Loss: 1.86512\n",
      "Epoch: 01 [ 97984/244490 ( 40%)], Train Loss: 1.86404\n",
      "Epoch: 01 [ 98624/244490 ( 40%)], Train Loss: 1.86316\n",
      "Epoch: 01 [ 99264/244490 ( 41%)], Train Loss: 1.86310\n",
      "Epoch: 01 [ 99904/244490 ( 41%)], Train Loss: 1.86259\n",
      "Epoch: 01 [100544/244490 ( 41%)], Train Loss: 1.86201\n",
      "Epoch: 01 [101184/244490 ( 41%)], Train Loss: 1.86196\n",
      "Epoch: 01 [101824/244490 ( 42%)], Train Loss: 1.86072\n",
      "Epoch: 01 [102464/244490 ( 42%)], Train Loss: 1.85982\n",
      "Epoch: 01 [103104/244490 ( 42%)], Train Loss: 1.85914\n",
      "Epoch: 01 [103744/244490 ( 42%)], Train Loss: 1.85900\n",
      "Epoch: 01 [104384/244490 ( 43%)], Train Loss: 1.85841\n",
      "Epoch: 01 [105024/244490 ( 43%)], Train Loss: 1.85745\n",
      "Epoch: 01 [105664/244490 ( 43%)], Train Loss: 1.85696\n",
      "Epoch: 01 [106304/244490 ( 43%)], Train Loss: 1.85643\n",
      "Epoch: 01 [106944/244490 ( 44%)], Train Loss: 1.85566\n",
      "Epoch: 01 [107584/244490 ( 44%)], Train Loss: 1.85511\n",
      "Epoch: 01 [108224/244490 ( 44%)], Train Loss: 1.85438\n",
      "Epoch: 01 [108864/244490 ( 45%)], Train Loss: 1.85371\n",
      "Epoch: 01 [109504/244490 ( 45%)], Train Loss: 1.85339\n",
      "Epoch: 01 [110144/244490 ( 45%)], Train Loss: 1.85263\n",
      "Epoch: 01 [110784/244490 ( 45%)], Train Loss: 1.85168\n",
      "Epoch: 01 [111424/244490 ( 46%)], Train Loss: 1.85123\n",
      "Epoch: 01 [112064/244490 ( 46%)], Train Loss: 1.85069\n",
      "Epoch: 01 [112704/244490 ( 46%)], Train Loss: 1.85065\n",
      "Epoch: 01 [113344/244490 ( 46%)], Train Loss: 1.85027\n",
      "Epoch: 01 [113984/244490 ( 47%)], Train Loss: 1.84954\n",
      "Epoch: 01 [114624/244490 ( 47%)], Train Loss: 1.84795\n",
      "Epoch: 01 [115264/244490 ( 47%)], Train Loss: 1.84737\n",
      "Epoch: 01 [115904/244490 ( 47%)], Train Loss: 1.84604\n",
      "Epoch: 01 [116544/244490 ( 48%)], Train Loss: 1.84526\n",
      "Epoch: 01 [117184/244490 ( 48%)], Train Loss: 1.84446\n",
      "Epoch: 01 [117824/244490 ( 48%)], Train Loss: 1.84350\n",
      "Epoch: 01 [118464/244490 ( 48%)], Train Loss: 1.84315\n",
      "Epoch: 01 [119104/244490 ( 49%)], Train Loss: 1.84301\n",
      "Epoch: 01 [119744/244490 ( 49%)], Train Loss: 1.84199\n",
      "Epoch: 01 [120384/244490 ( 49%)], Train Loss: 1.84193\n",
      "Epoch: 01 [121024/244490 ( 50%)], Train Loss: 1.84220\n",
      "Epoch: 01 [121664/244490 ( 50%)], Train Loss: 1.84125\n",
      "Epoch: 01 [122304/244490 ( 50%)], Train Loss: 1.84079\n",
      "Epoch: 01 [122944/244490 ( 50%)], Train Loss: 1.84050\n",
      "Epoch: 01 [123584/244490 ( 51%)], Train Loss: 1.84007\n",
      "Epoch: 01 [124224/244490 ( 51%)], Train Loss: 1.84035\n",
      "Epoch: 01 [124864/244490 ( 51%)], Train Loss: 1.84059\n",
      "Epoch: 01 [125504/244490 ( 51%)], Train Loss: 1.83942\n",
      "Epoch: 01 [126144/244490 ( 52%)], Train Loss: 1.83863\n",
      "Epoch: 01 [126784/244490 ( 52%)], Train Loss: 1.83857\n",
      "Epoch: 01 [127424/244490 ( 52%)], Train Loss: 1.83774\n",
      "Epoch: 01 [128064/244490 ( 52%)], Train Loss: 1.83744\n",
      "Epoch: 01 [128704/244490 ( 53%)], Train Loss: 1.83722\n",
      "Epoch: 01 [129344/244490 ( 53%)], Train Loss: 1.83733\n",
      "Epoch: 01 [129984/244490 ( 53%)], Train Loss: 1.83725\n",
      "Epoch: 01 [130624/244490 ( 53%)], Train Loss: 1.83685\n",
      "Epoch: 01 [131264/244490 ( 54%)], Train Loss: 1.83638\n",
      "Epoch: 01 [131904/244490 ( 54%)], Train Loss: 1.83673\n",
      "Epoch: 01 [132544/244490 ( 54%)], Train Loss: 1.83616\n",
      "Epoch: 01 [133184/244490 ( 54%)], Train Loss: 1.83516\n",
      "Epoch: 01 [133824/244490 ( 55%)], Train Loss: 1.83485\n",
      "Epoch: 01 [134464/244490 ( 55%)], Train Loss: 1.83390\n",
      "Epoch: 01 [135104/244490 ( 55%)], Train Loss: 1.83319\n",
      "Epoch: 01 [135744/244490 ( 56%)], Train Loss: 1.83250\n",
      "Epoch: 01 [136384/244490 ( 56%)], Train Loss: 1.83212\n",
      "Epoch: 01 [137024/244490 ( 56%)], Train Loss: 1.83246\n",
      "Epoch: 01 [137664/244490 ( 56%)], Train Loss: 1.83193\n",
      "Epoch: 01 [138304/244490 ( 57%)], Train Loss: 1.83109\n",
      "Epoch: 01 [138944/244490 ( 57%)], Train Loss: 1.83076\n",
      "Epoch: 01 [139584/244490 ( 57%)], Train Loss: 1.83071\n",
      "Epoch: 01 [140224/244490 ( 57%)], Train Loss: 1.82971\n",
      "Epoch: 01 [140864/244490 ( 58%)], Train Loss: 1.82885\n",
      "Epoch: 01 [141504/244490 ( 58%)], Train Loss: 1.82867\n",
      "Epoch: 01 [142144/244490 ( 58%)], Train Loss: 1.82777\n",
      "Epoch: 01 [142784/244490 ( 58%)], Train Loss: 1.82693\n",
      "Epoch: 01 [143424/244490 ( 59%)], Train Loss: 1.82672\n",
      "Epoch: 01 [144064/244490 ( 59%)], Train Loss: 1.82594\n",
      "Epoch: 01 [144704/244490 ( 59%)], Train Loss: 1.82598\n",
      "Epoch: 01 [145344/244490 ( 59%)], Train Loss: 1.82534\n",
      "Epoch: 01 [145984/244490 ( 60%)], Train Loss: 1.82426\n",
      "Epoch: 01 [146624/244490 ( 60%)], Train Loss: 1.82328\n",
      "Epoch: 01 [147264/244490 ( 60%)], Train Loss: 1.82270\n",
      "Epoch: 01 [147904/244490 ( 60%)], Train Loss: 1.82197\n",
      "Epoch: 01 [148544/244490 ( 61%)], Train Loss: 1.82178\n",
      "Epoch: 01 [149184/244490 ( 61%)], Train Loss: 1.82136\n",
      "Epoch: 01 [149824/244490 ( 61%)], Train Loss: 1.82100\n",
      "Epoch: 01 [150464/244490 ( 62%)], Train Loss: 1.82104\n",
      "Epoch: 01 [151104/244490 ( 62%)], Train Loss: 1.82066\n",
      "Epoch: 01 [151744/244490 ( 62%)], Train Loss: 1.82036\n",
      "Epoch: 01 [152384/244490 ( 62%)], Train Loss: 1.82021\n",
      "Epoch: 01 [153024/244490 ( 63%)], Train Loss: 1.82001\n",
      "Epoch: 01 [153664/244490 ( 63%)], Train Loss: 1.81942\n",
      "Epoch: 01 [154304/244490 ( 63%)], Train Loss: 1.81903\n",
      "Epoch: 01 [154944/244490 ( 63%)], Train Loss: 1.81859\n",
      "Epoch: 01 [155584/244490 ( 64%)], Train Loss: 1.81836\n",
      "Epoch: 01 [156224/244490 ( 64%)], Train Loss: 1.81806\n",
      "Epoch: 01 [156864/244490 ( 64%)], Train Loss: 1.81754\n",
      "Epoch: 01 [157504/244490 ( 64%)], Train Loss: 1.81688\n",
      "Epoch: 01 [158144/244490 ( 65%)], Train Loss: 1.81697\n",
      "Epoch: 01 [158784/244490 ( 65%)], Train Loss: 1.81620\n",
      "Epoch: 01 [159424/244490 ( 65%)], Train Loss: 1.81601\n",
      "Epoch: 01 [160064/244490 ( 65%)], Train Loss: 1.81553\n",
      "Epoch: 01 [160704/244490 ( 66%)], Train Loss: 1.81493\n",
      "Epoch: 01 [161344/244490 ( 66%)], Train Loss: 1.81453\n",
      "Epoch: 01 [161984/244490 ( 66%)], Train Loss: 1.81424\n",
      "Epoch: 01 [162624/244490 ( 67%)], Train Loss: 1.81397\n",
      "Epoch: 01 [163264/244490 ( 67%)], Train Loss: 1.81342\n",
      "Epoch: 01 [163904/244490 ( 67%)], Train Loss: 1.81314\n",
      "Epoch: 01 [164544/244490 ( 67%)], Train Loss: 1.81278\n",
      "Epoch: 01 [165184/244490 ( 68%)], Train Loss: 1.81287\n",
      "Epoch: 01 [165824/244490 ( 68%)], Train Loss: 1.81260\n",
      "Epoch: 01 [166464/244490 ( 68%)], Train Loss: 1.81187\n",
      "Epoch: 01 [167104/244490 ( 68%)], Train Loss: 1.81094\n",
      "Epoch: 01 [167744/244490 ( 69%)], Train Loss: 1.81034\n",
      "Epoch: 01 [168384/244490 ( 69%)], Train Loss: 1.81007\n",
      "Epoch: 01 [169024/244490 ( 69%)], Train Loss: 1.81019\n",
      "Epoch: 01 [169664/244490 ( 69%)], Train Loss: 1.80981\n",
      "Epoch: 01 [170304/244490 ( 70%)], Train Loss: 1.80939\n",
      "Epoch: 01 [170944/244490 ( 70%)], Train Loss: 1.80917\n",
      "Epoch: 01 [171584/244490 ( 70%)], Train Loss: 1.80924\n",
      "Epoch: 01 [172224/244490 ( 70%)], Train Loss: 1.80903\n",
      "Epoch: 01 [172864/244490 ( 71%)], Train Loss: 1.80907\n",
      "Epoch: 01 [173504/244490 ( 71%)], Train Loss: 1.80840\n",
      "Epoch: 01 [174144/244490 ( 71%)], Train Loss: 1.80789\n",
      "Epoch: 01 [174784/244490 ( 71%)], Train Loss: 1.80746\n",
      "Epoch: 01 [175424/244490 ( 72%)], Train Loss: 1.80708\n",
      "Epoch: 01 [176064/244490 ( 72%)], Train Loss: 1.80677\n",
      "Epoch: 01 [176704/244490 ( 72%)], Train Loss: 1.80665\n",
      "Epoch: 01 [177344/244490 ( 73%)], Train Loss: 1.80588\n",
      "Epoch: 01 [177984/244490 ( 73%)], Train Loss: 1.80472\n",
      "Epoch: 01 [178624/244490 ( 73%)], Train Loss: 1.80457\n",
      "Epoch: 01 [179264/244490 ( 73%)], Train Loss: 1.80435\n",
      "Epoch: 01 [179904/244490 ( 74%)], Train Loss: 1.80413\n",
      "Epoch: 01 [180544/244490 ( 74%)], Train Loss: 1.80356\n",
      "Epoch: 01 [181184/244490 ( 74%)], Train Loss: 1.80326\n",
      "Epoch: 01 [181824/244490 ( 74%)], Train Loss: 1.80283\n",
      "Epoch: 01 [182464/244490 ( 75%)], Train Loss: 1.80238\n",
      "Epoch: 01 [183104/244490 ( 75%)], Train Loss: 1.80174\n",
      "Epoch: 01 [183744/244490 ( 75%)], Train Loss: 1.80100\n",
      "Epoch: 01 [184384/244490 ( 75%)], Train Loss: 1.80075\n",
      "Epoch: 01 [185024/244490 ( 76%)], Train Loss: 1.80055\n",
      "Epoch: 01 [185664/244490 ( 76%)], Train Loss: 1.79960\n",
      "Epoch: 01 [186304/244490 ( 76%)], Train Loss: 1.79880\n",
      "Epoch: 01 [186944/244490 ( 76%)], Train Loss: 1.79860\n",
      "Epoch: 01 [187584/244490 ( 77%)], Train Loss: 1.79835\n",
      "Epoch: 01 [188224/244490 ( 77%)], Train Loss: 1.79767\n",
      "Epoch: 01 [188864/244490 ( 77%)], Train Loss: 1.79717\n",
      "Epoch: 01 [189504/244490 ( 78%)], Train Loss: 1.79659\n",
      "Epoch: 01 [190144/244490 ( 78%)], Train Loss: 1.79631\n",
      "Epoch: 01 [190784/244490 ( 78%)], Train Loss: 1.79590\n",
      "Epoch: 01 [191424/244490 ( 78%)], Train Loss: 1.79544\n",
      "Epoch: 01 [192064/244490 ( 79%)], Train Loss: 1.79530\n",
      "Epoch: 01 [192704/244490 ( 79%)], Train Loss: 1.79501\n",
      "Epoch: 01 [193344/244490 ( 79%)], Train Loss: 1.79448\n",
      "Epoch: 01 [193984/244490 ( 79%)], Train Loss: 1.79384\n",
      "Epoch: 01 [194624/244490 ( 80%)], Train Loss: 1.79341\n",
      "Epoch: 01 [195264/244490 ( 80%)], Train Loss: 1.79264\n",
      "Epoch: 01 [195904/244490 ( 80%)], Train Loss: 1.79221\n",
      "Epoch: 01 [196544/244490 ( 80%)], Train Loss: 1.79166\n",
      "Epoch: 01 [197184/244490 ( 81%)], Train Loss: 1.79160\n",
      "Epoch: 01 [197824/244490 ( 81%)], Train Loss: 1.79097\n",
      "Epoch: 01 [198464/244490 ( 81%)], Train Loss: 1.79041\n",
      "Epoch: 01 [199104/244490 ( 81%)], Train Loss: 1.78998\n",
      "Epoch: 01 [199744/244490 ( 82%)], Train Loss: 1.78930\n",
      "Epoch: 01 [200384/244490 ( 82%)], Train Loss: 1.78894\n",
      "Epoch: 01 [201024/244490 ( 82%)], Train Loss: 1.78821\n",
      "Epoch: 01 [201664/244490 ( 82%)], Train Loss: 1.78743\n",
      "Epoch: 01 [202304/244490 ( 83%)], Train Loss: 1.78691\n",
      "Epoch: 01 [202944/244490 ( 83%)], Train Loss: 1.78645\n",
      "Epoch: 01 [203584/244490 ( 83%)], Train Loss: 1.78626\n",
      "Epoch: 01 [204224/244490 ( 84%)], Train Loss: 1.78532\n",
      "Epoch: 01 [204864/244490 ( 84%)], Train Loss: 1.78457\n",
      "Epoch: 01 [205504/244490 ( 84%)], Train Loss: 1.78378\n",
      "Epoch: 01 [206144/244490 ( 84%)], Train Loss: 1.78317\n",
      "Epoch: 01 [206784/244490 ( 85%)], Train Loss: 1.78251\n",
      "Epoch: 01 [207424/244490 ( 85%)], Train Loss: 1.78207\n",
      "Epoch: 01 [208064/244490 ( 85%)], Train Loss: 1.78184\n",
      "Epoch: 01 [208704/244490 ( 85%)], Train Loss: 1.78171\n",
      "Epoch: 01 [209344/244490 ( 86%)], Train Loss: 1.78166\n",
      "Epoch: 01 [209984/244490 ( 86%)], Train Loss: 1.78112\n",
      "Epoch: 01 [210624/244490 ( 86%)], Train Loss: 1.78056\n",
      "Epoch: 01 [211264/244490 ( 86%)], Train Loss: 1.78001\n",
      "Epoch: 01 [211904/244490 ( 87%)], Train Loss: 1.77947\n",
      "Epoch: 01 [212544/244490 ( 87%)], Train Loss: 1.77903\n",
      "Epoch: 01 [213184/244490 ( 87%)], Train Loss: 1.77869\n",
      "Epoch: 01 [213824/244490 ( 87%)], Train Loss: 1.77810\n",
      "Epoch: 01 [214464/244490 ( 88%)], Train Loss: 1.77794\n",
      "Epoch: 01 [215104/244490 ( 88%)], Train Loss: 1.77727\n",
      "Epoch: 01 [215744/244490 ( 88%)], Train Loss: 1.77659\n",
      "Epoch: 01 [216384/244490 ( 89%)], Train Loss: 1.77602\n",
      "Epoch: 01 [217024/244490 ( 89%)], Train Loss: 1.77568\n",
      "Epoch: 01 [217664/244490 ( 89%)], Train Loss: 1.77489\n",
      "Epoch: 01 [218304/244490 ( 89%)], Train Loss: 1.77425\n",
      "Epoch: 01 [218944/244490 ( 90%)], Train Loss: 1.77377\n",
      "Epoch: 01 [219584/244490 ( 90%)], Train Loss: 1.77333\n",
      "Epoch: 01 [220224/244490 ( 90%)], Train Loss: 1.77302\n",
      "Epoch: 01 [220864/244490 ( 90%)], Train Loss: 1.77280\n",
      "Epoch: 01 [221504/244490 ( 91%)], Train Loss: 1.77227\n",
      "Epoch: 01 [222144/244490 ( 91%)], Train Loss: 1.77152\n",
      "Epoch: 01 [222784/244490 ( 91%)], Train Loss: 1.77117\n",
      "Epoch: 01 [223424/244490 ( 91%)], Train Loss: 1.77070\n",
      "Epoch: 01 [224064/244490 ( 92%)], Train Loss: 1.77024\n",
      "Epoch: 01 [224704/244490 ( 92%)], Train Loss: 1.76964\n",
      "Epoch: 01 [225344/244490 ( 92%)], Train Loss: 1.76909\n",
      "Epoch: 01 [225984/244490 ( 92%)], Train Loss: 1.76852\n",
      "Epoch: 01 [226624/244490 ( 93%)], Train Loss: 1.76808\n",
      "Epoch: 01 [227264/244490 ( 93%)], Train Loss: 1.76770\n",
      "Epoch: 01 [227904/244490 ( 93%)], Train Loss: 1.76728\n",
      "Epoch: 01 [228544/244490 ( 93%)], Train Loss: 1.76677\n",
      "Epoch: 01 [229184/244490 ( 94%)], Train Loss: 1.76658\n",
      "Epoch: 01 [229824/244490 ( 94%)], Train Loss: 1.76595\n",
      "Epoch: 01 [230464/244490 ( 94%)], Train Loss: 1.76560\n",
      "Epoch: 01 [231104/244490 ( 95%)], Train Loss: 1.76488\n",
      "Epoch: 01 [231744/244490 ( 95%)], Train Loss: 1.76431\n",
      "Epoch: 01 [232384/244490 ( 95%)], Train Loss: 1.76392\n",
      "Epoch: 01 [233024/244490 ( 95%)], Train Loss: 1.76396\n",
      "Epoch: 01 [233664/244490 ( 96%)], Train Loss: 1.76379\n",
      "Epoch: 01 [234304/244490 ( 96%)], Train Loss: 1.76364\n",
      "Epoch: 01 [234944/244490 ( 96%)], Train Loss: 1.76332\n",
      "Epoch: 01 [235584/244490 ( 96%)], Train Loss: 1.76280\n",
      "Epoch: 01 [236224/244490 ( 97%)], Train Loss: 1.76260\n",
      "Epoch: 01 [236864/244490 ( 97%)], Train Loss: 1.76268\n",
      "Epoch: 01 [237504/244490 ( 97%)], Train Loss: 1.76217\n",
      "Epoch: 01 [238144/244490 ( 97%)], Train Loss: 1.76183\n",
      "Epoch: 01 [238784/244490 ( 98%)], Train Loss: 1.76177\n",
      "Epoch: 01 [239424/244490 ( 98%)], Train Loss: 1.76129\n",
      "Epoch: 01 [240064/244490 ( 98%)], Train Loss: 1.76082\n",
      "Epoch: 01 [240704/244490 ( 98%)], Train Loss: 1.76069\n",
      "Epoch: 01 [241344/244490 ( 99%)], Train Loss: 1.76027\n",
      "Epoch: 01 [241984/244490 ( 99%)], Train Loss: 1.75994\n",
      "Epoch: 01 [242624/244490 ( 99%)], Train Loss: 1.75929\n",
      "Epoch: 01 [243264/244490 ( 99%)], Train Loss: 1.75920\n",
      "Epoch: 01 [243904/244490 (100%)], Train Loss: 1.75881\n",
      "Epoch: 01 [244490/244490 (100%)], Train Loss: 1.75807\n",
      "----Validation Results Summary----\n",
      "Epoch: [1] Valid Loss: 1.85267\n",
      "1 Epoch, Best epoch was updated! Valid Loss: 1.85267\n",
      "Saving model checkpoint to output/checkpoint-fold-2.\n",
      "\n",
      "Epoch: 02 [    64/244490 (  0%)], Train Loss: 1.37720\n",
      "Epoch: 02 [   704/244490 (  0%)], Train Loss: 1.57230\n",
      "Epoch: 02 [  1344/244490 (  1%)], Train Loss: 1.52253\n",
      "Epoch: 02 [  1984/244490 (  1%)], Train Loss: 1.51089\n",
      "Epoch: 02 [  2624/244490 (  1%)], Train Loss: 1.53464\n",
      "Epoch: 02 [  3264/244490 (  1%)], Train Loss: 1.55451\n",
      "Epoch: 02 [  3904/244490 (  2%)], Train Loss: 1.55600\n",
      "Epoch: 02 [  4544/244490 (  2%)], Train Loss: 1.57999\n",
      "Epoch: 02 [  5184/244490 (  2%)], Train Loss: 1.57177\n",
      "Epoch: 02 [  5824/244490 (  2%)], Train Loss: 1.58408\n",
      "Epoch: 02 [  6464/244490 (  3%)], Train Loss: 1.58514\n",
      "Epoch: 02 [  7104/244490 (  3%)], Train Loss: 1.58900\n",
      "Epoch: 02 [  7744/244490 (  3%)], Train Loss: 1.60728\n",
      "Epoch: 02 [  8384/244490 (  3%)], Train Loss: 1.61226\n",
      "Epoch: 02 [  9024/244490 (  4%)], Train Loss: 1.61468\n",
      "Epoch: 02 [  9664/244490 (  4%)], Train Loss: 1.61501\n",
      "Epoch: 02 [ 10304/244490 (  4%)], Train Loss: 1.61053\n",
      "Epoch: 02 [ 10944/244490 (  4%)], Train Loss: 1.61641\n",
      "Epoch: 02 [ 11584/244490 (  5%)], Train Loss: 1.61355\n",
      "Epoch: 02 [ 12224/244490 (  5%)], Train Loss: 1.60825\n",
      "Epoch: 02 [ 12864/244490 (  5%)], Train Loss: 1.60953\n",
      "Epoch: 02 [ 13504/244490 (  6%)], Train Loss: 1.60613\n",
      "Epoch: 02 [ 14144/244490 (  6%)], Train Loss: 1.60438\n",
      "Epoch: 02 [ 14784/244490 (  6%)], Train Loss: 1.59828\n",
      "Epoch: 02 [ 15424/244490 (  6%)], Train Loss: 1.59336\n",
      "Epoch: 02 [ 16064/244490 (  7%)], Train Loss: 1.59393\n",
      "Epoch: 02 [ 16704/244490 (  7%)], Train Loss: 1.59786\n",
      "Epoch: 02 [ 17344/244490 (  7%)], Train Loss: 1.60036\n",
      "Epoch: 02 [ 17984/244490 (  7%)], Train Loss: 1.60317\n",
      "Epoch: 02 [ 18624/244490 (  8%)], Train Loss: 1.60775\n",
      "Epoch: 02 [ 19264/244490 (  8%)], Train Loss: 1.60450\n",
      "Epoch: 02 [ 19904/244490 (  8%)], Train Loss: 1.60316\n",
      "Epoch: 02 [ 20544/244490 (  8%)], Train Loss: 1.60596\n",
      "Epoch: 02 [ 21184/244490 (  9%)], Train Loss: 1.60577\n",
      "Epoch: 02 [ 21824/244490 (  9%)], Train Loss: 1.60459\n",
      "Epoch: 02 [ 22464/244490 (  9%)], Train Loss: 1.60731\n",
      "Epoch: 02 [ 23104/244490 (  9%)], Train Loss: 1.60751\n",
      "Epoch: 02 [ 23744/244490 ( 10%)], Train Loss: 1.60445\n",
      "Epoch: 02 [ 24384/244490 ( 10%)], Train Loss: 1.60027\n",
      "Epoch: 02 [ 25024/244490 ( 10%)], Train Loss: 1.59905\n",
      "Epoch: 02 [ 25664/244490 ( 10%)], Train Loss: 1.59747\n",
      "Epoch: 02 [ 26304/244490 ( 11%)], Train Loss: 1.59615\n",
      "Epoch: 02 [ 26944/244490 ( 11%)], Train Loss: 1.59887\n",
      "Epoch: 02 [ 27584/244490 ( 11%)], Train Loss: 1.59750\n",
      "Epoch: 02 [ 28224/244490 ( 12%)], Train Loss: 1.59745\n",
      "Epoch: 02 [ 28864/244490 ( 12%)], Train Loss: 1.59653\n",
      "Epoch: 02 [ 29504/244490 ( 12%)], Train Loss: 1.59897\n",
      "Epoch: 02 [ 30144/244490 ( 12%)], Train Loss: 1.60272\n",
      "Epoch: 02 [ 30784/244490 ( 13%)], Train Loss: 1.60251\n",
      "Epoch: 02 [ 31424/244490 ( 13%)], Train Loss: 1.60474\n",
      "Epoch: 02 [ 32064/244490 ( 13%)], Train Loss: 1.60596\n",
      "Epoch: 02 [ 32704/244490 ( 13%)], Train Loss: 1.60451\n",
      "Epoch: 02 [ 33344/244490 ( 14%)], Train Loss: 1.60243\n",
      "Epoch: 02 [ 33984/244490 ( 14%)], Train Loss: 1.60083\n",
      "Epoch: 02 [ 34624/244490 ( 14%)], Train Loss: 1.59882\n",
      "Epoch: 02 [ 35264/244490 ( 14%)], Train Loss: 1.59871\n",
      "Epoch: 02 [ 35904/244490 ( 15%)], Train Loss: 1.59718\n",
      "Epoch: 02 [ 36544/244490 ( 15%)], Train Loss: 1.59776\n",
      "Epoch: 02 [ 37184/244490 ( 15%)], Train Loss: 1.59680\n",
      "Epoch: 02 [ 37824/244490 ( 15%)], Train Loss: 1.59593\n",
      "Epoch: 02 [ 38464/244490 ( 16%)], Train Loss: 1.59421\n",
      "Epoch: 02 [ 39104/244490 ( 16%)], Train Loss: 1.59466\n",
      "Epoch: 02 [ 39744/244490 ( 16%)], Train Loss: 1.59354\n",
      "Epoch: 02 [ 40384/244490 ( 17%)], Train Loss: 1.59091\n",
      "Epoch: 02 [ 41024/244490 ( 17%)], Train Loss: 1.58927\n",
      "Epoch: 02 [ 41664/244490 ( 17%)], Train Loss: 1.59058\n",
      "Epoch: 02 [ 42304/244490 ( 17%)], Train Loss: 1.58941\n",
      "Epoch: 02 [ 42944/244490 ( 18%)], Train Loss: 1.58903\n",
      "Epoch: 02 [ 43584/244490 ( 18%)], Train Loss: 1.58762\n",
      "Epoch: 02 [ 44224/244490 ( 18%)], Train Loss: 1.58698\n",
      "Epoch: 02 [ 44864/244490 ( 18%)], Train Loss: 1.58574\n",
      "Epoch: 02 [ 45504/244490 ( 19%)], Train Loss: 1.58643\n",
      "Epoch: 02 [ 46144/244490 ( 19%)], Train Loss: 1.58758\n",
      "Epoch: 02 [ 46784/244490 ( 19%)], Train Loss: 1.58710\n",
      "Epoch: 02 [ 47424/244490 ( 19%)], Train Loss: 1.58684\n",
      "Epoch: 02 [ 48064/244490 ( 20%)], Train Loss: 1.58625\n",
      "Epoch: 02 [ 48704/244490 ( 20%)], Train Loss: 1.58747\n",
      "Epoch: 02 [ 49344/244490 ( 20%)], Train Loss: 1.58633\n",
      "Epoch: 02 [ 49984/244490 ( 20%)], Train Loss: 1.58574\n",
      "Epoch: 02 [ 50624/244490 ( 21%)], Train Loss: 1.58636\n",
      "Epoch: 02 [ 51264/244490 ( 21%)], Train Loss: 1.58511\n",
      "Epoch: 02 [ 51904/244490 ( 21%)], Train Loss: 1.58465\n",
      "Epoch: 02 [ 52544/244490 ( 21%)], Train Loss: 1.58492\n",
      "Epoch: 02 [ 53184/244490 ( 22%)], Train Loss: 1.58456\n",
      "Epoch: 02 [ 53824/244490 ( 22%)], Train Loss: 1.58293\n",
      "Epoch: 02 [ 54464/244490 ( 22%)], Train Loss: 1.58071\n",
      "Epoch: 02 [ 55104/244490 ( 23%)], Train Loss: 1.57967\n",
      "Epoch: 02 [ 55744/244490 ( 23%)], Train Loss: 1.57949\n",
      "Epoch: 02 [ 56384/244490 ( 23%)], Train Loss: 1.57798\n",
      "Epoch: 02 [ 57024/244490 ( 23%)], Train Loss: 1.57769\n",
      "Epoch: 02 [ 57664/244490 ( 24%)], Train Loss: 1.57746\n",
      "Epoch: 02 [ 58304/244490 ( 24%)], Train Loss: 1.57709\n",
      "Epoch: 02 [ 58944/244490 ( 24%)], Train Loss: 1.57610\n",
      "Epoch: 02 [ 59584/244490 ( 24%)], Train Loss: 1.57599\n",
      "Epoch: 02 [ 60224/244490 ( 25%)], Train Loss: 1.57533\n",
      "Epoch: 02 [ 60864/244490 ( 25%)], Train Loss: 1.57304\n",
      "Epoch: 02 [ 61504/244490 ( 25%)], Train Loss: 1.57267\n",
      "Epoch: 02 [ 62144/244490 ( 25%)], Train Loss: 1.57244\n",
      "Epoch: 02 [ 62784/244490 ( 26%)], Train Loss: 1.57131\n",
      "Epoch: 02 [ 63424/244490 ( 26%)], Train Loss: 1.57078\n",
      "Epoch: 02 [ 64064/244490 ( 26%)], Train Loss: 1.56891\n",
      "Epoch: 02 [ 64704/244490 ( 26%)], Train Loss: 1.56852\n",
      "Epoch: 02 [ 65344/244490 ( 27%)], Train Loss: 1.56836\n",
      "Epoch: 02 [ 65984/244490 ( 27%)], Train Loss: 1.56760\n",
      "Epoch: 02 [ 66624/244490 ( 27%)], Train Loss: 1.56702\n",
      "Epoch: 02 [ 67264/244490 ( 28%)], Train Loss: 1.56709\n",
      "Epoch: 02 [ 67904/244490 ( 28%)], Train Loss: 1.56589\n",
      "Epoch: 02 [ 68544/244490 ( 28%)], Train Loss: 1.56528\n",
      "Epoch: 02 [ 69184/244490 ( 28%)], Train Loss: 1.56496\n",
      "Epoch: 02 [ 69824/244490 ( 29%)], Train Loss: 1.56527\n",
      "Epoch: 02 [ 70464/244490 ( 29%)], Train Loss: 1.56613\n",
      "Epoch: 02 [ 71104/244490 ( 29%)], Train Loss: 1.56534\n",
      "Epoch: 02 [ 71744/244490 ( 29%)], Train Loss: 1.56406\n",
      "Epoch: 02 [ 72384/244490 ( 30%)], Train Loss: 1.56369\n",
      "Epoch: 02 [ 73024/244490 ( 30%)], Train Loss: 1.56214\n",
      "Epoch: 02 [ 73664/244490 ( 30%)], Train Loss: 1.56082\n",
      "Epoch: 02 [ 74304/244490 ( 30%)], Train Loss: 1.56096\n",
      "Epoch: 02 [ 74944/244490 ( 31%)], Train Loss: 1.56046\n",
      "Epoch: 02 [ 75584/244490 ( 31%)], Train Loss: 1.55884\n",
      "Epoch: 02 [ 76224/244490 ( 31%)], Train Loss: 1.55702\n",
      "Epoch: 02 [ 76864/244490 ( 31%)], Train Loss: 1.55744\n",
      "Epoch: 02 [ 77504/244490 ( 32%)], Train Loss: 1.55697\n",
      "Epoch: 02 [ 78144/244490 ( 32%)], Train Loss: 1.55659\n",
      "Epoch: 02 [ 78784/244490 ( 32%)], Train Loss: 1.55603\n",
      "Epoch: 02 [ 79424/244490 ( 32%)], Train Loss: 1.55461\n",
      "Epoch: 02 [ 80064/244490 ( 33%)], Train Loss: 1.55394\n",
      "Epoch: 02 [ 80704/244490 ( 33%)], Train Loss: 1.55328\n",
      "Epoch: 02 [ 81344/244490 ( 33%)], Train Loss: 1.55355\n",
      "Epoch: 02 [ 81984/244490 ( 34%)], Train Loss: 1.55406\n",
      "Epoch: 02 [ 82624/244490 ( 34%)], Train Loss: 1.55379\n",
      "Epoch: 02 [ 83264/244490 ( 34%)], Train Loss: 1.55341\n",
      "Epoch: 02 [ 83904/244490 ( 34%)], Train Loss: 1.55373\n",
      "Epoch: 02 [ 84544/244490 ( 35%)], Train Loss: 1.55247\n",
      "Epoch: 02 [ 85184/244490 ( 35%)], Train Loss: 1.55141\n",
      "Epoch: 02 [ 85824/244490 ( 35%)], Train Loss: 1.55042\n",
      "Epoch: 02 [ 86464/244490 ( 35%)], Train Loss: 1.54975\n",
      "Epoch: 02 [ 87104/244490 ( 36%)], Train Loss: 1.54915\n",
      "Epoch: 02 [ 87744/244490 ( 36%)], Train Loss: 1.54812\n",
      "Epoch: 02 [ 88384/244490 ( 36%)], Train Loss: 1.54697\n",
      "Epoch: 02 [ 89024/244490 ( 36%)], Train Loss: 1.54619\n",
      "Epoch: 02 [ 89664/244490 ( 37%)], Train Loss: 1.54537\n",
      "Epoch: 02 [ 90304/244490 ( 37%)], Train Loss: 1.54572\n",
      "Epoch: 02 [ 90944/244490 ( 37%)], Train Loss: 1.54586\n",
      "Epoch: 02 [ 91584/244490 ( 37%)], Train Loss: 1.54590\n",
      "Epoch: 02 [ 92224/244490 ( 38%)], Train Loss: 1.54519\n",
      "Epoch: 02 [ 92864/244490 ( 38%)], Train Loss: 1.54402\n",
      "Epoch: 02 [ 93504/244490 ( 38%)], Train Loss: 1.54325\n",
      "Epoch: 02 [ 94144/244490 ( 39%)], Train Loss: 1.54272\n",
      "Epoch: 02 [ 94784/244490 ( 39%)], Train Loss: 1.54225\n",
      "Epoch: 02 [ 95424/244490 ( 39%)], Train Loss: 1.54175\n",
      "Epoch: 02 [ 96064/244490 ( 39%)], Train Loss: 1.54065\n",
      "Epoch: 02 [ 96704/244490 ( 40%)], Train Loss: 1.53991\n",
      "Epoch: 02 [ 97344/244490 ( 40%)], Train Loss: 1.53883\n",
      "Epoch: 02 [ 97984/244490 ( 40%)], Train Loss: 1.53772\n",
      "Epoch: 02 [ 98624/244490 ( 40%)], Train Loss: 1.53686\n",
      "Epoch: 02 [ 99264/244490 ( 41%)], Train Loss: 1.53679\n",
      "Epoch: 02 [ 99904/244490 ( 41%)], Train Loss: 1.53638\n",
      "Epoch: 02 [100544/244490 ( 41%)], Train Loss: 1.53536\n",
      "Epoch: 02 [101184/244490 ( 41%)], Train Loss: 1.53498\n",
      "Epoch: 02 [101824/244490 ( 42%)], Train Loss: 1.53379\n",
      "Epoch: 02 [102464/244490 ( 42%)], Train Loss: 1.53290\n",
      "Epoch: 02 [103104/244490 ( 42%)], Train Loss: 1.53202\n",
      "Epoch: 02 [103744/244490 ( 42%)], Train Loss: 1.53174\n",
      "Epoch: 02 [104384/244490 ( 43%)], Train Loss: 1.53099\n",
      "Epoch: 02 [105024/244490 ( 43%)], Train Loss: 1.53008\n",
      "Epoch: 02 [105664/244490 ( 43%)], Train Loss: 1.52916\n",
      "Epoch: 02 [106304/244490 ( 43%)], Train Loss: 1.52847\n",
      "Epoch: 02 [106944/244490 ( 44%)], Train Loss: 1.52863\n",
      "Epoch: 02 [107584/244490 ( 44%)], Train Loss: 1.52812\n",
      "Epoch: 02 [108224/244490 ( 44%)], Train Loss: 1.52720\n",
      "Epoch: 02 [108864/244490 ( 45%)], Train Loss: 1.52607\n",
      "Epoch: 02 [109504/244490 ( 45%)], Train Loss: 1.52559\n",
      "Epoch: 02 [110144/244490 ( 45%)], Train Loss: 1.52465\n",
      "Epoch: 02 [110784/244490 ( 45%)], Train Loss: 1.52360\n",
      "Epoch: 02 [111424/244490 ( 46%)], Train Loss: 1.52322\n",
      "Epoch: 02 [112064/244490 ( 46%)], Train Loss: 1.52272\n",
      "Epoch: 02 [112704/244490 ( 46%)], Train Loss: 1.52274\n",
      "Epoch: 02 [113344/244490 ( 46%)], Train Loss: 1.52230\n",
      "Epoch: 02 [113984/244490 ( 47%)], Train Loss: 1.52194\n",
      "Epoch: 02 [114624/244490 ( 47%)], Train Loss: 1.52064\n",
      "Epoch: 02 [115264/244490 ( 47%)], Train Loss: 1.51980\n",
      "Epoch: 02 [115904/244490 ( 47%)], Train Loss: 1.51853\n",
      "Epoch: 02 [116544/244490 ( 48%)], Train Loss: 1.51772\n",
      "Epoch: 02 [117184/244490 ( 48%)], Train Loss: 1.51671\n",
      "Epoch: 02 [117824/244490 ( 48%)], Train Loss: 1.51591\n",
      "Epoch: 02 [118464/244490 ( 48%)], Train Loss: 1.51520\n",
      "Epoch: 02 [119104/244490 ( 49%)], Train Loss: 1.51509\n",
      "Epoch: 02 [119744/244490 ( 49%)], Train Loss: 1.51411\n",
      "Epoch: 02 [120384/244490 ( 49%)], Train Loss: 1.51409\n",
      "Epoch: 02 [121024/244490 ( 50%)], Train Loss: 1.51420\n",
      "Epoch: 02 [121664/244490 ( 50%)], Train Loss: 1.51299\n",
      "Epoch: 02 [122304/244490 ( 50%)], Train Loss: 1.51227\n",
      "Epoch: 02 [122944/244490 ( 50%)], Train Loss: 1.51183\n",
      "Epoch: 02 [123584/244490 ( 51%)], Train Loss: 1.51147\n",
      "Epoch: 02 [124224/244490 ( 51%)], Train Loss: 1.51158\n",
      "Epoch: 02 [124864/244490 ( 51%)], Train Loss: 1.51190\n",
      "Epoch: 02 [125504/244490 ( 51%)], Train Loss: 1.51125\n",
      "Epoch: 02 [126144/244490 ( 52%)], Train Loss: 1.51107\n",
      "Epoch: 02 [126784/244490 ( 52%)], Train Loss: 1.51144\n",
      "Epoch: 02 [127424/244490 ( 52%)], Train Loss: 1.51061\n",
      "Epoch: 02 [128064/244490 ( 52%)], Train Loss: 1.51033\n",
      "Epoch: 02 [128704/244490 ( 53%)], Train Loss: 1.51007\n",
      "Epoch: 02 [129344/244490 ( 53%)], Train Loss: 1.51007\n",
      "Epoch: 02 [129984/244490 ( 53%)], Train Loss: 1.50991\n",
      "Epoch: 02 [130624/244490 ( 53%)], Train Loss: 1.50930\n",
      "Epoch: 02 [131264/244490 ( 54%)], Train Loss: 1.50845\n",
      "Epoch: 02 [131904/244490 ( 54%)], Train Loss: 1.50857\n",
      "Epoch: 02 [132544/244490 ( 54%)], Train Loss: 1.50776\n",
      "Epoch: 02 [133184/244490 ( 54%)], Train Loss: 1.50681\n",
      "Epoch: 02 [133824/244490 ( 55%)], Train Loss: 1.50646\n",
      "Epoch: 02 [134464/244490 ( 55%)], Train Loss: 1.50583\n",
      "Epoch: 02 [135104/244490 ( 55%)], Train Loss: 1.50517\n",
      "Epoch: 02 [135744/244490 ( 56%)], Train Loss: 1.50481\n",
      "Epoch: 02 [136384/244490 ( 56%)], Train Loss: 1.50422\n",
      "Epoch: 02 [137024/244490 ( 56%)], Train Loss: 1.50404\n",
      "Epoch: 02 [137664/244490 ( 56%)], Train Loss: 1.50357\n",
      "Epoch: 02 [138304/244490 ( 57%)], Train Loss: 1.50301\n",
      "Epoch: 02 [138944/244490 ( 57%)], Train Loss: 1.50284\n",
      "Epoch: 02 [139584/244490 ( 57%)], Train Loss: 1.50295\n",
      "Epoch: 02 [140224/244490 ( 57%)], Train Loss: 1.50229\n",
      "Epoch: 02 [140864/244490 ( 58%)], Train Loss: 1.50145\n",
      "Epoch: 02 [141504/244490 ( 58%)], Train Loss: 1.50099\n",
      "Epoch: 02 [142144/244490 ( 58%)], Train Loss: 1.50011\n",
      "Epoch: 02 [142784/244490 ( 58%)], Train Loss: 1.49926\n",
      "Epoch: 02 [143424/244490 ( 59%)], Train Loss: 1.49928\n",
      "Epoch: 02 [144064/244490 ( 59%)], Train Loss: 1.49849\n",
      "Epoch: 02 [144704/244490 ( 59%)], Train Loss: 1.49871\n",
      "Epoch: 02 [145344/244490 ( 59%)], Train Loss: 1.49828\n",
      "Epoch: 02 [145984/244490 ( 60%)], Train Loss: 1.49761\n",
      "Epoch: 02 [146624/244490 ( 60%)], Train Loss: 1.49677\n",
      "Epoch: 02 [147264/244490 ( 60%)], Train Loss: 1.49615\n",
      "Epoch: 02 [147904/244490 ( 60%)], Train Loss: 1.49552\n",
      "Epoch: 02 [148544/244490 ( 61%)], Train Loss: 1.49544\n",
      "Epoch: 02 [149184/244490 ( 61%)], Train Loss: 1.49492\n",
      "Epoch: 02 [149824/244490 ( 61%)], Train Loss: 1.49448\n",
      "Epoch: 02 [150464/244490 ( 62%)], Train Loss: 1.49430\n",
      "Epoch: 02 [151104/244490 ( 62%)], Train Loss: 1.49393\n",
      "Epoch: 02 [151744/244490 ( 62%)], Train Loss: 1.49364\n",
      "Epoch: 02 [152384/244490 ( 62%)], Train Loss: 1.49346\n",
      "Epoch: 02 [153024/244490 ( 63%)], Train Loss: 1.49310\n",
      "Epoch: 02 [153664/244490 ( 63%)], Train Loss: 1.49270\n",
      "Epoch: 02 [154304/244490 ( 63%)], Train Loss: 1.49235\n",
      "Epoch: 02 [154944/244490 ( 63%)], Train Loss: 1.49194\n",
      "Epoch: 02 [155584/244490 ( 64%)], Train Loss: 1.49171\n",
      "Epoch: 02 [156224/244490 ( 64%)], Train Loss: 1.49156\n",
      "Epoch: 02 [156864/244490 ( 64%)], Train Loss: 1.49116\n",
      "Epoch: 02 [157504/244490 ( 64%)], Train Loss: 1.49045\n",
      "Epoch: 02 [158144/244490 ( 65%)], Train Loss: 1.49013\n",
      "Epoch: 02 [158784/244490 ( 65%)], Train Loss: 1.48952\n",
      "Epoch: 02 [159424/244490 ( 65%)], Train Loss: 1.48930\n",
      "Epoch: 02 [160064/244490 ( 65%)], Train Loss: 1.48871\n",
      "Epoch: 02 [160704/244490 ( 66%)], Train Loss: 1.48797\n",
      "Epoch: 02 [161344/244490 ( 66%)], Train Loss: 1.48739\n",
      "Epoch: 02 [161984/244490 ( 66%)], Train Loss: 1.48720\n",
      "Epoch: 02 [162624/244490 ( 67%)], Train Loss: 1.48663\n",
      "Epoch: 02 [163264/244490 ( 67%)], Train Loss: 1.48625\n",
      "Epoch: 02 [163904/244490 ( 67%)], Train Loss: 1.48606\n",
      "Epoch: 02 [164544/244490 ( 67%)], Train Loss: 1.48553\n",
      "Epoch: 02 [165184/244490 ( 68%)], Train Loss: 1.48510\n",
      "Epoch: 02 [165824/244490 ( 68%)], Train Loss: 1.48491\n",
      "Epoch: 02 [166464/244490 ( 68%)], Train Loss: 1.48428\n",
      "Epoch: 02 [167104/244490 ( 68%)], Train Loss: 1.48338\n",
      "Epoch: 02 [167744/244490 ( 69%)], Train Loss: 1.48287\n",
      "Epoch: 02 [168384/244490 ( 69%)], Train Loss: 1.48262\n",
      "Epoch: 02 [169024/244490 ( 69%)], Train Loss: 1.48295\n",
      "Epoch: 02 [169664/244490 ( 69%)], Train Loss: 1.48283\n",
      "Epoch: 02 [170304/244490 ( 70%)], Train Loss: 1.48221\n",
      "Epoch: 02 [170944/244490 ( 70%)], Train Loss: 1.48176\n",
      "Epoch: 02 [171584/244490 ( 70%)], Train Loss: 1.48162\n",
      "Epoch: 02 [172224/244490 ( 70%)], Train Loss: 1.48148\n",
      "Epoch: 02 [172864/244490 ( 71%)], Train Loss: 1.48175\n",
      "Epoch: 02 [173504/244490 ( 71%)], Train Loss: 1.48108\n",
      "Epoch: 02 [174144/244490 ( 71%)], Train Loss: 1.48052\n",
      "Epoch: 02 [174784/244490 ( 71%)], Train Loss: 1.48015\n",
      "Epoch: 02 [175424/244490 ( 72%)], Train Loss: 1.47974\n",
      "Epoch: 02 [176064/244490 ( 72%)], Train Loss: 1.47923\n",
      "Epoch: 02 [176704/244490 ( 72%)], Train Loss: 1.47914\n",
      "Epoch: 02 [177344/244490 ( 73%)], Train Loss: 1.47869\n",
      "Epoch: 02 [177984/244490 ( 73%)], Train Loss: 1.47757\n",
      "Epoch: 02 [178624/244490 ( 73%)], Train Loss: 1.47730\n",
      "Epoch: 02 [179264/244490 ( 73%)], Train Loss: 1.47703\n",
      "Epoch: 02 [179904/244490 ( 74%)], Train Loss: 1.47678\n",
      "Epoch: 02 [180544/244490 ( 74%)], Train Loss: 1.47623\n",
      "Epoch: 02 [181184/244490 ( 74%)], Train Loss: 1.47579\n",
      "Epoch: 02 [181824/244490 ( 74%)], Train Loss: 1.47536\n",
      "Epoch: 02 [182464/244490 ( 75%)], Train Loss: 1.47496\n",
      "Epoch: 02 [183104/244490 ( 75%)], Train Loss: 1.47474\n",
      "Epoch: 02 [183744/244490 ( 75%)], Train Loss: 1.47421\n",
      "Epoch: 02 [184384/244490 ( 75%)], Train Loss: 1.47395\n",
      "Epoch: 02 [185024/244490 ( 76%)], Train Loss: 1.47373\n",
      "Epoch: 02 [185664/244490 ( 76%)], Train Loss: 1.47281\n",
      "Epoch: 02 [186304/244490 ( 76%)], Train Loss: 1.47216\n",
      "Epoch: 02 [186944/244490 ( 76%)], Train Loss: 1.47187\n",
      "Epoch: 02 [187584/244490 ( 77%)], Train Loss: 1.47157\n",
      "Epoch: 02 [188224/244490 ( 77%)], Train Loss: 1.47103\n",
      "Epoch: 02 [188864/244490 ( 77%)], Train Loss: 1.47044\n",
      "Epoch: 02 [189504/244490 ( 78%)], Train Loss: 1.47000\n",
      "Epoch: 02 [190144/244490 ( 78%)], Train Loss: 1.46976\n",
      "Epoch: 02 [190784/244490 ( 78%)], Train Loss: 1.46965\n",
      "Epoch: 02 [191424/244490 ( 78%)], Train Loss: 1.46920\n",
      "Epoch: 02 [192064/244490 ( 79%)], Train Loss: 1.46915\n",
      "Epoch: 02 [192704/244490 ( 79%)], Train Loss: 1.46902\n",
      "Epoch: 02 [193344/244490 ( 79%)], Train Loss: 1.46873\n",
      "Epoch: 02 [193984/244490 ( 79%)], Train Loss: 1.46812\n",
      "Epoch: 02 [194624/244490 ( 80%)], Train Loss: 1.46770\n",
      "Epoch: 02 [195264/244490 ( 80%)], Train Loss: 1.46678\n",
      "Epoch: 02 [195904/244490 ( 80%)], Train Loss: 1.46637\n",
      "Epoch: 02 [196544/244490 ( 80%)], Train Loss: 1.46584\n",
      "Epoch: 02 [197184/244490 ( 81%)], Train Loss: 1.46564\n",
      "Epoch: 02 [197824/244490 ( 81%)], Train Loss: 1.46494\n",
      "Epoch: 02 [198464/244490 ( 81%)], Train Loss: 1.46433\n",
      "Epoch: 02 [199104/244490 ( 81%)], Train Loss: 1.46392\n",
      "Epoch: 02 [199744/244490 ( 82%)], Train Loss: 1.46324\n",
      "Epoch: 02 [200384/244490 ( 82%)], Train Loss: 1.46294\n",
      "Epoch: 02 [201024/244490 ( 82%)], Train Loss: 1.46240\n",
      "Epoch: 02 [201664/244490 ( 82%)], Train Loss: 1.46187\n",
      "Epoch: 02 [202304/244490 ( 83%)], Train Loss: 1.46137\n",
      "Epoch: 02 [202944/244490 ( 83%)], Train Loss: 1.46105\n",
      "Epoch: 02 [203584/244490 ( 83%)], Train Loss: 1.46107\n",
      "Epoch: 02 [204224/244490 ( 84%)], Train Loss: 1.46023\n",
      "Epoch: 02 [204864/244490 ( 84%)], Train Loss: 1.45953\n",
      "Epoch: 02 [205504/244490 ( 84%)], Train Loss: 1.45894\n",
      "Epoch: 02 [206144/244490 ( 84%)], Train Loss: 1.45842\n",
      "Epoch: 02 [206784/244490 ( 85%)], Train Loss: 1.45783\n",
      "Epoch: 02 [207424/244490 ( 85%)], Train Loss: 1.45745\n",
      "Epoch: 02 [208064/244490 ( 85%)], Train Loss: 1.45714\n",
      "Epoch: 02 [208704/244490 ( 85%)], Train Loss: 1.45687\n",
      "Epoch: 02 [209344/244490 ( 86%)], Train Loss: 1.45655\n",
      "Epoch: 02 [209984/244490 ( 86%)], Train Loss: 1.45614\n",
      "Epoch: 02 [210624/244490 ( 86%)], Train Loss: 1.45530\n",
      "Epoch: 02 [211264/244490 ( 86%)], Train Loss: 1.45485\n",
      "Epoch: 02 [211904/244490 ( 87%)], Train Loss: 1.45433\n",
      "Epoch: 02 [212544/244490 ( 87%)], Train Loss: 1.45403\n",
      "Epoch: 02 [213184/244490 ( 87%)], Train Loss: 1.45378\n",
      "Epoch: 02 [213824/244490 ( 87%)], Train Loss: 1.45358\n",
      "Epoch: 02 [214464/244490 ( 88%)], Train Loss: 1.45348\n",
      "Epoch: 02 [215104/244490 ( 88%)], Train Loss: 1.45281\n",
      "Epoch: 02 [215744/244490 ( 88%)], Train Loss: 1.45219\n",
      "Epoch: 02 [216384/244490 ( 89%)], Train Loss: 1.45148\n",
      "Epoch: 02 [217024/244490 ( 89%)], Train Loss: 1.45116\n",
      "Epoch: 02 [217664/244490 ( 89%)], Train Loss: 1.45037\n",
      "Epoch: 02 [218304/244490 ( 89%)], Train Loss: 1.44944\n",
      "Epoch: 02 [218944/244490 ( 90%)], Train Loss: 1.44884\n",
      "Epoch: 02 [219584/244490 ( 90%)], Train Loss: 1.44826\n",
      "Epoch: 02 [220224/244490 ( 90%)], Train Loss: 1.44820\n",
      "Epoch: 02 [220864/244490 ( 90%)], Train Loss: 1.44802\n",
      "Epoch: 02 [221504/244490 ( 91%)], Train Loss: 1.44739\n",
      "Epoch: 02 [222144/244490 ( 91%)], Train Loss: 1.44670\n",
      "Epoch: 02 [222784/244490 ( 91%)], Train Loss: 1.44621\n",
      "Epoch: 02 [223424/244490 ( 91%)], Train Loss: 1.44556\n",
      "Epoch: 02 [224064/244490 ( 92%)], Train Loss: 1.44507\n",
      "Epoch: 02 [224704/244490 ( 92%)], Train Loss: 1.44433\n",
      "Epoch: 02 [225344/244490 ( 92%)], Train Loss: 1.44381\n",
      "Epoch: 02 [225984/244490 ( 92%)], Train Loss: 1.44322\n",
      "Epoch: 02 [226624/244490 ( 93%)], Train Loss: 1.44279\n",
      "Epoch: 02 [227264/244490 ( 93%)], Train Loss: 1.44225\n",
      "Epoch: 02 [227904/244490 ( 93%)], Train Loss: 1.44172\n",
      "Epoch: 02 [228544/244490 ( 93%)], Train Loss: 1.44098\n",
      "Epoch: 02 [229184/244490 ( 94%)], Train Loss: 1.44046\n",
      "Epoch: 02 [229824/244490 ( 94%)], Train Loss: 1.43984\n",
      "Epoch: 02 [230464/244490 ( 94%)], Train Loss: 1.43942\n",
      "Epoch: 02 [231104/244490 ( 95%)], Train Loss: 1.43886\n",
      "Epoch: 02 [231744/244490 ( 95%)], Train Loss: 1.43830\n",
      "Epoch: 02 [232384/244490 ( 95%)], Train Loss: 1.43781\n",
      "Epoch: 02 [233024/244490 ( 95%)], Train Loss: 1.43768\n",
      "Epoch: 02 [233664/244490 ( 96%)], Train Loss: 1.43742\n",
      "Epoch: 02 [234304/244490 ( 96%)], Train Loss: 1.43725\n",
      "Epoch: 02 [234944/244490 ( 96%)], Train Loss: 1.43698\n",
      "Epoch: 02 [235584/244490 ( 96%)], Train Loss: 1.43650\n",
      "Epoch: 02 [236224/244490 ( 97%)], Train Loss: 1.43633\n",
      "Epoch: 02 [236864/244490 ( 97%)], Train Loss: 1.43630\n",
      "Epoch: 02 [237504/244490 ( 97%)], Train Loss: 1.43576\n",
      "Epoch: 02 [238144/244490 ( 97%)], Train Loss: 1.43544\n",
      "Epoch: 02 [238784/244490 ( 98%)], Train Loss: 1.43528\n",
      "Epoch: 02 [239424/244490 ( 98%)], Train Loss: 1.43483\n",
      "Epoch: 02 [240064/244490 ( 98%)], Train Loss: 1.43420\n",
      "Epoch: 02 [240704/244490 ( 98%)], Train Loss: 1.43394\n",
      "Epoch: 02 [241344/244490 ( 99%)], Train Loss: 1.43342\n",
      "Epoch: 02 [241984/244490 ( 99%)], Train Loss: 1.43318\n",
      "Epoch: 02 [242624/244490 ( 99%)], Train Loss: 1.43246\n",
      "Epoch: 02 [243264/244490 ( 99%)], Train Loss: 1.43230\n",
      "Epoch: 02 [243904/244490 (100%)], Train Loss: 1.43187\n",
      "Epoch: 02 [244490/244490 (100%)], Train Loss: 1.43128\n",
      "----Validation Results Summary----\n",
      "Epoch: [2] Valid Loss: 1.78961\n",
      "2 Epoch, Best epoch was updated! Valid Loss: 1.78961\n",
      "Saving model checkpoint to output/checkpoint-fold-2.\n",
      "\n",
      "Epoch: 03 [    64/244490 (  0%)], Train Loss: 1.18407\n",
      "Epoch: 03 [   704/244490 (  0%)], Train Loss: 1.28604\n",
      "Epoch: 03 [  1344/244490 (  1%)], Train Loss: 1.29810\n",
      "Epoch: 03 [  1984/244490 (  1%)], Train Loss: 1.29020\n",
      "Epoch: 03 [  2624/244490 (  1%)], Train Loss: 1.28492\n",
      "Epoch: 03 [  3264/244490 (  1%)], Train Loss: 1.30488\n",
      "Epoch: 03 [  3904/244490 (  2%)], Train Loss: 1.31431\n",
      "Epoch: 03 [  4544/244490 (  2%)], Train Loss: 1.31856\n",
      "Epoch: 03 [  5184/244490 (  2%)], Train Loss: 1.30838\n",
      "Epoch: 03 [  5824/244490 (  2%)], Train Loss: 1.31519\n",
      "Epoch: 03 [  6464/244490 (  3%)], Train Loss: 1.30821\n",
      "Epoch: 03 [  7104/244490 (  3%)], Train Loss: 1.31874\n",
      "Epoch: 03 [  7744/244490 (  3%)], Train Loss: 1.33647\n",
      "Epoch: 03 [  8384/244490 (  3%)], Train Loss: 1.33906\n",
      "Epoch: 03 [  9024/244490 (  4%)], Train Loss: 1.34134\n",
      "Epoch: 03 [  9664/244490 (  4%)], Train Loss: 1.34125\n",
      "Epoch: 03 [ 10304/244490 (  4%)], Train Loss: 1.33348\n",
      "Epoch: 03 [ 10944/244490 (  4%)], Train Loss: 1.33403\n",
      "Epoch: 03 [ 11584/244490 (  5%)], Train Loss: 1.32949\n",
      "Epoch: 03 [ 12224/244490 (  5%)], Train Loss: 1.32236\n",
      "Epoch: 03 [ 12864/244490 (  5%)], Train Loss: 1.31969\n",
      "Epoch: 03 [ 13504/244490 (  6%)], Train Loss: 1.31562\n",
      "Epoch: 03 [ 14144/244490 (  6%)], Train Loss: 1.31137\n",
      "Epoch: 03 [ 14784/244490 (  6%)], Train Loss: 1.30496\n",
      "Epoch: 03 [ 15424/244490 (  6%)], Train Loss: 1.30167\n",
      "Epoch: 03 [ 16064/244490 (  7%)], Train Loss: 1.30269\n",
      "Epoch: 03 [ 16704/244490 (  7%)], Train Loss: 1.30337\n",
      "Epoch: 03 [ 17344/244490 (  7%)], Train Loss: 1.30576\n",
      "Epoch: 03 [ 17984/244490 (  7%)], Train Loss: 1.31013\n",
      "Epoch: 03 [ 18624/244490 (  8%)], Train Loss: 1.31419\n",
      "Epoch: 03 [ 19264/244490 (  8%)], Train Loss: 1.31051\n",
      "Epoch: 03 [ 19904/244490 (  8%)], Train Loss: 1.30735\n",
      "Epoch: 03 [ 20544/244490 (  8%)], Train Loss: 1.30626\n",
      "Epoch: 03 [ 21184/244490 (  9%)], Train Loss: 1.30456\n",
      "Epoch: 03 [ 21824/244490 (  9%)], Train Loss: 1.30281\n",
      "Epoch: 03 [ 22464/244490 (  9%)], Train Loss: 1.30475\n",
      "Epoch: 03 [ 23104/244490 (  9%)], Train Loss: 1.30621\n",
      "Epoch: 03 [ 23744/244490 ( 10%)], Train Loss: 1.30479\n",
      "Epoch: 03 [ 24384/244490 ( 10%)], Train Loss: 1.29999\n",
      "Epoch: 03 [ 25024/244490 ( 10%)], Train Loss: 1.29727\n",
      "Epoch: 03 [ 25664/244490 ( 10%)], Train Loss: 1.29447\n",
      "Epoch: 03 [ 26304/244490 ( 11%)], Train Loss: 1.29273\n",
      "Epoch: 03 [ 26944/244490 ( 11%)], Train Loss: 1.29591\n",
      "Epoch: 03 [ 27584/244490 ( 11%)], Train Loss: 1.29511\n",
      "Epoch: 03 [ 28224/244490 ( 12%)], Train Loss: 1.29686\n",
      "Epoch: 03 [ 28864/244490 ( 12%)], Train Loss: 1.29752\n",
      "Epoch: 03 [ 29504/244490 ( 12%)], Train Loss: 1.29971\n",
      "Epoch: 03 [ 30144/244490 ( 12%)], Train Loss: 1.30292\n",
      "Epoch: 03 [ 30784/244490 ( 13%)], Train Loss: 1.30505\n",
      "Epoch: 03 [ 31424/244490 ( 13%)], Train Loss: 1.30601\n",
      "Epoch: 03 [ 32064/244490 ( 13%)], Train Loss: 1.30656\n",
      "Epoch: 03 [ 32704/244490 ( 13%)], Train Loss: 1.30494\n",
      "Epoch: 03 [ 33344/244490 ( 14%)], Train Loss: 1.30192\n",
      "Epoch: 03 [ 33984/244490 ( 14%)], Train Loss: 1.29957\n",
      "Epoch: 03 [ 34624/244490 ( 14%)], Train Loss: 1.29612\n",
      "Epoch: 03 [ 35264/244490 ( 14%)], Train Loss: 1.29494\n",
      "Epoch: 03 [ 35904/244490 ( 15%)], Train Loss: 1.29298\n",
      "Epoch: 03 [ 36544/244490 ( 15%)], Train Loss: 1.29509\n",
      "Epoch: 03 [ 37184/244490 ( 15%)], Train Loss: 1.29497\n",
      "Epoch: 03 [ 37824/244490 ( 15%)], Train Loss: 1.29455\n",
      "Epoch: 03 [ 38464/244490 ( 16%)], Train Loss: 1.29344\n",
      "Epoch: 03 [ 39104/244490 ( 16%)], Train Loss: 1.29432\n",
      "Epoch: 03 [ 39744/244490 ( 16%)], Train Loss: 1.29385\n",
      "Epoch: 03 [ 40384/244490 ( 17%)], Train Loss: 1.29184\n",
      "Epoch: 03 [ 41024/244490 ( 17%)], Train Loss: 1.28993\n",
      "Epoch: 03 [ 41664/244490 ( 17%)], Train Loss: 1.29059\n",
      "Epoch: 03 [ 42304/244490 ( 17%)], Train Loss: 1.28945\n",
      "Epoch: 03 [ 42944/244490 ( 18%)], Train Loss: 1.28886\n",
      "Epoch: 03 [ 43584/244490 ( 18%)], Train Loss: 1.28743\n",
      "Epoch: 03 [ 44224/244490 ( 18%)], Train Loss: 1.28611\n",
      "Epoch: 03 [ 44864/244490 ( 18%)], Train Loss: 1.28523\n",
      "Epoch: 03 [ 45504/244490 ( 19%)], Train Loss: 1.28528\n",
      "Epoch: 03 [ 46144/244490 ( 19%)], Train Loss: 1.28597\n",
      "Epoch: 03 [ 46784/244490 ( 19%)], Train Loss: 1.28537\n",
      "Epoch: 03 [ 47424/244490 ( 19%)], Train Loss: 1.28545\n",
      "Epoch: 03 [ 48064/244490 ( 20%)], Train Loss: 1.28517\n",
      "Epoch: 03 [ 48704/244490 ( 20%)], Train Loss: 1.28630\n",
      "Epoch: 03 [ 49344/244490 ( 20%)], Train Loss: 1.28546\n",
      "Epoch: 03 [ 49984/244490 ( 20%)], Train Loss: 1.28590\n",
      "Epoch: 03 [ 50624/244490 ( 21%)], Train Loss: 1.28769\n",
      "Epoch: 03 [ 51264/244490 ( 21%)], Train Loss: 1.28721\n",
      "Epoch: 03 [ 51904/244490 ( 21%)], Train Loss: 1.28739\n",
      "Epoch: 03 [ 52544/244490 ( 21%)], Train Loss: 1.28686\n",
      "Epoch: 03 [ 53184/244490 ( 22%)], Train Loss: 1.28633\n",
      "Epoch: 03 [ 53824/244490 ( 22%)], Train Loss: 1.28465\n",
      "Epoch: 03 [ 54464/244490 ( 22%)], Train Loss: 1.28202\n",
      "Epoch: 03 [ 55104/244490 ( 23%)], Train Loss: 1.27984\n",
      "Epoch: 03 [ 55744/244490 ( 23%)], Train Loss: 1.27906\n",
      "Epoch: 03 [ 56384/244490 ( 23%)], Train Loss: 1.27751\n",
      "Epoch: 03 [ 57024/244490 ( 23%)], Train Loss: 1.27683\n",
      "Epoch: 03 [ 57664/244490 ( 24%)], Train Loss: 1.27641\n",
      "Epoch: 03 [ 58304/244490 ( 24%)], Train Loss: 1.27548\n",
      "Epoch: 03 [ 58944/244490 ( 24%)], Train Loss: 1.27406\n",
      "Epoch: 03 [ 59584/244490 ( 24%)], Train Loss: 1.27355\n",
      "Epoch: 03 [ 60224/244490 ( 25%)], Train Loss: 1.27288\n",
      "Epoch: 03 [ 60864/244490 ( 25%)], Train Loss: 1.27123\n",
      "Epoch: 03 [ 61504/244490 ( 25%)], Train Loss: 1.27122\n",
      "Epoch: 03 [ 62144/244490 ( 25%)], Train Loss: 1.27188\n",
      "Epoch: 03 [ 62784/244490 ( 26%)], Train Loss: 1.27257\n",
      "Epoch: 03 [ 63424/244490 ( 26%)], Train Loss: 1.27315\n",
      "Epoch: 03 [ 64064/244490 ( 26%)], Train Loss: 1.27214\n",
      "Epoch: 03 [ 64704/244490 ( 26%)], Train Loss: 1.27198\n",
      "Epoch: 03 [ 65344/244490 ( 27%)], Train Loss: 1.27201\n",
      "Epoch: 03 [ 65984/244490 ( 27%)], Train Loss: 1.27103\n",
      "Epoch: 03 [ 66624/244490 ( 27%)], Train Loss: 1.27013\n",
      "Epoch: 03 [ 67264/244490 ( 28%)], Train Loss: 1.27037\n",
      "Epoch: 03 [ 67904/244490 ( 28%)], Train Loss: 1.26912\n",
      "Epoch: 03 [ 68544/244490 ( 28%)], Train Loss: 1.26897\n",
      "Epoch: 03 [ 69184/244490 ( 28%)], Train Loss: 1.26797\n",
      "Epoch: 03 [ 69824/244490 ( 29%)], Train Loss: 1.26857\n",
      "Epoch: 03 [ 70464/244490 ( 29%)], Train Loss: 1.26945\n",
      "Epoch: 03 [ 71104/244490 ( 29%)], Train Loss: 1.26856\n",
      "Epoch: 03 [ 71744/244490 ( 29%)], Train Loss: 1.26746\n",
      "Epoch: 03 [ 72384/244490 ( 30%)], Train Loss: 1.26697\n",
      "Epoch: 03 [ 73024/244490 ( 30%)], Train Loss: 1.26643\n",
      "Epoch: 03 [ 73664/244490 ( 30%)], Train Loss: 1.26551\n",
      "Epoch: 03 [ 74304/244490 ( 30%)], Train Loss: 1.26530\n",
      "Epoch: 03 [ 74944/244490 ( 31%)], Train Loss: 1.26440\n",
      "Epoch: 03 [ 75584/244490 ( 31%)], Train Loss: 1.26296\n",
      "Epoch: 03 [ 76224/244490 ( 31%)], Train Loss: 1.26112\n",
      "Epoch: 03 [ 76864/244490 ( 31%)], Train Loss: 1.26193\n",
      "Epoch: 03 [ 77504/244490 ( 32%)], Train Loss: 1.26271\n",
      "Epoch: 03 [ 78144/244490 ( 32%)], Train Loss: 1.26268\n",
      "Epoch: 03 [ 78784/244490 ( 32%)], Train Loss: 1.26268\n",
      "Epoch: 03 [ 79424/244490 ( 32%)], Train Loss: 1.26199\n",
      "Epoch: 03 [ 80064/244490 ( 33%)], Train Loss: 1.26232\n",
      "Epoch: 03 [ 80704/244490 ( 33%)], Train Loss: 1.26240\n",
      "Epoch: 03 [ 81344/244490 ( 33%)], Train Loss: 1.26198\n",
      "Epoch: 03 [ 81984/244490 ( 34%)], Train Loss: 1.26166\n",
      "Epoch: 03 [ 82624/244490 ( 34%)], Train Loss: 1.26104\n",
      "Epoch: 03 [ 83264/244490 ( 34%)], Train Loss: 1.26077\n",
      "Epoch: 03 [ 83904/244490 ( 34%)], Train Loss: 1.26090\n",
      "Epoch: 03 [ 84544/244490 ( 35%)], Train Loss: 1.25986\n",
      "Epoch: 03 [ 85184/244490 ( 35%)], Train Loss: 1.25865\n",
      "Epoch: 03 [ 85824/244490 ( 35%)], Train Loss: 1.25821\n",
      "Epoch: 03 [ 86464/244490 ( 35%)], Train Loss: 1.25796\n",
      "Epoch: 03 [ 87104/244490 ( 36%)], Train Loss: 1.25771\n",
      "Epoch: 03 [ 87744/244490 ( 36%)], Train Loss: 1.25703\n",
      "Epoch: 03 [ 88384/244490 ( 36%)], Train Loss: 1.25597\n",
      "Epoch: 03 [ 89024/244490 ( 36%)], Train Loss: 1.25513\n",
      "Epoch: 03 [ 89664/244490 ( 37%)], Train Loss: 1.25439\n",
      "Epoch: 03 [ 90304/244490 ( 37%)], Train Loss: 1.25435\n",
      "Epoch: 03 [ 90944/244490 ( 37%)], Train Loss: 1.25400\n",
      "Epoch: 03 [ 91584/244490 ( 37%)], Train Loss: 1.25413\n",
      "Epoch: 03 [ 92224/244490 ( 38%)], Train Loss: 1.25374\n",
      "Epoch: 03 [ 92864/244490 ( 38%)], Train Loss: 1.25296\n",
      "Epoch: 03 [ 93504/244490 ( 38%)], Train Loss: 1.25261\n",
      "Epoch: 03 [ 94144/244490 ( 39%)], Train Loss: 1.25333\n",
      "Epoch: 03 [ 94784/244490 ( 39%)], Train Loss: 1.25329\n",
      "Epoch: 03 [ 95424/244490 ( 39%)], Train Loss: 1.25347\n",
      "Epoch: 03 [ 96064/244490 ( 39%)], Train Loss: 1.25296\n",
      "Epoch: 03 [ 96704/244490 ( 40%)], Train Loss: 1.25263\n",
      "Epoch: 03 [ 97344/244490 ( 40%)], Train Loss: 1.25203\n",
      "Epoch: 03 [ 97984/244490 ( 40%)], Train Loss: 1.25115\n",
      "Epoch: 03 [ 98624/244490 ( 40%)], Train Loss: 1.25011\n",
      "Epoch: 03 [ 99264/244490 ( 41%)], Train Loss: 1.24965\n",
      "Epoch: 03 [ 99904/244490 ( 41%)], Train Loss: 1.24991\n",
      "Epoch: 03 [100544/244490 ( 41%)], Train Loss: 1.25037\n",
      "Epoch: 03 [101184/244490 ( 41%)], Train Loss: 1.25081\n",
      "Epoch: 03 [101824/244490 ( 42%)], Train Loss: 1.25050\n",
      "Epoch: 03 [102464/244490 ( 42%)], Train Loss: 1.24981\n",
      "Epoch: 03 [103104/244490 ( 42%)], Train Loss: 1.24930\n",
      "Epoch: 03 [103744/244490 ( 42%)], Train Loss: 1.24922\n",
      "Epoch: 03 [104384/244490 ( 43%)], Train Loss: 1.24919\n",
      "Epoch: 03 [105024/244490 ( 43%)], Train Loss: 1.24871\n",
      "Epoch: 03 [105664/244490 ( 43%)], Train Loss: 1.24836\n",
      "Epoch: 03 [106304/244490 ( 43%)], Train Loss: 1.24873\n",
      "Epoch: 03 [106944/244490 ( 44%)], Train Loss: 1.24913\n",
      "Epoch: 03 [107584/244490 ( 44%)], Train Loss: 1.24896\n",
      "Epoch: 03 [108224/244490 ( 44%)], Train Loss: 1.24877\n",
      "Epoch: 03 [108864/244490 ( 45%)], Train Loss: 1.24816\n",
      "Epoch: 03 [109504/244490 ( 45%)], Train Loss: 1.24790\n",
      "Epoch: 03 [110144/244490 ( 45%)], Train Loss: 1.24781\n",
      "Epoch: 03 [110784/244490 ( 45%)], Train Loss: 1.24686\n",
      "Epoch: 03 [111424/244490 ( 46%)], Train Loss: 1.24667\n",
      "Epoch: 03 [112064/244490 ( 46%)], Train Loss: 1.24690\n",
      "Epoch: 03 [112704/244490 ( 46%)], Train Loss: 1.24719\n",
      "Epoch: 03 [113344/244490 ( 46%)], Train Loss: 1.24739\n",
      "Epoch: 03 [113984/244490 ( 47%)], Train Loss: 1.24779\n",
      "Epoch: 03 [114624/244490 ( 47%)], Train Loss: 1.24748\n",
      "Epoch: 03 [115264/244490 ( 47%)], Train Loss: 1.24672\n",
      "Epoch: 03 [115904/244490 ( 47%)], Train Loss: 1.24588\n",
      "Epoch: 03 [116544/244490 ( 48%)], Train Loss: 1.24521\n",
      "Epoch: 03 [117184/244490 ( 48%)], Train Loss: 1.24426\n",
      "Epoch: 03 [117824/244490 ( 48%)], Train Loss: 1.24397\n",
      "Epoch: 03 [118464/244490 ( 48%)], Train Loss: 1.24419\n",
      "Epoch: 03 [119104/244490 ( 49%)], Train Loss: 1.24519\n",
      "Epoch: 03 [119744/244490 ( 49%)], Train Loss: 1.24479\n",
      "Epoch: 03 [120384/244490 ( 49%)], Train Loss: 1.24565\n",
      "Epoch: 03 [121024/244490 ( 50%)], Train Loss: 1.24651\n",
      "Epoch: 03 [121664/244490 ( 50%)], Train Loss: 1.24617\n",
      "Epoch: 03 [122304/244490 ( 50%)], Train Loss: 1.24584\n",
      "Epoch: 03 [122944/244490 ( 50%)], Train Loss: 1.24562\n",
      "Epoch: 03 [123584/244490 ( 51%)], Train Loss: 1.24545\n",
      "Epoch: 03 [124224/244490 ( 51%)], Train Loss: 1.24551\n",
      "Epoch: 03 [124864/244490 ( 51%)], Train Loss: 1.24552\n",
      "Epoch: 03 [125504/244490 ( 51%)], Train Loss: 1.24531\n",
      "Epoch: 03 [126144/244490 ( 52%)], Train Loss: 1.24536\n",
      "Epoch: 03 [126784/244490 ( 52%)], Train Loss: 1.24590\n",
      "Epoch: 03 [127424/244490 ( 52%)], Train Loss: 1.24575\n",
      "Epoch: 03 [128064/244490 ( 52%)], Train Loss: 1.24585\n",
      "Epoch: 03 [128704/244490 ( 53%)], Train Loss: 1.24580\n",
      "Epoch: 03 [129344/244490 ( 53%)], Train Loss: 1.24588\n",
      "Epoch: 03 [129984/244490 ( 53%)], Train Loss: 1.24599\n",
      "Epoch: 03 [130624/244490 ( 53%)], Train Loss: 1.24562\n",
      "Epoch: 03 [131264/244490 ( 54%)], Train Loss: 1.24518\n",
      "Epoch: 03 [131904/244490 ( 54%)], Train Loss: 1.24526\n",
      "Epoch: 03 [132544/244490 ( 54%)], Train Loss: 1.24446\n",
      "Epoch: 03 [133184/244490 ( 54%)], Train Loss: 1.24389\n",
      "Epoch: 03 [133824/244490 ( 55%)], Train Loss: 1.24332\n",
      "Epoch: 03 [134464/244490 ( 55%)], Train Loss: 1.24258\n",
      "Epoch: 03 [135104/244490 ( 55%)], Train Loss: 1.24244\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\n",
    "for fold in range(5):\n",
    "    print();print()\n",
    "    print('-'*50)\n",
    "    print(f'FOLD: {fold}')\n",
    "    print('-'*50)\n",
    "    run(train, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c434092c-c6a6-4db8-9529-6382ff609d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
