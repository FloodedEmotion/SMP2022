{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e5e5fa-4616-4900-bb69-18d9c519eac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This script is used for text regression\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ca3b072-0cdc-4d0c-af2f-3bec302ad7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.sh\n",
    "git clone -b 22.04-dev https://github.com/NVIDIA/apex\n",
    "cd apex\n",
    "pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./\n",
    "rm -rf ./apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11dddfe5-6cf3-4852-85d3-d94c128f0401",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !sh setup.sh\n",
    "# !pip -q install madgrad\n",
    "# !pip -q install lit_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06db59df-fa8e-46ff-84b2-58973197c823",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-02 12:29:45.621110: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-02 12:29:46.479866: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-02 12:29:46.479938: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-02 12:29:46.479948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apex AMP Installed :: False\n",
      "SWA Available :: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "gc.enable()\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn import model_selection\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader,\n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    APEX_INSTALLED = True\n",
    "except ImportError:\n",
    "    APEX_INSTALLED = False\n",
    "\n",
    "from madgrad import MADGRAD\n",
    "\n",
    "try:\n",
    "    from torch.optim.swa_utils import (\n",
    "        AveragedModel, update_bn, SWALR\n",
    "    )\n",
    "    SWA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SWA_AVAILABLE = False\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    logging,\n",
    "    MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING,\n",
    ")\n",
    "logging.set_verbosity_warning()\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def optimal_num_of_loader_workers():\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n",
    "    return optimal_value\n",
    "\n",
    "print(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\n",
    "print(f\"SWA Available :: {SWA_AVAILABLE}\")\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d31579-92a1-463e-b096-39413fcfce43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('../data/forauto.csv')\n",
    "alltags = all_data[['Alltags','label']]\n",
    "train = alltags[:-180581]\n",
    "test = alltags[-180581:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "020a89e9-17d8-4ddf-a41e-2f274bafd8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88/1693006389.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"kfold\"] = -1\n"
     ]
    }
   ],
   "source": [
    "# train = pd.read_csv('../input/commonlitreadabilityprize/train.csv', low_memory=False)\n",
    "def create_folds(data, num_splits):\n",
    "    data[\"kfold\"] = -1\n",
    "    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=2021)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data)):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    return data\n",
    "train = create_folds(train, num_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "613a22bf-9e39-4620-aae3-c71b8514ae2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # model\n",
    "    num_labels = 1\n",
    "    model_type = 'roberta'\n",
    "    model_name_or_path = 'roberta-base'\n",
    "    config_name = 'roberta-base'\n",
    "    fp16 = True if APEX_INSTALLED else False\n",
    "    fp16_opt_level = \"O1\"\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer_name = 'roberta-base'\n",
    "    max_seq_length = 250\n",
    "\n",
    "    # train\n",
    "    epochs = 3\n",
    "    train_batch_size = 64\n",
    "    eval_batch_size = 32\n",
    "\n",
    "    # optimizer\n",
    "    optimizer_type = 'MADGRAD'\n",
    "    learning_rate = 2e-5\n",
    "    weight_decay = 1e-5\n",
    "    epsilon = 1e-6\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    # stochastic weight averaging\n",
    "    swa = False\n",
    "    swa_start = 2\n",
    "    swa_learning_rate = 1e-4\n",
    "    anneal_epochs=1 \n",
    "    anneal_strategy='cos'\n",
    "\n",
    "    # scheduler\n",
    "    decay_name = 'cosine-warmup'\n",
    "    warmup_ratio = 0.03\n",
    "\n",
    "    # logging\n",
    "    logging_steps = 100\n",
    "\n",
    "    # evaluate\n",
    "    output_dir = 'output'\n",
    "    seed = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eac10ff8-1912-4426-bf50-f5f33bc8501d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.max = 0\n",
    "        self.min = 1e5\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        if val > self.max:\n",
    "            self.max = val\n",
    "        if val < self.min:\n",
    "            self.min = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48fb6d90-51c3-4ced-ad99-4e37ba71ee3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
    "        super(DatasetRetriever, self).__init__()\n",
    "        self.data = data\n",
    "        self.is_test = is_test\n",
    "        self.excerpts = self.data.Alltags.values.tolist()\n",
    "        if not self.is_test:\n",
    "            self.targets = self.data.label.values.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        excerpt = self.excerpts[item]\n",
    "        features = self.convert_examples_to_features(\n",
    "            excerpt, self.tokenizer, \n",
    "            self.max_len\n",
    "        )\n",
    "        features = {key : torch.tensor(value, dtype=torch.long) for key, value in features.items()}\n",
    "        if not self.is_test:\n",
    "            label = self.targets[item]\n",
    "            features['labels'] = torch.tensor(label, dtype=torch.double)\n",
    "        return features\n",
    "    \n",
    "    def convert_examples_to_features(self, example, tokenizer, max_len):\n",
    "        features = tokenizer.encode_plus(\n",
    "            example.replace('\\n', ''), \n",
    "            max_length=max_len, \n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ef08c93-4201-4ba1-b10b-7451c89a8be7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name, \n",
    "        config\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = config\n",
    "        self.roberta = AutoModel.from_pretrained(\n",
    "            model_name, \n",
    "            config=config\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p=0.0)\n",
    "        self.high_dropout = nn.Dropout(p=0.0)\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-5)\n",
    "        self._init_weights(self.layer_norm)\n",
    "        self.regressor = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self._init_weights(self.regressor)\n",
    "        \n",
    "        weights_init = torch.zeros(config.num_hidden_layers + 1).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    " \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    " \n",
    "    def forward(\n",
    "        self, input_ids=None,\n",
    "        attention_mask=None, labels=None\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        all_hidden_states = outputs[2]\n",
    "        \n",
    "        # weighted layer pooling\n",
    "        cls_embeddings = torch.stack(\n",
    "            [self.dropout(layer[:, 0]) for layer in all_hidden_states], \n",
    "            dim=2\n",
    "        )\n",
    "        cls_output = (\n",
    "            torch.softmax(self.layer_weights, dim=0) * cls_embeddings\n",
    "        ).sum(-1)\n",
    "        cls_output = self.layer_norm(cls_output)\n",
    "        \n",
    "        # multi-sample dropout\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [self.regressor(self.high_dropout(cls_output)) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    " \n",
    "        # calculate loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # regression task\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            logits = logits.view(-1).to(labels.dtype)\n",
    "            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n",
    "        output = (logits,) + outputs[2:]\n",
    "        \n",
    "        del all_hidden_states, cls_embeddings\n",
    "        del cls_output, logits\n",
    "        gc.collect();\n",
    "        \n",
    "        return ((loss,) + output) if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "294684cb-fbba-49a7-a09a-b26565401c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimizer_grouped_parameters(args, model):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
    "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
    "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': args.weight_decay, 'lr': args.learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': args.weight_decay, 'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': args.weight_decay, 'lr': args.learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': args.learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': args.learning_rate},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': args.learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.named_parameters() if args.model_type not in n], 'lr':args.learning_rate*20, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0acb9db-6f27-42f9-8317-8bb3881e9601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model(args, output_attentions=False):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n",
    "    config = AutoConfig.from_pretrained(args.config_name)\n",
    "    config.update({'num_labels':args.num_labels})\n",
    "    config.update({\"output_hidden_states\":True})\n",
    "    if output_attentions:\n",
    "        config.update({\"output_attentions\":True})\n",
    "    model = Model(args.model_name_or_path, config=config)\n",
    "    return model, config, tokenizer\n",
    "\n",
    "def make_optimizer(args, model):\n",
    "    optimizer_grouped_parameters = get_optimizer_grouped_parameters(args, model)\n",
    "    if args.optimizer_type == \"AdamW\":\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            eps=args.epsilon,\n",
    "            correct_bias=not args.use_bertadam\n",
    "        )\n",
    "    else:\n",
    "        optimizer = MADGRAD(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            eps=args.epsilon,\n",
    "            weight_decay=args.weight_decay\n",
    "        )\n",
    "    return optimizer\n",
    "\n",
    "def make_scheduler(\n",
    "    args, optimizer, \n",
    "    num_warmup_steps, \n",
    "    num_training_steps\n",
    "):\n",
    "    if args.decay_name == \"cosine-warmup\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    else:\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "    return scheduler    \n",
    "\n",
    "def make_loader(\n",
    "    args, data, \n",
    "    tokenizer, fold\n",
    "):\n",
    "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
    "\n",
    "    train_dataset = DatasetRetriever(train_set, tokenizer, args.max_seq_length)\n",
    "    valid_dataset = DatasetRetriever(valid_set, tokenizer, args.max_seq_length)\n",
    "    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n",
    "    \n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    valid_sampler = SequentialSampler(valid_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=optimal_num_of_loader_workers(),\n",
    "        pin_memory=True,\n",
    "        drop_last=False \n",
    "    )\n",
    "\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=args.eval_batch_size, \n",
    "        sampler=valid_sampler,\n",
    "        num_workers=optimal_num_of_loader_workers(),\n",
    "        pin_memory=True, \n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2f00bda-acea-4853-8742-18efccb6e511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, model, tokenizer, \n",
    "        optimizer, scheduler, \n",
    "        swa_model=None, swa_scheduler=None\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.swa_model = swa_model\n",
    "        self.swa_scheduler = swa_scheduler\n",
    "\n",
    "    def train(\n",
    "        self, args, \n",
    "        train_dataloader, \n",
    "        epoch, result_dict\n",
    "    ):\n",
    "        count = 0\n",
    "        losses = AverageMeter()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        self.model.train()\n",
    "        \n",
    "        fix_all_seeds(args.seed)\n",
    "        for batch_idx, batch_data in enumerate(train_dataloader):\n",
    "            input_ids, attention_mask, labels = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], batch_data['labels']\n",
    "            input_ids, attention_mask, labels = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n",
    "\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss, logits = outputs[:2]\n",
    "            \n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            count += labels.size(0)\n",
    "            losses.update(loss.item(), input_ids.size(0))\n",
    "\n",
    "            if args.fp16:\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(self.optimizer), args.max_grad_norm)\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            if not args.swa:\n",
    "                self.scheduler.step()\n",
    "            else:\n",
    "                if (epoch+1) < args.swa_start:\n",
    "                    self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            if (batch_idx % args.logging_steps == 0) or (batch_idx+1)==len(train_dataloader):\n",
    "                _s = str(len(str(len(train_dataloader.sampler))))\n",
    "                ret = [\n",
    "                    ('Epoch: {:0>2} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_dataloader.sampler), 100 * count / len(train_dataloader.sampler)),\n",
    "                    'Train Loss: {: >4.5f}'.format(losses.avg),\n",
    "                ]\n",
    "                print(', '.join(ret))\n",
    "            \n",
    "        if args.swa and (epoch+1) >= args.swa_start:\n",
    "            self.swa_model.update_parameters(self.model)\n",
    "            self.swa_scheduler.step()\n",
    "\n",
    "        result_dict['train_loss'].append(losses.avg)\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95b1c8aa-b414-40b7-9822-a2e5cb7a3661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model, swa_model):\n",
    "        self.model = model\n",
    "        self.swa_model = swa_model\n",
    "    \n",
    "    def save(self, result, output_dir):\n",
    "        with open(f'{output_dir}/result_dict.json', 'w') as f:\n",
    "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
    "\n",
    "    def evaluate(self, valid_dataloader, epoch, result_dict):\n",
    "        losses = AverageMeter()\n",
    "        for batch_idx, batch_data in enumerate(valid_dataloader):\n",
    "            self.model = self.model.eval()\n",
    "            input_ids, attention_mask, labels = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], batch_data['labels']\n",
    "            input_ids, attention_mask, labels = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n",
    "            with torch.no_grad():            \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss, logits = outputs[:2]\n",
    "                losses.update(loss.item(), input_ids.size(0))\n",
    "        print('----Validation Results Summary----')\n",
    "        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n",
    "        result_dict['val_loss'].append(losses.avg)        \n",
    "        return result_dict\n",
    "    \n",
    "    def swa_evaluate(self, valid_dataloader, epoch, result_dict):\n",
    "        losses = AverageMeter()\n",
    "        for batch_idx, batch_data in enumerate(valid_dataloader):\n",
    "            self.swa_model = self.swa_model.eval()\n",
    "            input_ids, attention_mask, labels = \\\n",
    "                batch_data['input_ids'], batch_data['attention_mask'], batch_data['labels']\n",
    "            input_ids, attention_mask, labels = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), labels.cuda()\n",
    "            with torch.no_grad():            \n",
    "                outputs = self.swa_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss, logits = outputs[:2]\n",
    "                losses.update(loss.item(), input_ids.size(0))\n",
    "        print('----SWA Validation Results Summary----')\n",
    "        print('Epoch: [{}] Valid Loss: {: >4.5f}'.format(epoch, losses.avg))\n",
    "        result_dict['swa_loss'].append(losses.avg)        \n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c846db24-34b5-4425-b0d4-4450e10ddb2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_training(args, data, fold):\n",
    "    fix_all_seeds(args.seed)\n",
    "    \n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    \n",
    "    # model\n",
    "    model, model_config, tokenizer = make_model(args)\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "    \n",
    "    # data loaders for training and evaluation\n",
    "    train_dataloader, valid_dataloader = make_loader(args, data, tokenizer, fold)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = make_optimizer(args, model)\n",
    "\n",
    "    # scheduler\n",
    "    num_training_steps = len(train_dataloader) * args.epochs\n",
    "    if args.warmup_ratio > 0:\n",
    "        num_warmup_steps = int(args.warmup_ratio * num_training_steps)\n",
    "    else:\n",
    "        num_warmup_steps = 0\n",
    "    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}\")\n",
    "    scheduler = make_scheduler(args, optimizer, num_warmup_steps, num_training_steps)\n",
    "\n",
    "    # stochastic weight averaging\n",
    "    swa_model = AveragedModel(model)\n",
    "    swa_scheduler = SWALR(\n",
    "        optimizer, swa_lr=args.swa_learning_rate, \n",
    "        anneal_epochs=args.anneal_epochs, \n",
    "        anneal_strategy=args.anneal_strategy\n",
    "    )\n",
    "\n",
    "    print(f\"Total Training Steps: {num_training_steps}, Total Warmup Steps: {num_warmup_steps}, SWA Start Step: {args.swa_start}\")\n",
    "\n",
    "    # mixed precision training with NVIDIA Apex\n",
    "    if args.fp16:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "    \n",
    "    result_dict = {\n",
    "        'epoch':[], \n",
    "        'train_loss': [], \n",
    "        'val_loss' : [], \n",
    "        'swa_loss': [],\n",
    "        'best_val_loss': np.inf\n",
    "    }\n",
    "\n",
    "    return (\n",
    "        model, model_config, tokenizer, optimizer, scheduler, \n",
    "        train_dataloader, valid_dataloader, result_dict,\n",
    "        swa_model, swa_scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "170718ef-4ce9-47e8-80e1-519268a5e9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(data, fold):\n",
    "    args = Config()\n",
    "    model, model_config, tokenizer, optimizer, scheduler, train_dataloader, \\\n",
    "        valid_dataloader, result_dict, swa_model, swa_scheduler = init_training(args, data, fold)\n",
    "    \n",
    "    trainer = Trainer(model, tokenizer, optimizer, scheduler, swa_model, swa_scheduler)\n",
    "    evaluator = Evaluator(model, swa_model)\n",
    "\n",
    "    train_time_list = []\n",
    "    valid_time_list = []\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        result_dict['epoch'].append(epoch)\n",
    "\n",
    "        # Train\n",
    "        torch.cuda.synchronize()\n",
    "        tic1 = time.time()\n",
    "        result_dict = trainer.train(\n",
    "            args, train_dataloader, \n",
    "            epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "        tic2 = time.time() \n",
    "        train_time_list.append(tic2 - tic1)\n",
    "        \n",
    "        # Evaluate\n",
    "        torch.cuda.synchronize()\n",
    "        tic3 = time.time()\n",
    "        result_dict = evaluator.evaluate(\n",
    "            valid_dataloader, epoch, result_dict\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "        tic4 = time.time() \n",
    "        valid_time_list.append(tic4 - tic3)\n",
    "            \n",
    "        output_dir = os.path.join(args.output_dir, f\"checkpoint-fold-{fold}\")\n",
    "        if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
    "            print(\"{} Epoch, Best epoch was updated! Valid Loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
    "            result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]        \n",
    "            \n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{output_dir}/pytorch_model.bin\")\n",
    "            model_config.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "            print(f\"Saving model checkpoint to {output_dir}.\")\n",
    "    \n",
    "            #torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "            #torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "            #print(f\"Saving optimizer and scheduler states to {output_dir}.\")\n",
    "        print()\n",
    "        \n",
    "    if args.swa:\n",
    "        update_bn(train_dataloader, swa_model, device=torch.device('cuda'))\n",
    "    result_dict = evaluator.swa_evaluate(valid_dataloader, epoch, result_dict)\n",
    "    \n",
    "    evaluator.save(result_dict, output_dir)\n",
    "    torch.save(swa_model.state_dict(), f\"{output_dir}/swa_pytorch_model.bin\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"Total Training Time: {np.sum(train_time_list)}secs, Average Training Time per Epoch: {np.mean(train_time_list)}secs.\")\n",
    "    print(f\"Total Validation Time: {np.sum(valid_time_list)}secs, Average Validation Time per Epoch: {np.mean(valid_time_list)}secs.\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    del trainer, evaluator\n",
    "    del model, model_config, tokenizer\n",
    "    del optimizer, scheduler\n",
    "    del train_dataloader, valid_dataloader, result_dict\n",
    "    del swa_model, swa_scheduler\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5be57a1-bcb2-49cd-8679-c228890d6560",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "FOLD: 0\n",
      "--------------------------------------------------\n",
      "Model pushed to 2 GPU(s), type NVIDIA GeForce RTX 3090.\n",
      "Num examples Train= 244490, Num examples Valid=61123\n",
      "Total Training Steps: 11463, Total Warmup Steps: 343\n",
      "Total Training Steps: 11463, Total Warmup Steps: 343, SWA Start Step: 2\n",
      "Epoch: 00 [    64/244490 (  0%)], Train Loss: 5.48113\n",
      "Epoch: 00 [  6464/244490 (  3%)], Train Loss: 3.22064\n",
      "Epoch: 00 [ 12864/244490 (  5%)], Train Loss: 2.68849\n",
      "Epoch: 00 [ 19264/244490 (  8%)], Train Loss: 2.50205\n",
      "Epoch: 00 [ 25664/244490 ( 10%)], Train Loss: 2.38964\n",
      "Epoch: 00 [ 32064/244490 ( 13%)], Train Loss: 2.29342\n",
      "Epoch: 00 [ 38464/244490 ( 16%)], Train Loss: 2.22184\n",
      "Epoch: 00 [ 44864/244490 ( 18%)], Train Loss: 2.17067\n",
      "Epoch: 00 [ 51264/244490 ( 21%)], Train Loss: 2.12324\n",
      "Epoch: 00 [ 57664/244490 ( 24%)], Train Loss: 2.08388\n",
      "Epoch: 00 [ 64064/244490 ( 26%)], Train Loss: 2.05188\n",
      "Epoch: 00 [ 70464/244490 ( 29%)], Train Loss: 2.02877\n",
      "Epoch: 00 [ 76864/244490 ( 31%)], Train Loss: 1.99944\n",
      "Epoch: 00 [ 83264/244490 ( 34%)], Train Loss: 1.97723\n",
      "Epoch: 00 [ 89664/244490 ( 37%)], Train Loss: 1.95306\n",
      "Epoch: 00 [ 96064/244490 ( 39%)], Train Loss: 1.93182\n",
      "Epoch: 00 [102464/244490 ( 42%)], Train Loss: 1.91605\n",
      "Epoch: 00 [108864/244490 ( 45%)], Train Loss: 1.90003\n",
      "Epoch: 00 [115264/244490 ( 47%)], Train Loss: 1.88421\n",
      "Epoch: 00 [121664/244490 ( 50%)], Train Loss: 1.87001\n",
      "Epoch: 00 [128064/244490 ( 52%)], Train Loss: 1.85981\n",
      "Epoch: 00 [134464/244490 ( 55%)], Train Loss: 1.84917\n",
      "Epoch: 00 [140864/244490 ( 58%)], Train Loss: 1.83810\n",
      "Epoch: 00 [147264/244490 ( 60%)], Train Loss: 1.82840\n",
      "Epoch: 00 [153664/244490 ( 63%)], Train Loss: 1.81771\n",
      "Epoch: 00 [160064/244490 ( 65%)], Train Loss: 1.80881\n",
      "Epoch: 00 [166464/244490 ( 68%)], Train Loss: 1.79966\n",
      "Epoch: 00 [172864/244490 ( 71%)], Train Loss: 1.78976\n",
      "Epoch: 00 [179264/244490 ( 73%)], Train Loss: 1.78055\n",
      "Epoch: 00 [185664/244490 ( 76%)], Train Loss: 1.77100\n",
      "Epoch: 00 [192064/244490 ( 79%)], Train Loss: 1.76266\n",
      "Epoch: 00 [198464/244490 ( 81%)], Train Loss: 1.75423\n",
      "Epoch: 00 [204864/244490 ( 84%)], Train Loss: 1.74670\n",
      "Epoch: 00 [211264/244490 ( 86%)], Train Loss: 1.73904\n",
      "Epoch: 00 [217664/244490 ( 89%)], Train Loss: 1.73285\n",
      "Epoch: 00 [224064/244490 ( 92%)], Train Loss: 1.72552\n",
      "Epoch: 00 [230464/244490 ( 94%)], Train Loss: 1.71951\n",
      "Epoch: 00 [236864/244490 ( 97%)], Train Loss: 1.71430\n",
      "Epoch: 00 [243264/244490 ( 99%)], Train Loss: 1.70791\n",
      "Epoch: 00 [244490/244490 (100%)], Train Loss: 1.70618\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] Valid Loss: 1.39646\n",
      "0 Epoch, Best epoch was updated! Valid Loss: 1.39646\n",
      "Saving model checkpoint to output/checkpoint-fold-0.\n",
      "\n",
      "Epoch: 01 [    64/244490 (  0%)], Train Loss: 1.35299\n",
      "Epoch: 01 [  6464/244490 (  3%)], Train Loss: 1.44750\n",
      "Epoch: 01 [ 12864/244490 (  5%)], Train Loss: 1.44031\n",
      "Epoch: 01 [ 19264/244490 (  8%)], Train Loss: 1.44319\n",
      "Epoch: 01 [ 25664/244490 ( 10%)], Train Loss: 1.44622\n",
      "Epoch: 01 [ 32064/244490 ( 13%)], Train Loss: 1.43238\n",
      "Epoch: 01 [ 38464/244490 ( 16%)], Train Loss: 1.42508\n",
      "Epoch: 01 [ 44864/244490 ( 18%)], Train Loss: 1.42122\n",
      "Epoch: 01 [ 51264/244490 ( 21%)], Train Loss: 1.41480\n",
      "Epoch: 01 [ 57664/244490 ( 24%)], Train Loss: 1.40988\n",
      "Epoch: 01 [ 64064/244490 ( 26%)], Train Loss: 1.40340\n",
      "Epoch: 01 [ 70464/244490 ( 29%)], Train Loss: 1.40206\n",
      "Epoch: 01 [ 76864/244490 ( 31%)], Train Loss: 1.39636\n",
      "Epoch: 01 [ 83264/244490 ( 34%)], Train Loss: 1.39271\n",
      "Epoch: 01 [ 89664/244490 ( 37%)], Train Loss: 1.38190\n",
      "Epoch: 01 [ 96064/244490 ( 39%)], Train Loss: 1.37510\n",
      "Epoch: 01 [102464/244490 ( 42%)], Train Loss: 1.37140\n",
      "Epoch: 01 [108864/244490 ( 45%)], Train Loss: 1.36604\n",
      "Epoch: 01 [115264/244490 ( 47%)], Train Loss: 1.35950\n",
      "Epoch: 01 [121664/244490 ( 50%)], Train Loss: 1.35419\n",
      "Epoch: 01 [128064/244490 ( 52%)], Train Loss: 1.35200\n",
      "Epoch: 01 [134464/244490 ( 55%)], Train Loss: 1.34829\n",
      "Epoch: 01 [140864/244490 ( 58%)], Train Loss: 1.34538\n",
      "Epoch: 01 [147264/244490 ( 60%)], Train Loss: 1.34350\n",
      "Epoch: 01 [153664/244490 ( 63%)], Train Loss: 1.33857\n",
      "Epoch: 01 [160064/244490 ( 65%)], Train Loss: 1.33532\n",
      "Epoch: 01 [166464/244490 ( 68%)], Train Loss: 1.33156\n",
      "Epoch: 01 [172864/244490 ( 71%)], Train Loss: 1.32569\n",
      "Epoch: 01 [179264/244490 ( 73%)], Train Loss: 1.32122\n",
      "Epoch: 01 [185664/244490 ( 76%)], Train Loss: 1.31670\n",
      "Epoch: 01 [192064/244490 ( 79%)], Train Loss: 1.31194\n",
      "Epoch: 01 [198464/244490 ( 81%)], Train Loss: 1.30788\n",
      "Epoch: 01 [204864/244490 ( 84%)], Train Loss: 1.30381\n",
      "Epoch: 01 [211264/244490 ( 86%)], Train Loss: 1.30033\n",
      "Epoch: 01 [217664/244490 ( 89%)], Train Loss: 1.29779\n",
      "Epoch: 01 [224064/244490 ( 92%)], Train Loss: 1.29332\n",
      "Epoch: 01 [230464/244490 ( 94%)], Train Loss: 1.28996\n",
      "Epoch: 01 [236864/244490 ( 97%)], Train Loss: 1.28733\n",
      "Epoch: 01 [243264/244490 ( 99%)], Train Loss: 1.28361\n",
      "Epoch: 01 [244490/244490 (100%)], Train Loss: 1.28250\n",
      "----Validation Results Summary----\n",
      "Epoch: [1] Valid Loss: 1.35262\n",
      "1 Epoch, Best epoch was updated! Valid Loss: 1.35262\n",
      "Saving model checkpoint to output/checkpoint-fold-0.\n",
      "\n",
      "Epoch: 02 [    64/244490 (  0%)], Train Loss: 1.18988\n",
      "Epoch: 02 [  6464/244490 (  3%)], Train Loss: 1.88290\n",
      "Epoch: 02 [ 12864/244490 (  5%)], Train Loss: 1.92656\n",
      "Epoch: 02 [ 19264/244490 (  8%)], Train Loss: 1.95304\n",
      "Epoch: 02 [ 25664/244490 ( 10%)], Train Loss: 1.95603\n",
      "Epoch: 02 [ 32064/244490 ( 13%)], Train Loss: 1.92859\n",
      "Epoch: 02 [ 38464/244490 ( 16%)], Train Loss: 1.90434\n",
      "Epoch: 02 [ 44864/244490 ( 18%)], Train Loss: 1.88155\n",
      "Epoch: 02 [ 51264/244490 ( 21%)], Train Loss: 1.86939\n",
      "Epoch: 02 [ 57664/244490 ( 24%)], Train Loss: 1.85671\n",
      "Epoch: 02 [ 64064/244490 ( 26%)], Train Loss: 1.84328\n",
      "Epoch: 02 [ 70464/244490 ( 29%)], Train Loss: 1.83149\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">'-'</span>*<span style=\"color: #0000ff; text-decoration-color: #0000ff\">50</span>)                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f'FOLD: {</span>fold<span style=\"color: #808000; text-decoration-color: #808000\">}'</span>)                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">'-'</span>*<span style=\"color: #0000ff; text-decoration-color: #0000ff\">50</span>)                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>8 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>run(train, fold)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">9 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">run</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">18</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Train</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.cuda.synchronize()                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>tic1 = time.time()                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>18 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>result_dict = trainer.train(                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>args, train_dataloader,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>epoch, result_dict                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">44</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">41 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> amp.scale_loss(loss, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.optimizer) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> scaled_loss:                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">42 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>scaled_loss.backward()                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">43 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>44 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>loss.backward()                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">46 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>count += labels.size(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">47 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>losses.update(loss.item(), input_ids.size(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>))                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">488</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 485 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 486 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 487 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 488 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 489 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 490 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 491 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.8/dist-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">197</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">194 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The reason we repeat same the comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">195 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">196 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>197 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the bac</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">198 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>tensors, grad_tensors_, retain_graph, create_graph, inputs,                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">199 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>allow_unreachable=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, accumulate_grad=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to ru</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">200 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m8\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m'\u001b[0m\u001b[33m-\u001b[0m\u001b[33m'\u001b[0m*\u001b[94m50\u001b[0m)                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m6 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33mFOLD: \u001b[0m\u001b[33m{\u001b[0mfold\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m)                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m7 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m'\u001b[0m\u001b[33m-\u001b[0m\u001b[33m'\u001b[0m*\u001b[94m50\u001b[0m)                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m8 \u001b[2m│   \u001b[0mrun(train, fold)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m9 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mrun\u001b[0m:\u001b[94m18\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Train\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2m│   │   \u001b[0mtorch.cuda.synchronize()                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   │   \u001b[0mtic1 = time.time()                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m18 \u001b[2m│   │   \u001b[0mresult_dict = trainer.train(                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   │   │   \u001b[0margs, train_dataloader,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   │   │   \u001b[0mepoch, result_dict                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mtrain\u001b[0m:\u001b[94m44\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m41 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m amp.scale_loss(loss, \u001b[96mself\u001b[0m.optimizer) \u001b[94mas\u001b[0m scaled_loss:                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m42 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mscaled_loss.backward()                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m43 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m44 \u001b[2m│   │   │   │   \u001b[0mloss.backward()                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m45 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m46 \u001b[0m\u001b[2m│   │   │   \u001b[0mcount += labels.size(\u001b[94m0\u001b[0m)                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m47 \u001b[0m\u001b[2m│   │   │   \u001b[0mlosses.update(loss.item(), input_ids.size(\u001b[94m0\u001b[0m))                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m488\u001b[0m in \u001b[92mbackward\u001b[0m                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 485 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 486 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 487 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 488 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 489 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 490 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 491 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m197\u001b[0m in \u001b[92mbackward\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m194 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat same the comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m195 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m196 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m197 \u001b[2m│   \u001b[0mVariable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to run the bac\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m198 \u001b[0m\u001b[2m│   │   \u001b[0mtensors, grad_tensors_, retain_graph, create_graph, inputs,                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   │   \u001b[0mallow_unreachable=\u001b[94mTrue\u001b[0m, accumulate_grad=\u001b[94mTrue\u001b[0m)  \u001b[2m# Calls into the C++ engine to ru\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "for fold in range(5):\n",
    "    print();print()\n",
    "    print('-'*50)\n",
    "    print(f'FOLD: {fold}')\n",
    "    print('-'*50)\n",
    "    run(train, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00811b55-ad08-462c-8f66-e5ed01f62f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee9b0dc-148e-45b5-ac27-a59ac383b706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "loader, optimizer, model, loss_fn = ...\n",
    "swa_start = 5\n",
    "swa_model = AveragedModel(model)\n",
    "swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "for epoch in range(100):\n",
    "      for input, target in loader:\n",
    "          optimizer.zero_grad()\n",
    "          loss_fn(model(input), target).backward()\n",
    "          optimizer.step()\n",
    "      if epoch > swa_start:\n",
    "          swa_model.update_parameters(model)\n",
    "          swa_scheduler.step()\n",
    "      else:\n",
    "          scheduler.step()\n",
    "\n",
    "# Update bn statistics for the swa_model at the end\n",
    "torch.optim.swa_utils.update_bn(loader, swa_model)\n",
    "# Use swa_model to make predictions on test data \n",
    "preds = swa_model(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3496ae9b-4ef9-4a30-9626-7cf7688878d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b120f2b-c80a-4b36-ab66-5c15cebead25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from lit_nlp.api import dataset as lit_dataset\n",
    "from lit_nlp.api import types as lit_types\n",
    "from lit_nlp.api import model as lit_model\n",
    "from lit_nlp.lib import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853147b6-58c6-4aa4-b4ac-12431a82e7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download snapshots\n",
    "! conda install -y gdown\n",
    "!gdown --id 1-RO8zoPGuX4HI1KsvH_Urjg6XxDf-Lq0\n",
    "!gdown --id 1-Xcg0lBn6yehLkQnzadrWoCdLg9IGH3-\n",
    "!gdown --id 1-UbtAiZsgCgo0SvnNa9uLzO6coPsODv7\n",
    "!gdown --id 1-Qm2BYi-STYXfJ6DAK1yb88T7RBMerCh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485eaf73-f538-4ed0-8272-03b8fefbbe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitData(lit_dataset.Dataset):\n",
    "    def __init__(self, df, fold, split='val'):\n",
    "        self._examples = self.load_datapoints(df, fold, split)\n",
    "    \n",
    "    def load_datapoints(self, df, fold, split):\n",
    "        if split == 'val':\n",
    "            df = df[df['kfold']==fold].reset_index()\n",
    "        else:\n",
    "            df = df[df['kfold']!=fold].reset_index()\n",
    "        return [{\n",
    "            \"excerpt\": row[\"excerpt\"],\n",
    "            \"label\": row[\"target\"],\n",
    "        } for _, row in df.iterrows()]\n",
    "\n",
    "    def spec(self):\n",
    "        return {\n",
    "            'excerpt': lit_types.TextSegment(),\n",
    "            'label': lit_types.RegressionScore(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd01a46-a54c-4cc4-8506-b4165122c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonLitModel(lit_model.Model):\n",
    "    compute_grads = False\n",
    "    def __init__(self, args):\n",
    "        self.model, self.config, self.tokenizer = make_model(args, output_attentions=True)\n",
    "        self.model.eval()\n",
    "\n",
    "    def max_minibatch_size(self):\n",
    "        return 8\n",
    "\n",
    "    def predict_minibatch(self, inputs):\n",
    "        encoded_input = self.tokenizer.batch_encode_plus(\n",
    "            [ex[\"excerpt\"].replace(\"\\n\", \"\") for ex in inputs],\n",
    "            add_special_tokens=True,\n",
    "            max_length=256,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        encoded_input = {\n",
    "            key : torch.tensor(value, dtype=torch.long) for key, value in encoded_input.items()\n",
    "        }\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "            for tensor in encoded_input:\n",
    "                encoded_input[tensor] = encoded_input[tensor].cuda()\n",
    "    \n",
    "        with torch.set_grad_enabled(self.compute_grads):\n",
    "            outputs = self.model(encoded_input['input_ids'], encoded_input['attention_mask'])\n",
    "            if self.model.config.output_attentions:\n",
    "                logits, hidden_states, output_attentions = outputs[0], outputs[1], outputs[2]\n",
    "            else:\n",
    "                logits, hidden_states = outputs[0], outputs[1]\n",
    "\n",
    "        batched_outputs = {\n",
    "            \"input_ids\": encoded_input[\"input_ids\"],\n",
    "            \"ntok\": torch.sum(encoded_input[\"attention_mask\"], dim=1),\n",
    "            \"cls_emb\": hidden_states[-1][:, 0],\n",
    "            \"score\": torch.squeeze(logits, dim=-1)\n",
    "        }\n",
    "        \n",
    "        if self.model.config.output_attentions:\n",
    "            assert len(output_attentions) == self.model.config.num_hidden_layers\n",
    "            for i, layer_attention in enumerate(output_attentions[-2:]):\n",
    "                batched_outputs[f\"layer_{i}/attention\"] = layer_attention\n",
    "\n",
    "        if self.compute_grads:\n",
    "            scalar_pred_for_gradients = batched_outputs[\"score\"]\n",
    "            batched_outputs[\"input_emb_grad\"] = torch.autograd.grad(\n",
    "                scalar_pred_for_gradients,\n",
    "                hidden_states[0],\n",
    "                grad_outputs=torch.ones_like(scalar_pred_for_gradients)\n",
    "            )[0]\n",
    "\n",
    "        detached_outputs = {k: v.cpu().numpy() for k, v in batched_outputs.items()}\n",
    "        for output in utils.unbatch_preds(detached_outputs):\n",
    "            ntok = output.pop(\"ntok\")\n",
    "            output[\"tokens\"] = self.tokenizer.convert_ids_to_tokens(\n",
    "                output.pop(\"input_ids\")[1:ntok - 1]\n",
    "            )\n",
    "            if self.compute_grads:\n",
    "                output[\"token_grad_sentence\"] = output[\"input_emb_grad\"][:ntok]\n",
    "            if self.model.config.output_attentions:\n",
    "                for key in output:\n",
    "                    if not re.match(r\"layer_(\\d+)/attention\", key):\n",
    "                        continue\n",
    "                    output[key] = output[key][:, :ntok, :ntok].transpose((0, 2, 1))\n",
    "                    output[key] = output[key].copy()\n",
    "            yield output\n",
    "\n",
    "    def input_spec(self) -> lit_types.Spec:\n",
    "        return {\n",
    "            \"excerpt\": lit_types.TextSegment(),\n",
    "            \"label\": lit_types.RegressionScore()\n",
    "        }\n",
    "\n",
    "    def output_spec(self) -> lit_types.Spec:\n",
    "        ret = {\n",
    "            \"tokens\": lit_types.Tokens(),\n",
    "            \"score\": lit_types.RegressionScore(parent=\"label\"),\n",
    "            \"cls_emb\": lit_types.Embeddings()\n",
    "        }\n",
    "        if self.compute_grads:\n",
    "            ret[\"token_grad_sentence\"] = lit_types.TokenGradients(\n",
    "                align=\"tokens\"\n",
    "            )\n",
    "        if self.model.config.output_attentions:\n",
    "            for i in range(2): # self.model.config.num_hidden_layers\n",
    "                ret[f\"layer_{i}/attention\"] = lit_types.AttentionHeads(\n",
    "                    align_in=\"tokens\", align_out=\"tokens\")\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c0d491-3b9c-4372-8ee7-7212a31f8c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(path):\n",
    "    args = Config()\n",
    "    args.config_name = path\n",
    "    args.model_name_or_path = path\n",
    "    args.tokenizer_name = path\n",
    "    return CommonLitModel(args)\n",
    "\n",
    "datasets = {\n",
    "    'validation_0':CommonLitData(train, fold=0, split='val'),\n",
    "    'validation_1':CommonLitData(train, fold=1, split='val'),\n",
    "    'validation_2':CommonLitData(train, fold=2, split='val'),\n",
    "    'validation_3':CommonLitData(train, fold=3, split='val'),\n",
    "    'validation_4':CommonLitData(train, fold=4, split='val'),\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'model_0':create_model('output/checkpoint-fold-0/'),\n",
    "    'model_1':create_model('output/checkpoint-fold-1/'),\n",
    "    'model_2':create_model('output/checkpoint-fold-2/'),\n",
    "    'model_3':create_model('output/checkpoint-fold-3/'),\n",
    "    'model_4':create_model('output/checkpoint-fold-4/'),\n",
    "}\n",
    "\n",
    "\n",
    "from lit_nlp import notebook\n",
    "widget = notebook.LitWidget(models, datasets, height=800)\n",
    "# widget.render() -->> uncomment this line to render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d314fc99-d452-44c9-a921-25e80a99cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, model_config, \n",
    "optimizer = make_optimizer(args, model)\n",
    "scheduler = make_scheduler(args, optimizer, 1, 10)\n",
    "swa_scheduler = SWALR(optimizer, swa_lr=1e-6, anneal_epochs=3, anneal_strategy='cos')\n",
    "swa_start = 7\n",
    "for epoch in range(10):\n",
    "    optimizer.step()\n",
    "    if (epoch+1) >= swa_start:\n",
    "        print(\"starting swa\", i)\n",
    "        swa_scheduler.step()\n",
    "\n",
    "    if (epoch+1) < swa_start:\n",
    "        print('using simple scheduler')\n",
    "        scheduler.step()\n",
    "    print(optimizer.param_groups[0]['lr'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
