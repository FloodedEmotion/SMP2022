{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aac41d4-e39d-48d8-bf23-1ac5f7fbf804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import plotly.express as px\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from joblib import Parallel, delayed\n",
    "import shutil\n",
    "import glob\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error\n",
    "np.random.seed(0)\n",
    "import os\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from scipy.stats import norm, skew\n",
    "# train\n",
    "\n",
    "all_data = pd.read_csv('../data/feature_data_530.csv')\n",
    "# glove\n",
    "glove_tags = pd.read_csv('../data/alltags_feature.csv')\n",
    "glove_title = pd.read_csv('../data/title_feature.csv')\n",
    "# glove_title = pd.read_csv('../data/title_feature.csv')\n",
    "all_data = pd.concat([all_data, glove_tags, glove_title], axis=1)\n",
    "columns = ['Title_len', 'Title_number', 'Alltags_len', 'Alltags_number', 'photo_count', 'totalTags', 'totalGeotagged', 'totalFaves',\n",
    "          'totalInGroup','photoCount','meanView', 'meanTags', 'meanFaves', 'followerCount','followingCount']\n",
    "skew_features = all_data[columns].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "high_skew = skew_features[abs(skew_features) > 0.75]\n",
    "skew_index = high_skew.index\n",
    "for i in skew_index:\n",
    "    all_data[i] = np.log1p(all_data[i])\n",
    "    \n",
    "useless_columns = ['Pid','mean_label'] \n",
    "useless_columns += ['user_fe_{}'.format(i) for i in range(399)]\n",
    "useless_columns += ['loc_fe_{}'.format(i) for i in range(400)]\n",
    "all_data = all_data.drop(useless_columns, axis=1)\n",
    "\n",
    "\n",
    "train_all_data = all_data[all_data['train_type'] != -1]\n",
    "submit_all_data = all_data[all_data['train_type'] == -1]\n",
    "feature_columns = ['train_type','label']\n",
    "all_data = all_data.drop(feature_columns, axis=1)\n",
    "\n",
    "train_all_data = train_all_data.reset_index(drop=True)\n",
    "submit_all_data = submit_all_data.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "853639a4-1d4f-4d77-91ad-b88879271f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305613 180581 682\n",
      "305613 180581 682\n"
     ]
    }
   ],
   "source": [
    "train_label_df = train_all_data[['label']]\n",
    "train_feature_df = train_all_data.drop(feature_columns, axis=1)\n",
    "\n",
    "submit_label_df = submit_all_data[['label']]\n",
    "submit_feature_df = submit_all_data.drop(feature_columns, axis=1)\n",
    "\n",
    "print(len(train_feature_df), len(submit_feature_df), len(train_feature_df.columns))\n",
    "print(len(train_label_df), len(submit_label_df), len(train_feature_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde0a7a1-4883-4223-b58f-6d83e015f3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Consider everything as categorical variables might be useful : this is the only trick of this notebook\n",
    "# CAT_COLS = [c for c in df_train.columns if c.startswith(\"feature_\")] \n",
    "categories_columns = ['Uid', 'Category', 'Subcategory', 'Concept', 'Mediatype', 'hour', 'day', 'weekday', 'week_hour', 'year_weekday','Geoaccuracy', 'ispro' , 'Ispublic']\n",
    "# CAT_COLS = [c for c in df_train.columns if c.startswith(\"feature_\")] \n",
    "# NUM_COLS = [] \n",
    "\n",
    "# FEATURES = CAT_COLS + NUM_COLS\n",
    "CAT_COLS = [c for c in categories_columns] \n",
    "NUM_COLS = [c for c in all_data.columns if c not in CAT_COLS] \n",
    "# NUM_COLS = [] \n",
    "FEATURES = CAT_COLS + NUM_COLS\n",
    "\n",
    "encoders = {}\n",
    "for cat_col in CAT_COLS:\n",
    "    label_enc = LabelEncoder()\n",
    "        \n",
    "    train_feature_df[cat_col] = label_enc.fit_transform(train_feature_df[cat_col])\n",
    "    encoders[cat_col] = label_enc\n",
    "\n",
    "for cat_col in CAT_COLS:\n",
    "    label_enc = encoders[cat_col]\n",
    "    le_dict = dict(zip(label_enc.classes_, label_enc.transform(label_enc.classes_)))\n",
    "    # Replace unknown values by the most common value\n",
    "    # Changing this to another value might make more sense\n",
    "    if le_dict.get(\"low_frequency\") is not None:\n",
    "        default_val = le_dict[\"low_frequency\"]\n",
    "    else:\n",
    "        default_val = train_feature_df[cat_col].mode().values[0]\n",
    "    submit_feature_df[cat_col] = submit_feature_df[cat_col].apply(lambda x: le_dict.get(x, default_val ))\n",
    "    \n",
    "# Clip numerical features in test set to match training set\n",
    "for num_col in NUM_COLS:\n",
    "    submit_feature_df[num_col] = np.clip(submit_feature_df[num_col], train_feature_df[num_col].min(), train_feature_df[num_col].max())\n",
    "# for col in CAT_COLS:\n",
    "#     l_enc = LabelEncoder()\n",
    "#     train_feature_df[col] = l_enc.fit_transform(train_feature_df[col].values)\n",
    "#     categorical_dims[col] = len(l_enc.classes_)\n",
    "    \n",
    "# cat_idxs = [ i for i, f in enumerate(all_data.columns.tolist()) if f in categories_columns]\n",
    "\n",
    "# cat_dims = [ categorical_dims[f] for i, f in enumerate(all_data.columns.tolist()) if f in categories_columns] \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4747829-0844-4db2-8066-86d6044cfae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# + NUM_COLS\n",
    "# FEATURES = CAT_COLS \n",
    "cat_dims = train_feature_df[CAT_COLS].nunique().to_list()\n",
    "cat_idxs = [FEATURES.index(cat_col) for cat_col in CAT_COLS]\n",
    "cat_emb_dims = np.ceil(np.log(cat_dims)).astype(int).tolist()\n",
    "# cat_emb_dims = np.ceil(np.clip((np.array(cat_dims)) / 2, a_min=1, a_max=50)).astype(np.int).tolist()\n",
    "# cat_emb_dims=1\n",
    "\n",
    "X = train_feature_df[FEATURES].values\n",
    "y = train_label_df['label'].values\n",
    "\n",
    "X_test = submit_feature_df[FEATURES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d684f11f-e98f-44cd-b509-fcfbe410f371",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 9761.31812| val_0_unsup_loss_numpy: 435.54290771484375|  0:00:13s\n",
      "epoch 1  | loss: 3228.27576| val_0_unsup_loss_numpy: 239.22085571289062|  0:00:26s\n",
      "epoch 2  | loss: 1578.97198| val_0_unsup_loss_numpy: 42.75197982788086|  0:00:39s\n",
      "epoch 3  | loss: 1016.66782| val_0_unsup_loss_numpy: 27.963619232177734|  0:00:51s\n",
      "epoch 4  | loss: 602.55606| val_0_unsup_loss_numpy: 21.5297794342041|  0:01:08s\n",
      "epoch 5  | loss: 353.57324| val_0_unsup_loss_numpy: 17.94179916381836|  0:01:22s\n",
      "epoch 6  | loss: 204.92841| val_0_unsup_loss_numpy: 14.631319999694824|  0:01:37s\n",
      "epoch 7  | loss: 120.13941| val_0_unsup_loss_numpy: 13.140230178833008|  0:01:49s\n",
      "epoch 8  | loss: 96.00509| val_0_unsup_loss_numpy: 10.76550006866455|  0:02:02s\n",
      "epoch 9  | loss: 58.71519| val_0_unsup_loss_numpy: 8.765219688415527|  0:02:15s\n",
      "epoch 10 | loss: 38.74199| val_0_unsup_loss_numpy: 6.794950008392334|  0:02:27s\n",
      "epoch 11 | loss: 38.02461| val_0_unsup_loss_numpy: 5.68025016784668|  0:02:40s\n",
      "epoch 12 | loss: 26.5178 | val_0_unsup_loss_numpy: 4.77672004699707|  0:02:52s\n",
      "epoch 13 | loss: 20.23621| val_0_unsup_loss_numpy: 4.10522985458374|  0:03:05s\n",
      "epoch 14 | loss: 16.06538| val_0_unsup_loss_numpy: 3.3706700801849365|  0:03:18s\n",
      "epoch 15 | loss: 14.50688| val_0_unsup_loss_numpy: 3.1182498931884766|  0:03:31s\n",
      "epoch 16 | loss: 13.58986| val_0_unsup_loss_numpy: 2.9469499588012695|  0:03:43s\n",
      "epoch 17 | loss: 9.95461 | val_0_unsup_loss_numpy: 2.4436099529266357|  0:03:56s\n",
      "epoch 18 | loss: 9.56187 | val_0_unsup_loss_numpy: 2.283440113067627|  0:04:09s\n",
      "epoch 19 | loss: 7.31187 | val_0_unsup_loss_numpy: 2.1500298976898193|  0:04:21s\n",
      "epoch 20 | loss: 6.61673 | val_0_unsup_loss_numpy: 1.9998500347137451|  0:04:34s\n",
      "epoch 21 | loss: 5.41418 | val_0_unsup_loss_numpy: 1.9368200302124023|  0:04:47s\n",
      "epoch 22 | loss: 5.09224 | val_0_unsup_loss_numpy: 1.8578799962997437|  0:05:00s\n",
      "epoch 23 | loss: 4.76019 | val_0_unsup_loss_numpy: 1.793370008468628|  0:05:12s\n",
      "epoch 24 | loss: 5.50576 | val_0_unsup_loss_numpy: 1.797070026397705|  0:05:26s\n",
      "epoch 25 | loss: 4.61267 | val_0_unsup_loss_numpy: 1.703160047531128|  0:05:38s\n",
      "epoch 26 | loss: 3.99052 | val_0_unsup_loss_numpy: 1.573240041732788|  0:05:51s\n",
      "epoch 27 | loss: 4.3887  | val_0_unsup_loss_numpy: 1.651859998703003|  0:06:04s\n",
      "epoch 28 | loss: 4.3059  | val_0_unsup_loss_numpy: 1.6370099782943726|  0:06:17s\n",
      "epoch 29 | loss: 3.83321 | val_0_unsup_loss_numpy: 1.5367399454116821|  0:06:29s\n",
      "epoch 30 | loss: 4.73127 | val_0_unsup_loss_numpy: 1.4778000116348267|  0:06:42s\n",
      "epoch 31 | loss: 3.76517 | val_0_unsup_loss_numpy: 1.4583699703216553|  0:06:55s\n",
      "epoch 32 | loss: 3.77941 | val_0_unsup_loss_numpy: 1.4033199548721313|  0:07:07s\n",
      "epoch 33 | loss: 3.80748 | val_0_unsup_loss_numpy: 1.4352600574493408|  0:07:20s\n",
      "epoch 34 | loss: 3.68717 | val_0_unsup_loss_numpy: 1.4536800384521484|  0:07:33s\n",
      "epoch 35 | loss: 3.09373 | val_0_unsup_loss_numpy: 1.382040023803711|  0:07:45s\n",
      "epoch 36 | loss: 2.70332 | val_0_unsup_loss_numpy: 1.333549976348877|  0:07:58s\n",
      "epoch 37 | loss: 3.09229 | val_0_unsup_loss_numpy: 1.3794200420379639|  0:08:11s\n",
      "epoch 38 | loss: 2.74897 | val_0_unsup_loss_numpy: 1.3566399812698364|  0:08:24s\n",
      "epoch 39 | loss: 2.53058 | val_0_unsup_loss_numpy: 1.3421399593353271|  0:08:37s\n",
      "epoch 40 | loss: 2.91934 | val_0_unsup_loss_numpy: 1.3411200046539307|  0:08:49s\n",
      "epoch 41 | loss: 3.64498 | val_0_unsup_loss_numpy: 1.30964994430542|  0:09:02s\n",
      "epoch 42 | loss: 2.28706 | val_0_unsup_loss_numpy: 1.3097200393676758|  0:09:14s\n",
      "epoch 43 | loss: 2.82091 | val_0_unsup_loss_numpy: 1.329110026359558|  0:09:27s\n",
      "epoch 44 | loss: 2.16735 | val_0_unsup_loss_numpy: 1.2988200187683105|  0:09:39s\n",
      "epoch 45 | loss: 2.40246 | val_0_unsup_loss_numpy: 1.2888100147247314|  0:09:54s\n",
      "epoch 46 | loss: 2.14184 | val_0_unsup_loss_numpy: 1.285770058631897|  0:10:08s\n",
      "epoch 47 | loss: 1.88538 | val_0_unsup_loss_numpy: 1.2892500162124634|  0:10:20s\n",
      "epoch 48 | loss: 1.90982 | val_0_unsup_loss_numpy: 1.2964099645614624|  0:10:33s\n",
      "epoch 49 | loss: 1.86183 | val_0_unsup_loss_numpy: 1.2757200002670288|  0:10:46s\n",
      "epoch 50 | loss: 1.83859 | val_0_unsup_loss_numpy: 1.2655500173568726|  0:10:59s\n",
      "epoch 51 | loss: 1.85381 | val_0_unsup_loss_numpy: 1.2683099508285522|  0:11:12s\n",
      "epoch 52 | loss: 1.69889 | val_0_unsup_loss_numpy: 1.2700300216674805|  0:11:25s\n",
      "epoch 53 | loss: 1.69079 | val_0_unsup_loss_numpy: 1.2662700414657593|  0:11:38s\n",
      "epoch 54 | loss: 1.93556 | val_0_unsup_loss_numpy: 1.256809949874878|  0:11:51s\n",
      "epoch 55 | loss: 1.69207 | val_0_unsup_loss_numpy: 1.2553199529647827|  0:12:03s\n",
      "epoch 56 | loss: 1.70934 | val_0_unsup_loss_numpy: 1.2557300329208374|  0:12:16s\n",
      "epoch 57 | loss: 1.6204  | val_0_unsup_loss_numpy: 1.2629300355911255|  0:12:29s\n",
      "epoch 58 | loss: 1.70966 | val_0_unsup_loss_numpy: 1.2453900575637817|  0:12:42s\n",
      "epoch 59 | loss: 1.76646 | val_0_unsup_loss_numpy: 1.2459100484848022|  0:12:54s\n",
      "epoch 60 | loss: 1.71051 | val_0_unsup_loss_numpy: 1.2515000104904175|  0:13:07s\n",
      "epoch 61 | loss: 1.60768 | val_0_unsup_loss_numpy: 1.2474199533462524|  0:13:20s\n",
      "epoch 62 | loss: 1.79272 | val_0_unsup_loss_numpy: 1.2390999794006348|  0:13:33s\n",
      "epoch 63 | loss: 1.79705 | val_0_unsup_loss_numpy: 1.2393800020217896|  0:13:45s\n",
      "epoch 64 | loss: 1.70675 | val_0_unsup_loss_numpy: 1.2456400394439697|  0:13:58s\n",
      "epoch 65 | loss: 1.5788  | val_0_unsup_loss_numpy: 1.2395100593566895|  0:14:11s\n",
      "epoch 66 | loss: 1.69075 | val_0_unsup_loss_numpy: 1.2401299476623535|  0:14:23s\n",
      "epoch 67 | loss: 1.48123 | val_0_unsup_loss_numpy: 1.2337599992752075|  0:14:36s\n",
      "epoch 68 | loss: 1.86487 | val_0_unsup_loss_numpy: 1.231510043144226|  0:14:49s\n",
      "epoch 69 | loss: 1.65384 | val_0_unsup_loss_numpy: 1.2309399843215942|  0:15:01s\n",
      "epoch 70 | loss: 1.60514 | val_0_unsup_loss_numpy: 1.232759952545166|  0:15:14s\n",
      "epoch 71 | loss: 1.50888 | val_0_unsup_loss_numpy: 1.2310700416564941|  0:15:27s\n",
      "epoch 72 | loss: 1.41092 | val_0_unsup_loss_numpy: 1.229140043258667|  0:15:39s\n",
      "epoch 73 | loss: 1.39904 | val_0_unsup_loss_numpy: 1.2276999950408936|  0:15:52s\n",
      "epoch 74 | loss: 1.36083 | val_0_unsup_loss_numpy: 1.2272299528121948|  0:16:05s\n",
      "epoch 75 | loss: 1.48607 | val_0_unsup_loss_numpy: 1.226040005683899|  0:16:18s\n",
      "epoch 76 | loss: 1.51481 | val_0_unsup_loss_numpy: 1.2252700328826904|  0:16:31s\n",
      "epoch 77 | loss: 1.37841 | val_0_unsup_loss_numpy: 1.223039984703064|  0:16:43s\n",
      "epoch 78 | loss: 1.32957 | val_0_unsup_loss_numpy: 1.222659945487976|  0:16:56s\n",
      "epoch 79 | loss: 1.33041 | val_0_unsup_loss_numpy: 1.2224700450897217|  0:17:08s\n",
      "epoch 80 | loss: 1.37462 | val_0_unsup_loss_numpy: 1.2201800346374512|  0:17:21s\n",
      "epoch 81 | loss: 1.34723 | val_0_unsup_loss_numpy: 1.2211699485778809|  0:17:33s\n",
      "epoch 82 | loss: 1.42522 | val_0_unsup_loss_numpy: 1.2210500240325928|  0:17:46s\n",
      "epoch 83 | loss: 1.35971 | val_0_unsup_loss_numpy: 1.2191699743270874|  0:17:59s\n",
      "epoch 84 | loss: 1.31388 | val_0_unsup_loss_numpy: 1.2174299955368042|  0:18:12s\n",
      "epoch 85 | loss: 1.36313 | val_0_unsup_loss_numpy: 1.218500018119812|  0:18:24s\n",
      "epoch 86 | loss: 1.35837 | val_0_unsup_loss_numpy: 1.2161600589752197|  0:18:37s\n",
      "epoch 87 | loss: 1.33743 | val_0_unsup_loss_numpy: 1.215000033378601|  0:18:51s\n",
      "epoch 88 | loss: 1.41503 | val_0_unsup_loss_numpy: 1.2132999897003174|  0:19:06s\n",
      "epoch 89 | loss: 1.43936 | val_0_unsup_loss_numpy: 1.2145500183105469|  0:19:19s\n",
      "epoch 90 | loss: 1.67161 | val_0_unsup_loss_numpy: 1.2148499488830566|  0:19:35s\n",
      "epoch 91 | loss: 1.34815 | val_0_unsup_loss_numpy: 1.2145400047302246|  0:19:47s\n",
      "epoch 92 | loss: 1.40262 | val_0_unsup_loss_numpy: 1.2101399898529053|  0:20:01s\n",
      "epoch 93 | loss: 1.3171  | val_0_unsup_loss_numpy: 1.2089500427246094|  0:20:13s\n",
      "epoch 94 | loss: 1.36587 | val_0_unsup_loss_numpy: 1.210960030555725|  0:20:26s\n",
      "epoch 95 | loss: 1.51732 | val_0_unsup_loss_numpy: 1.2121399641036987|  0:20:39s\n",
      "epoch 96 | loss: 1.37742 | val_0_unsup_loss_numpy: 1.2097400426864624|  0:20:52s\n",
      "epoch 97 | loss: 1.37484 | val_0_unsup_loss_numpy: 1.2089099884033203|  0:21:05s\n",
      "epoch 98 | loss: 1.36179 | val_0_unsup_loss_numpy: 1.2089099884033203|  0:21:17s\n",
      "epoch 99 | loss: 1.27289 | val_0_unsup_loss_numpy: 1.2096600532531738|  0:21:30s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_val_0_unsup_loss_numpy = 1.2089099884033203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\n",
    "N_D = 64 #64 # 32\n",
    "N_A = 64 # 32\n",
    "N_INDEP = 1 #2\n",
    "N_SHARED = 1 #2\n",
    "N_STEPS = 3 #2\n",
    "MASK_TYPE = \"sparsemax\"\n",
    "GAMMA = 1.2\n",
    "BS = 85600\n",
    "MAX_EPOCH =  100\n",
    "PRETRAIN = True\n",
    "\n",
    "\n",
    "if PRETRAIN:\n",
    "    pretrain_params = dict(n_d=N_D, n_a=N_A, n_steps=N_STEPS,  #0.2,\n",
    "                           n_independent=N_INDEP, n_shared=N_SHARED,\n",
    "                           # device = 'gpu',\n",
    "                           device_name = 'cuda',\n",
    "                           cat_idxs=cat_idxs,\n",
    "                           cat_dims=cat_dims,\n",
    "                           cat_emb_dim=cat_emb_dims,\n",
    "                           gamma=GAMMA,\n",
    "                           lambda_sparse=0., optimizer_fn=torch.optim.Adam,\n",
    "                           optimizer_params=dict(lr=2e-2),\n",
    "                           mask_type=MASK_TYPE,\n",
    "                           scheduler_params=dict(mode=\"min\",\n",
    "                                                 patience=3,\n",
    "                                                 min_lr=1e-5,\n",
    "                                                 factor=0.5,),\n",
    "                           scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,                         \n",
    "                           verbose=1,\n",
    "                          )\n",
    "\n",
    "    pretrainer = TabNetPretrainer(**pretrain_params)\n",
    "\n",
    "    pretrainer.fit(X_train=X_test, \n",
    "                   eval_set=[X],\n",
    "                   max_epochs=MAX_EPOCH,\n",
    "                   patience=25, batch_size=BS, virtual_batch_size=BS, #128,\n",
    "                   num_workers=1, drop_last=True,\n",
    "                   pretraining_ratio=0.5 # The bigger your pretraining_ratio the harder it is to reconstruct\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d9a7e8-acbb-4e73-92cc-ebef8b0413bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/abstract_model.py:231: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 67.83841| train_mae: 3.20218 | valid_mae: 3.21709 |  0:00:07s\n",
      "epoch 1  | loss: 11.18268| train_mae: 2.2692  | valid_mae: 2.26563 |  0:00:13s\n",
      "epoch 2  | loss: 7.17016 | train_mae: 1.96228 | valid_mae: 1.96235 |  0:00:20s\n",
      "epoch 3  | loss: 5.97309 | train_mae: 1.84325 | valid_mae: 1.83898 |  0:00:27s\n",
      "epoch 4  | loss: 5.33999 | train_mae: 2.02426 | valid_mae: 2.02601 |  0:00:34s\n",
      "epoch 5  | loss: 4.60857 | train_mae: 1.8602  | valid_mae: 1.86103 |  0:00:41s\n",
      "epoch 6  | loss: 4.54146 | train_mae: 2.16754 | valid_mae: 2.15888 |  0:00:47s\n",
      "epoch 7  | loss: 4.87237 | train_mae: 2.02811 | valid_mae: 2.02132 |  0:00:54s\n",
      "epoch 8  | loss: 4.15161 | train_mae: 1.79249 | valid_mae: 1.79064 |  0:01:01s\n",
      "epoch 9  | loss: 4.10211 | train_mae: 2.12655 | valid_mae: 2.1242  |  0:01:08s\n",
      "epoch 10 | loss: 3.75107 | train_mae: 2.59462 | valid_mae: 2.59278 |  0:01:15s\n",
      "epoch 11 | loss: 3.80192 | train_mae: 2.09271 | valid_mae: 2.09426 |  0:01:21s\n",
      "epoch 12 | loss: 3.43539 | train_mae: 1.68802 | valid_mae: 1.69517 |  0:01:28s\n",
      "epoch 13 | loss: 3.7147  | train_mae: 1.81087 | valid_mae: 1.81775 |  0:01:35s\n",
      "epoch 14 | loss: 3.10105 | train_mae: 2.34639 | valid_mae: 2.35173 |  0:01:42s\n",
      "epoch 15 | loss: 2.8761  | train_mae: 1.95649 | valid_mae: 1.96556 |  0:01:49s\n",
      "epoch 16 | loss: 2.53273 | train_mae: 1.96803 | valid_mae: 1.98291 |  0:01:56s\n",
      "epoch 17 | loss: 2.28349 | train_mae: 1.94441 | valid_mae: 1.96267 |  0:02:03s\n",
      "epoch 18 | loss: 2.12085 | train_mae: 1.96083 | valid_mae: 1.98066 |  0:02:09s\n",
      "epoch 19 | loss: 1.94606 | train_mae: 1.816   | valid_mae: 1.84118 |  0:02:16s\n",
      "epoch 20 | loss: 1.79489 | train_mae: 1.86836 | valid_mae: 1.89586 |  0:02:23s\n",
      "epoch 21 | loss: 1.69121 | train_mae: 1.77478 | valid_mae: 1.80433 |  0:02:30s\n",
      "epoch 22 | loss: 1.59742 | train_mae: 1.74625 | valid_mae: 1.77711 |  0:02:37s\n",
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_valid_mae = 1.69517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 4.6632, MAE: 1.6952, SRC: 0.5214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/abstract_model.py:231: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 67.3116 | train_mae: 3.53936 | valid_mae: 3.53779 |  0:00:06s\n",
      "epoch 1  | loss: 11.00432| train_mae: 2.38244 | valid_mae: 2.38545 |  0:00:13s\n",
      "epoch 2  | loss: 7.07907 | train_mae: 2.06369 | valid_mae: 2.06375 |  0:00:20s\n",
      "epoch 3  | loss: 6.28045 | train_mae: 1.95002 | valid_mae: 1.95237 |  0:00:27s\n",
      "epoch 4  | loss: 6.38232 | train_mae: 1.84318 | valid_mae: 1.84882 |  0:00:34s\n",
      "epoch 5  | loss: 5.96901 | train_mae: 2.70183 | valid_mae: 2.70496 |  0:00:41s\n",
      "epoch 6  | loss: 7.41409 | train_mae: 1.87729 | valid_mae: 1.88523 |  0:00:47s\n",
      "epoch 7  | loss: 5.58371 | train_mae: 2.01861 | valid_mae: 2.02211 |  0:00:54s\n",
      "epoch 8  | loss: 8.63267 | train_mae: 1.89587 | valid_mae: 1.89541 |  0:01:01s\n",
      "epoch 9  | loss: 5.49021 | train_mae: 2.48204 | valid_mae: 2.47835 |  0:01:08s\n",
      "epoch 10 | loss: 4.24653 | train_mae: 2.32368 | valid_mae: 2.3259  |  0:01:15s\n",
      "epoch 11 | loss: 4.1698  | train_mae: 1.72278 | valid_mae: 1.72645 |  0:01:22s\n",
      "epoch 12 | loss: 3.80027 | train_mae: 1.70526 | valid_mae: 1.70891 |  0:01:28s\n",
      "epoch 13 | loss: 3.51256 | train_mae: 2.2769  | valid_mae: 2.28501 |  0:01:35s\n",
      "epoch 14 | loss: 3.34809 | train_mae: 2.325   | valid_mae: 2.33614 |  0:01:42s\n",
      "epoch 15 | loss: 3.06594 | train_mae: 1.85214 | valid_mae: 1.86173 |  0:01:49s\n",
      "epoch 16 | loss: 2.80996 | train_mae: 2.0533  | valid_mae: 2.06748 |  0:01:56s\n",
      "epoch 17 | loss: 2.68928 | train_mae: 1.98043 | valid_mae: 1.99529 |  0:02:03s\n",
      "epoch 18 | loss: 2.52802 | train_mae: 1.64481 | valid_mae: 1.65901 |  0:02:09s\n",
      "epoch 19 | loss: 2.43208 | train_mae: 1.5695  | valid_mae: 1.58539 |  0:02:16s\n",
      "epoch 20 | loss: 2.33084 | train_mae: 1.73298 | valid_mae: 1.75484 |  0:02:23s\n",
      "epoch 21 | loss: 2.16966 | train_mae: 1.89014 | valid_mae: 1.91467 |  0:02:30s\n",
      "epoch 22 | loss: 2.08668 | train_mae: 1.67388 | valid_mae: 1.6971  |  0:02:37s\n",
      "epoch 23 | loss: 1.95536 | train_mae: 1.42023 | valid_mae: 1.44499 |  0:02:45s\n",
      "epoch 24 | loss: 1.96569 | train_mae: 1.45986 | valid_mae: 1.48803 |  0:02:55s\n",
      "epoch 25 | loss: 1.80165 | train_mae: 1.69274 | valid_mae: 1.72546 |  0:03:02s\n",
      "epoch 26 | loss: 1.7057  | train_mae: 1.58652 | valid_mae: 1.61949 |  0:03:09s\n",
      "epoch 27 | loss: 1.61094 | train_mae: 1.36083 | valid_mae: 1.39557 |  0:03:16s\n",
      "epoch 28 | loss: 1.5385  | train_mae: 1.4766  | valid_mae: 1.51425 |  0:03:23s\n",
      "epoch 29 | loss: 1.45525 | train_mae: 1.43861 | valid_mae: 1.48112 |  0:03:29s\n",
      "epoch 30 | loss: 1.36546 | train_mae: 1.31388 | valid_mae: 1.36112 |  0:03:36s\n",
      "epoch 31 | loss: 1.30667 | train_mae: 1.36582 | valid_mae: 1.41577 |  0:03:43s\n",
      "epoch 32 | loss: 1.23671 | train_mae: 1.31374 | valid_mae: 1.36522 |  0:03:50s\n",
      "epoch 33 | loss: 1.1725  | train_mae: 1.33473 | valid_mae: 1.38929 |  0:03:57s\n",
      "epoch 34 | loss: 1.11053 | train_mae: 1.23776 | valid_mae: 1.29283 |  0:04:03s\n",
      "epoch 35 | loss: 1.05517 | train_mae: 1.29345 | valid_mae: 1.35126 |  0:04:10s\n",
      "epoch 36 | loss: 0.99248 | train_mae: 1.20224 | valid_mae: 1.26345 |  0:04:17s\n",
      "epoch 37 | loss: 0.9562  | train_mae: 1.21569 | valid_mae: 1.28433 |  0:04:24s\n",
      "epoch 38 | loss: 0.90668 | train_mae: 1.1333  | valid_mae: 1.20323 |  0:04:31s\n",
      "epoch 39 | loss: 0.858   | train_mae: 1.15696 | valid_mae: 1.22734 |  0:04:38s\n",
      "epoch 40 | loss: 0.82243 | train_mae: 1.11058 | valid_mae: 1.18238 |  0:04:45s\n",
      "epoch 41 | loss: 0.79246 | train_mae: 1.10031 | valid_mae: 1.17431 |  0:04:51s\n",
      "epoch 42 | loss: 0.76183 | train_mae: 1.07082 | valid_mae: 1.1473  |  0:04:58s\n",
      "epoch 43 | loss: 0.74877 | train_mae: 1.06456 | valid_mae: 1.14158 |  0:05:05s\n",
      "epoch 44 | loss: 0.72488 | train_mae: 1.03597 | valid_mae: 1.11506 |  0:05:12s\n",
      "epoch 45 | loss: 0.69476 | train_mae: 1.00891 | valid_mae: 1.0879  |  0:05:19s\n",
      "epoch 46 | loss: 0.6775  | train_mae: 1.01524 | valid_mae: 1.09588 |  0:05:26s\n",
      "epoch 47 | loss: 0.6633  | train_mae: 0.99712 | valid_mae: 1.07649 |  0:05:33s\n",
      "epoch 48 | loss: 0.64843 | train_mae: 1.00229 | valid_mae: 1.08243 |  0:05:39s\n",
      "epoch 49 | loss: 0.63379 | train_mae: 0.98547 | valid_mae: 1.06661 |  0:05:46s\n",
      "epoch 50 | loss: 0.61635 | train_mae: 0.96682 | valid_mae: 1.05168 |  0:05:53s\n",
      "epoch 51 | loss: 0.59331 | train_mae: 0.93202 | valid_mae: 1.01836 |  0:06:00s\n",
      "epoch 52 | loss: 0.5776  | train_mae: 0.92036 | valid_mae: 1.00807 |  0:06:07s\n",
      "epoch 53 | loss: 0.56473 | train_mae: 0.92297 | valid_mae: 1.01275 |  0:06:14s\n",
      "epoch 54 | loss: 0.55368 | train_mae: 0.91212 | valid_mae: 1.00143 |  0:06:21s\n",
      "epoch 55 | loss: 0.53816 | train_mae: 0.90316 | valid_mae: 0.99268 |  0:06:28s\n",
      "epoch 56 | loss: 0.51961 | train_mae: 0.88323 | valid_mae: 0.97407 |  0:06:34s\n",
      "epoch 57 | loss: 0.51029 | train_mae: 0.86471 | valid_mae: 0.95344 |  0:06:41s\n",
      "epoch 58 | loss: 0.51756 | train_mae: 0.8732  | valid_mae: 0.96339 |  0:06:48s\n",
      "epoch 59 | loss: 0.49697 | train_mae: 0.84626 | valid_mae: 0.94049 |  0:06:55s\n",
      "epoch 60 | loss: 0.48753 | train_mae: 0.84225 | valid_mae: 0.93496 |  0:07:02s\n",
      "epoch 61 | loss: 0.47324 | train_mae: 0.84131 | valid_mae: 0.93449 |  0:07:09s\n",
      "epoch 62 | loss: 0.46535 | train_mae: 0.85236 | valid_mae: 0.94486 |  0:07:16s\n",
      "epoch 63 | loss: 0.45202 | train_mae: 0.78663 | valid_mae: 0.88315 |  0:07:23s\n",
      "epoch 64 | loss: 0.44611 | train_mae: 0.8259  | valid_mae: 0.92435 |  0:07:29s\n",
      "epoch 65 | loss: 0.4472  | train_mae: 0.76451 | valid_mae: 0.86289 |  0:07:36s\n",
      "epoch 66 | loss: 0.43809 | train_mae: 0.81371 | valid_mae: 0.91409 |  0:07:43s\n",
      "epoch 67 | loss: 0.42764 | train_mae: 0.75872 | valid_mae: 0.85894 |  0:07:50s\n",
      "epoch 68 | loss: 0.42403 | train_mae: 0.78358 | valid_mae: 0.8865  |  0:07:57s\n",
      "epoch 69 | loss: 0.42727 | train_mae: 0.76815 | valid_mae: 0.86731 |  0:08:03s\n",
      "epoch 70 | loss: 0.41959 | train_mae: 1.13352 | valid_mae: 1.19723 |  0:08:10s\n",
      "epoch 71 | loss: 0.41825 | train_mae: 0.73488 | valid_mae: 0.83812 |  0:08:17s\n",
      "epoch 72 | loss: 0.4132  | train_mae: 0.74538 | valid_mae: 0.85153 |  0:08:24s\n",
      "epoch 73 | loss: 0.40092 | train_mae: 0.74003 | valid_mae: 0.84578 |  0:08:31s\n",
      "epoch 74 | loss: 0.40113 | train_mae: 0.72582 | valid_mae: 0.8315  |  0:08:37s\n",
      "epoch 75 | loss: 0.39216 | train_mae: 0.74491 | valid_mae: 0.85011 |  0:08:44s\n",
      "epoch 76 | loss: 0.39153 | train_mae: 0.72896 | valid_mae: 0.83773 |  0:08:51s\n",
      "epoch 77 | loss: 0.37655 | train_mae: 0.72595 | valid_mae: 0.83336 |  0:08:58s\n",
      "epoch 78 | loss: 0.37876 | train_mae: 0.71807 | valid_mae: 0.82767 |  0:09:04s\n",
      "epoch 79 | loss: 0.36992 | train_mae: 0.71353 | valid_mae: 0.82148 |  0:09:11s\n",
      "epoch 80 | loss: 0.37197 | train_mae: 0.70019 | valid_mae: 0.81007 |  0:09:18s\n",
      "epoch 81 | loss: 0.36676 | train_mae: 0.69591 | valid_mae: 0.8065  |  0:09:25s\n",
      "epoch 82 | loss: 0.35913 | train_mae: 0.6992  | valid_mae: 0.81153 |  0:09:32s\n",
      "epoch 83 | loss: 0.35516 | train_mae: 0.67502 | valid_mae: 0.79052 |  0:09:39s\n",
      "epoch 84 | loss: 0.3546  | train_mae: 0.68071 | valid_mae: 0.79621 |  0:09:45s\n",
      "epoch 85 | loss: 0.34736 | train_mae: 0.68464 | valid_mae: 0.8038  |  0:09:52s\n",
      "epoch 86 | loss: 0.34607 | train_mae: 0.67343 | valid_mae: 0.79021 |  0:09:59s\n",
      "epoch 87 | loss: 0.34037 | train_mae: 0.6542  | valid_mae: 0.77451 |  0:10:06s\n",
      "epoch 88 | loss: 0.3342  | train_mae: 0.67041 | valid_mae: 0.789   |  0:10:13s\n",
      "epoch 89 | loss: 0.33444 | train_mae: 0.65768 | valid_mae: 0.77733 |  0:10:19s\n",
      "epoch 90 | loss: 0.33583 | train_mae: 0.64419 | valid_mae: 0.76448 |  0:10:26s\n",
      "epoch 91 | loss: 0.33066 | train_mae: 0.64421 | valid_mae: 0.76484 |  0:10:33s\n",
      "epoch 92 | loss: 0.32074 | train_mae: 0.64962 | valid_mae: 0.7733  |  0:10:40s\n",
      "epoch 93 | loss: 0.32652 | train_mae: 0.6303  | valid_mae: 0.75202 |  0:10:47s\n",
      "epoch 94 | loss: 0.32656 | train_mae: 0.64375 | valid_mae: 0.76757 |  0:10:54s\n",
      "epoch 95 | loss: 0.32591 | train_mae: 0.61683 | valid_mae: 0.74278 |  0:11:01s\n",
      "epoch 96 | loss: 0.32291 | train_mae: 0.62524 | valid_mae: 0.75307 |  0:11:08s\n",
      "epoch 97 | loss: 0.31902 | train_mae: 0.62945 | valid_mae: 0.75841 |  0:11:15s\n",
      "epoch 98 | loss: 0.30974 | train_mae: 0.60639 | valid_mae: 0.73505 |  0:11:25s\n",
      "epoch 99 | loss: 0.31149 | train_mae: 0.616   | valid_mae: 0.74677 |  0:11:32s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 98 and best_valid_mae = 0.73505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.1268, MAE: 0.7350, SRC: 0.9112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/abstract_model.py:231: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 65.8426 | train_mae: 3.02324 | valid_mae: 3.01568 |  0:00:06s\n",
      "epoch 1  | loss: 11.15848| train_mae: 2.20465 | valid_mae: 2.21254 |  0:00:13s\n",
      "epoch 2  | loss: 7.2657  | train_mae: 1.93352 | valid_mae: 1.93892 |  0:00:23s\n",
      "epoch 3  | loss: 5.81263 | train_mae: 1.98406 | valid_mae: 1.97649 |  0:00:30s\n",
      "epoch 4  | loss: 5.25038 | train_mae: 1.87929 | valid_mae: 1.87242 |  0:00:37s\n",
      "epoch 5  | loss: 4.50471 | train_mae: 2.09199 | valid_mae: 2.08674 |  0:00:43s\n",
      "epoch 6  | loss: 3.93789 | train_mae: 2.41807 | valid_mae: 2.4147  |  0:00:50s\n",
      "epoch 7  | loss: 3.58211 | train_mae: 2.18575 | valid_mae: 2.18228 |  0:00:57s\n",
      "epoch 8  | loss: 3.40522 | train_mae: 2.59608 | valid_mae: 2.58829 |  0:01:04s\n",
      "epoch 9  | loss: 3.03056 | train_mae: 2.04429 | valid_mae: 2.0535  |  0:01:10s\n",
      "epoch 10 | loss: 2.84695 | train_mae: 2.17895 | valid_mae: 2.19415 |  0:01:17s\n",
      "epoch 11 | loss: 2.70511 | train_mae: 1.74402 | valid_mae: 1.76291 |  0:01:24s\n",
      "epoch 12 | loss: 2.43612 | train_mae: 2.24966 | valid_mae: 2.26672 |  0:01:30s\n",
      "epoch 13 | loss: 2.36254 | train_mae: 2.17229 | valid_mae: 2.18975 |  0:01:37s\n",
      "epoch 14 | loss: 2.17396 | train_mae: 1.53802 | valid_mae: 1.56432 |  0:01:44s\n",
      "epoch 15 | loss: 2.1814  | train_mae: 1.77023 | valid_mae: 1.79882 |  0:01:50s\n",
      "epoch 16 | loss: 1.87686 | train_mae: 1.89624 | valid_mae: 1.92925 |  0:01:57s\n",
      "epoch 17 | loss: 1.71687 | train_mae: 1.61656 | valid_mae: 1.65133 |  0:02:04s\n",
      "epoch 18 | loss: 1.67547 | train_mae: 1.95327 | valid_mae: 1.99099 |  0:02:11s\n",
      "epoch 19 | loss: 1.62021 | train_mae: 1.88368 | valid_mae: 1.92167 |  0:02:17s\n",
      "epoch 20 | loss: 1.45704 | train_mae: 1.39963 | valid_mae: 1.44277 |  0:02:24s\n",
      "epoch 21 | loss: 1.47254 | train_mae: 1.25836 | valid_mae: 1.30614 |  0:02:31s\n",
      "epoch 22 | loss: 1.43248 | train_mae: 1.43708 | valid_mae: 1.48506 |  0:02:37s\n",
      "epoch 23 | loss: 1.28064 | train_mae: 1.78151 | valid_mae: 1.82598 |  0:02:44s\n",
      "epoch 24 | loss: 1.31905 | train_mae: 1.62692 | valid_mae: 1.67448 |  0:02:51s\n",
      "epoch 25 | loss: 1.18407 | train_mae: 1.3123  | valid_mae: 1.36385 |  0:02:57s\n",
      "epoch 26 | loss: 1.20806 | train_mae: 1.33292 | valid_mae: 1.38542 |  0:03:04s\n",
      "epoch 27 | loss: 1.11231 | train_mae: 1.49698 | valid_mae: 1.54934 |  0:03:11s\n",
      "epoch 28 | loss: 1.1084  | train_mae: 1.56704 | valid_mae: 1.61982 |  0:03:18s\n",
      "epoch 29 | loss: 1.11381 | train_mae: 1.42566 | valid_mae: 1.48287 |  0:03:24s\n",
      "epoch 30 | loss: 1.03414 | train_mae: 1.27353 | valid_mae: 1.33404 |  0:03:31s\n",
      "epoch 31 | loss: 1.0083  | train_mae: 1.24424 | valid_mae: 1.3059  |  0:03:38s\n",
      "epoch 32 | loss: 1.00113 | train_mae: 1.29179 | valid_mae: 1.3536  |  0:03:45s\n",
      "epoch 33 | loss: 0.97279 | train_mae: 1.34763 | valid_mae: 1.40868 |  0:03:51s\n",
      "epoch 34 | loss: 0.95943 | train_mae: 1.28568 | valid_mae: 1.34943 |  0:03:58s\n",
      "epoch 35 | loss: 0.94062 | train_mae: 1.18365 | valid_mae: 1.25026 |  0:04:05s\n",
      "epoch 36 | loss: 0.92667 | train_mae: 1.21254 | valid_mae: 1.27949 |  0:04:11s\n",
      "epoch 37 | loss: 0.90498 | train_mae: 1.22748 | valid_mae: 1.29619 |  0:04:18s\n",
      "epoch 38 | loss: 0.8906  | train_mae: 1.20479 | valid_mae: 1.274   |  0:04:25s\n",
      "epoch 39 | loss: 0.8707  | train_mae: 1.14837 | valid_mae: 1.22089 |  0:04:32s\n",
      "epoch 40 | loss: 0.86928 | train_mae: 1.13046 | valid_mae: 1.2037  |  0:04:38s\n",
      "epoch 41 | loss: 0.84258 | train_mae: 1.16654 | valid_mae: 1.23918 |  0:04:45s\n",
      "epoch 42 | loss: 0.83553 | train_mae: 1.12521 | valid_mae: 1.1998  |  0:04:52s\n",
      "epoch 43 | loss: 0.82184 | train_mae: 1.07578 | valid_mae: 1.15208 |  0:04:59s\n",
      "epoch 44 | loss: 0.80884 | train_mae: 1.08584 | valid_mae: 1.16326 |  0:05:05s\n",
      "epoch 45 | loss: 0.79382 | train_mae: 1.09818 | valid_mae: 1.1764  |  0:05:12s\n",
      "epoch 46 | loss: 0.78507 | train_mae: 1.05474 | valid_mae: 1.13496 |  0:05:19s\n",
      "epoch 47 | loss: 0.77228 | train_mae: 1.02316 | valid_mae: 1.10477 |  0:05:26s\n",
      "epoch 48 | loss: 0.76071 | train_mae: 1.02317 | valid_mae: 1.10546 |  0:05:32s\n",
      "epoch 49 | loss: 0.74581 | train_mae: 1.00304 | valid_mae: 1.08645 |  0:05:39s\n",
      "epoch 50 | loss: 0.73678 | train_mae: 0.98108 | valid_mae: 1.06471 |  0:05:46s\n",
      "epoch 51 | loss: 0.72902 | train_mae: 0.98033 | valid_mae: 1.06508 |  0:05:53s\n",
      "epoch 52 | loss: 0.71941 | train_mae: 0.98425 | valid_mae: 1.06966 |  0:05:59s\n",
      "epoch 53 | loss: 0.71225 | train_mae: 0.94106 | valid_mae: 1.02922 |  0:06:06s\n",
      "epoch 54 | loss: 0.69734 | train_mae: 0.95061 | valid_mae: 1.04013 |  0:06:13s\n",
      "epoch 55 | loss: 0.68383 | train_mae: 0.9483  | valid_mae: 1.03917 |  0:06:20s\n",
      "epoch 56 | loss: 0.67214 | train_mae: 0.94098 | valid_mae: 1.03291 |  0:06:26s\n",
      "epoch 57 | loss: 0.66158 | train_mae: 0.91768 | valid_mae: 1.0102  |  0:06:33s\n",
      "epoch 58 | loss: 0.66194 | train_mae: 0.90551 | valid_mae: 0.99974 |  0:06:40s\n",
      "epoch 59 | loss: 0.65714 | train_mae: 0.89085 | valid_mae: 0.98599 |  0:06:47s\n",
      "epoch 60 | loss: 0.64346 | train_mae: 0.90524 | valid_mae: 1.00112 |  0:06:54s\n",
      "epoch 61 | loss: 0.6352  | train_mae: 0.88136 | valid_mae: 0.97974 |  0:07:00s\n",
      "epoch 62 | loss: 0.62262 | train_mae: 0.87763 | valid_mae: 0.97719 |  0:07:07s\n",
      "epoch 63 | loss: 0.6129  | train_mae: 0.86066 | valid_mae: 0.9624  |  0:07:14s\n",
      "epoch 64 | loss: 0.61222 | train_mae: 0.83596 | valid_mae: 0.94066 |  0:07:21s\n",
      "epoch 65 | loss: 0.60369 | train_mae: 0.84572 | valid_mae: 0.95003 |  0:07:28s\n",
      "epoch 66 | loss: 0.60334 | train_mae: 0.8219  | valid_mae: 0.92833 |  0:07:34s\n",
      "epoch 67 | loss: 0.59306 | train_mae: 0.83288 | valid_mae: 0.93854 |  0:07:41s\n",
      "epoch 68 | loss: 0.5849  | train_mae: 0.80253 | valid_mae: 0.91162 |  0:07:48s\n",
      "epoch 69 | loss: 0.57758 | train_mae: 0.80989 | valid_mae: 0.91915 |  0:07:55s\n",
      "epoch 70 | loss: 0.5746  | train_mae: 0.79034 | valid_mae: 0.90188 |  0:08:02s\n",
      "epoch 71 | loss: 0.56936 | train_mae: 0.77689 | valid_mae: 0.88973 |  0:08:09s\n",
      "epoch 72 | loss: 0.56474 | train_mae: 0.80629 | valid_mae: 0.91668 |  0:08:20s\n",
      "epoch 73 | loss: 0.55981 | train_mae: 0.76636 | valid_mae: 0.88183 |  0:08:27s\n",
      "epoch 74 | loss: 0.554   | train_mae: 0.77516 | valid_mae: 0.8896  |  0:08:34s\n",
      "epoch 75 | loss: 0.54713 | train_mae: 0.75713 | valid_mae: 0.87384 |  0:08:40s\n",
      "epoch 76 | loss: 0.54625 | train_mae: 0.76623 | valid_mae: 0.8825  |  0:08:47s\n",
      "epoch 77 | loss: 0.53814 | train_mae: 0.74744 | valid_mae: 0.86555 |  0:08:54s\n",
      "epoch 78 | loss: 0.5391  | train_mae: 0.73558 | valid_mae: 0.85429 |  0:09:01s\n",
      "epoch 79 | loss: 0.53219 | train_mae: 0.72678 | valid_mae: 0.84746 |  0:09:07s\n",
      "epoch 80 | loss: 0.52823 | train_mae: 0.72576 | valid_mae: 0.84633 |  0:09:14s\n",
      "epoch 81 | loss: 0.52214 | train_mae: 0.70888 | valid_mae: 0.83182 |  0:09:21s\n",
      "epoch 82 | loss: 0.52399 | train_mae: 0.70197 | valid_mae: 0.8262  |  0:09:28s\n",
      "epoch 83 | loss: 0.51865 | train_mae: 0.71529 | valid_mae: 0.83896 |  0:09:35s\n",
      "epoch 84 | loss: 0.5122  | train_mae: 0.68618 | valid_mae: 0.8135  |  0:09:41s\n",
      "epoch 85 | loss: 0.51232 | train_mae: 0.68909 | valid_mae: 0.81676 |  0:09:48s\n",
      "epoch 86 | loss: 0.5096  | train_mae: 0.67905 | valid_mae: 0.80727 |  0:09:55s\n",
      "epoch 87 | loss: 0.50688 | train_mae: 0.67835 | valid_mae: 0.80599 |  0:10:02s\n",
      "epoch 88 | loss: 0.5125  | train_mae: 0.68661 | valid_mae: 0.81374 |  0:10:08s\n",
      "epoch 89 | loss: 0.504   | train_mae: 0.67501 | valid_mae: 0.80455 |  0:10:15s\n",
      "epoch 90 | loss: 0.49864 | train_mae: 0.67359 | valid_mae: 0.80428 |  0:10:22s\n",
      "epoch 91 | loss: 0.50036 | train_mae: 0.67179 | valid_mae: 0.80392 |  0:10:29s\n",
      "epoch 92 | loss: 0.49017 | train_mae: 0.64981 | valid_mae: 0.78495 |  0:10:36s\n",
      "epoch 93 | loss: 0.48966 | train_mae: 0.63948 | valid_mae: 0.77542 |  0:10:42s\n",
      "epoch 94 | loss: 0.48663 | train_mae: 0.6431  | valid_mae: 0.77943 |  0:10:49s\n",
      "epoch 95 | loss: 0.48089 | train_mae: 0.6424  | valid_mae: 0.77964 |  0:10:56s\n",
      "epoch 96 | loss: 0.47653 | train_mae: 0.61805 | valid_mae: 0.75771 |  0:11:03s\n",
      "epoch 97 | loss: 0.46783 | train_mae: 0.63266 | valid_mae: 0.7716  |  0:11:10s\n",
      "epoch 98 | loss: 0.46867 | train_mae: 0.62386 | valid_mae: 0.76341 |  0:11:17s\n",
      "epoch 99 | loss: 0.47045 | train_mae: 0.62979 | valid_mae: 0.76918 |  0:11:23s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 96 and best_valid_mae = 0.75771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.1750, MAE: 0.7577, SRC: 0.9025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/abstract_model.py:231: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 66.71686| train_mae: 3.27632 | valid_mae: 3.27941 |  0:00:06s\n",
      "epoch 1  | loss: 11.0623 | train_mae: 2.0555  | valid_mae: 2.0461  |  0:00:13s\n",
      "epoch 2  | loss: 7.10822 | train_mae: 1.94152 | valid_mae: 1.94256 |  0:00:20s\n",
      "epoch 3  | loss: 7.89374 | train_mae: 2.5224  | valid_mae: 2.50989 |  0:00:27s\n",
      "epoch 4  | loss: 5.58036 | train_mae: 1.7959  | valid_mae: 1.79164 |  0:00:34s\n",
      "epoch 5  | loss: 5.24072 | train_mae: 1.75531 | valid_mae: 1.74887 |  0:00:41s\n",
      "epoch 6  | loss: 4.59979 | train_mae: 2.39396 | valid_mae: 2.3852  |  0:00:47s\n",
      "epoch 7  | loss: 4.12083 | train_mae: 2.19642 | valid_mae: 2.18984 |  0:00:54s\n",
      "epoch 8  | loss: 3.71537 | train_mae: 2.603   | valid_mae: 2.59615 |  0:01:01s\n",
      "epoch 9  | loss: 3.34599 | train_mae: 2.0719  | valid_mae: 2.06753 |  0:01:08s\n",
      "epoch 10 | loss: 2.87115 | train_mae: 2.2732  | valid_mae: 2.26992 |  0:01:14s\n",
      "epoch 11 | loss: 2.55452 | train_mae: 2.05176 | valid_mae: 2.05198 |  0:01:21s\n",
      "epoch 12 | loss: 2.29339 | train_mae: 2.14654 | valid_mae: 2.14856 |  0:01:28s\n",
      "epoch 13 | loss: 2.07831 | train_mae: 1.82995 | valid_mae: 1.83768 |  0:01:35s\n",
      "epoch 14 | loss: 1.92644 | train_mae: 2.05584 | valid_mae: 2.06335 |  0:01:42s\n",
      "epoch 15 | loss: 1.83943 | train_mae: 2.07716 | valid_mae: 2.08521 |  0:01:48s\n",
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_valid_mae = 1.74887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 4.9217, MAE: 1.7489, SRC: 0.4106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/abstract_model.py:231: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 66.70995| train_mae: 4.08361 | valid_mae: 4.08767 |  0:00:06s\n",
      "epoch 1  | loss: 11.05002| train_mae: 2.20612 | valid_mae: 2.20635 |  0:00:14s\n",
      "epoch 2  | loss: 7.96812 | train_mae: 2.21974 | valid_mae: 2.20939 |  0:00:20s\n",
      "epoch 3  | loss: 8.0157  | train_mae: 2.00669 | valid_mae: 2.00161 |  0:00:27s\n",
      "epoch 4  | loss: 6.59382 | train_mae: 2.66987 | valid_mae: 2.68104 |  0:00:34s\n",
      "epoch 5  | loss: 7.55469 | train_mae: 1.83114 | valid_mae: 1.83376 |  0:00:41s\n",
      "epoch 6  | loss: 5.13571 | train_mae: 1.98779 | valid_mae: 1.98939 |  0:00:48s\n",
      "epoch 7  | loss: 4.6001  | train_mae: 1.76128 | valid_mae: 1.76463 |  0:00:55s\n",
      "epoch 8  | loss: 4.10591 | train_mae: 1.82136 | valid_mae: 1.82445 |  0:01:01s\n",
      "epoch 9  | loss: 3.60588 | train_mae: 1.75075 | valid_mae: 1.75699 |  0:01:09s\n",
      "epoch 10 | loss: 3.1282  | train_mae: 1.64606 | valid_mae: 1.66018 |  0:01:16s\n",
      "epoch 11 | loss: 2.60313 | train_mae: 1.63031 | valid_mae: 1.65217 |  0:01:22s\n",
      "epoch 12 | loss: 2.19352 | train_mae: 1.56184 | valid_mae: 1.59413 |  0:01:29s\n",
      "epoch 13 | loss: 1.85152 | train_mae: 1.42651 | valid_mae: 1.47169 |  0:01:36s\n",
      "epoch 14 | loss: 1.58178 | train_mae: 1.26872 | valid_mae: 1.32387 |  0:01:43s\n",
      "epoch 15 | loss: 1.37738 | train_mae: 1.22938 | valid_mae: 1.28951 |  0:01:50s\n",
      "epoch 16 | loss: 1.22524 | train_mae: 1.17461 | valid_mae: 1.23567 |  0:01:57s\n",
      "epoch 17 | loss: 1.11552 | train_mae: 1.24475 | valid_mae: 1.31266 |  0:02:04s\n",
      "epoch 18 | loss: 1.05861 | train_mae: 1.31228 | valid_mae: 1.37973 |  0:02:11s\n",
      "epoch 19 | loss: 0.95723 | train_mae: 1.03364 | valid_mae: 1.11688 |  0:02:18s\n",
      "epoch 20 | loss: 0.92454 | train_mae: 1.04878 | valid_mae: 1.1243  |  0:02:24s\n",
      "epoch 21 | loss: 0.81656 | train_mae: 1.04298 | valid_mae: 1.13083 |  0:02:31s\n",
      "epoch 22 | loss: 0.78997 | train_mae: 1.2767  | valid_mae: 1.35235 |  0:02:38s\n",
      "epoch 23 | loss: 0.90336 | train_mae: 0.99942 | valid_mae: 1.07807 |  0:02:45s\n",
      "epoch 24 | loss: 0.84144 | train_mae: 1.18413 | valid_mae: 1.26406 |  0:02:55s\n",
      "epoch 25 | loss: 0.7388  | train_mae: 0.86552 | valid_mae: 0.95228 |  0:03:03s\n",
      "epoch 26 | loss: 0.81256 | train_mae: 1.1218  | valid_mae: 1.20189 |  0:03:10s\n",
      "epoch 27 | loss: 0.6735  | train_mae: 0.88759 | valid_mae: 0.97095 |  0:03:17s\n",
      "epoch 28 | loss: 0.74769 | train_mae: 1.08641 | valid_mae: 1.16792 |  0:03:23s\n",
      "epoch 29 | loss: 0.65949 | train_mae: 0.87645 | valid_mae: 0.95956 |  0:03:30s\n",
      "epoch 30 | loss: 0.6518  | train_mae: 1.01522 | valid_mae: 1.09831 |  0:03:37s\n",
      "epoch 31 | loss: 0.597   | train_mae: 1.18217 | valid_mae: 1.26182 |  0:03:44s\n",
      "epoch 32 | loss: 0.57767 | train_mae: 0.86403 | valid_mae: 0.95337 |  0:03:51s\n",
      "epoch 33 | loss: 0.58138 | train_mae: 0.98238 | valid_mae: 1.06815 |  0:03:57s\n",
      "epoch 34 | loss: 0.52859 | train_mae: 1.00211 | valid_mae: 1.08966 |  0:04:04s\n",
      "epoch 35 | loss: 0.49757 | train_mae: 0.93082 | valid_mae: 1.02105 |  0:04:11s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_valid_mae = 0.95228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.7271, MAE: 0.9523, SRC: 0.8549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "# BS = 2048\n",
    "BS=85600\n",
    "MAX_EPOCH =  100\n",
    "LAMBDA_SPARSE = 1e-5 #1e-5\n",
    "submit_proba = []\n",
    "N_SPLITS = 5\n",
    "NB_FOLDS = 5 # max N_SPLITS\n",
    "# skf = StratifiedKFold(n_splits=N_SPLITS, random_state=2021, shuffle=True)\n",
    "kfold = KFold(n_splits=N_SPLITS, shuffle=True, random_state=2020)\n",
    "from sklearn.metrics import f1_score, mean_absolute_error, mean_squared_error\n",
    "from scipy import stats\n",
    "# k = 0\n",
    "\n",
    "# for train_idx, valid_idx in kfold.split(train_feature_df, train_label_df):\n",
    "\n",
    "LR = 1e-1 # 5e-2\n",
    "fold_nb = 1\n",
    "for train_index, valid_index in kfold.split(X, y):\n",
    "    X_train, X_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index].reshape(-1, 1), y[valid_index].reshape(-1, 1)\n",
    "\n",
    "    tabnet_params = dict(n_d=N_D, \n",
    "                         n_a=N_A,\n",
    "                         n_steps=N_STEPS, gamma=GAMMA,\n",
    "                         n_independent=N_INDEP, n_shared=N_SHARED,\n",
    "                         lambda_sparse=LAMBDA_SPARSE,\n",
    "                         seed=0,\n",
    "                         # clip_value=2,\n",
    "                         cat_idxs=cat_idxs,\n",
    "                         cat_dims=cat_dims,\n",
    "                         cat_emb_dim=cat_emb_dims,\n",
    "                         mask_type=MASK_TYPE,\n",
    "                         device_name='auto',\n",
    "                         optimizer_fn=torch.optim.Adam,\n",
    "                         optimizer_params=dict(lr=LR, weight_decay=1e-5),\n",
    "#                          scheduler_params=dict(max_lr=LR,\n",
    "#                                                steps_per_epoch=int(X_train.shape[0] / BS),\n",
    "#                                                epochs=MAX_EPOCH,\n",
    "#                                                #final_div_factor=100,\n",
    "#                                                is_batch_level=True),\n",
    "#                         scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n",
    "                              scheduler_params=dict(mode='min',\n",
    "                                                    factor=0.5,\n",
    "                                                    patience=3,\n",
    "                                                    is_batch_level=False,),\n",
    "                              scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "                         verbose=1)\n",
    "    # Defining TabNet model\n",
    "    # model = TabNetClassifier(**tabnet_params)\n",
    "    model = TabNetRegressor(**tabnet_params)\n",
    "\n",
    "    model.fit(X_train=X_train, y_train=y_train,\n",
    "              from_unsupervised=pretrainer if PRETRAIN else None,\n",
    "              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "              eval_name=[\"train\", \"valid\"],\n",
    "              eval_metric=[\"mae\"],\n",
    "              batch_size=BS,\n",
    "              virtual_batch_size=256,\n",
    "              max_epochs=MAX_EPOCH,\n",
    "              drop_last=True,\n",
    "              pin_memory=True,\n",
    "              patience=10,\n",
    "             )  \n",
    "    \n",
    "    valid_pred = model.predict(X_valid).reshape(-1)\n",
    "    valid_mse = mean_squared_error(y_valid, valid_pred)\n",
    "    valid_mae = mean_absolute_error(y_valid, valid_pred)\n",
    "    valid_src = stats.spearmanr(y_valid, valid_pred)[0]\n",
    "    \n",
    "    print(\"MSE: %.4f, MAE: %.4f, SRC: %.4f\"%(valid_mse, valid_mae, valid_src))\n",
    "    test_preds = model.predict(X_test)\n",
    "    submit_proba.append(test_preds.reshape(-1))\n",
    "    # submit_proba[model.classes_] += test_preds.reshape(-1)\n",
    "    fold_nb+=1\n",
    "    \n",
    "    if fold_nb > NB_FOLDS:\n",
    "        break\n",
    "\n",
    "# df_sub[model.classes_] = df_sub[model.classes_] / NB_FOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6d8fa2-dfcc-436f-a37c-0638fd0a31ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 8.489922, 10.063567, 10.48105 , ...,  9.16613 ,  9.477614,\n",
       "         8.815059], dtype=float32),\n",
       " array([ 9.24217  , 12.742173 , 11.854801 , ...,  7.0721893,  7.7995334,\n",
       "         5.276364 ], dtype=float32),\n",
       " array([ 8.973942 , 10.998484 , 10.673437 , ...,  7.9875584,  9.080473 ,\n",
       "        10.133011 ], dtype=float32),\n",
       " array([6.2696753, 9.154137 , 9.3088255, ..., 8.560579 , 9.097906 ,\n",
       "        8.174707 ], dtype=float32),\n",
       " array([ 7.6893454, 11.436663 , 11.60103  , ...,  7.963258 ,  7.764239 ,\n",
       "         8.322953 ], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "796c983c-301e-49d8-97f2-aca2e1a77e57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>180571</th>\n",
       "      <th>180572</th>\n",
       "      <th>180573</th>\n",
       "      <th>180574</th>\n",
       "      <th>180575</th>\n",
       "      <th>180576</th>\n",
       "      <th>180577</th>\n",
       "      <th>180578</th>\n",
       "      <th>180579</th>\n",
       "      <th>180580</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.489922</td>\n",
       "      <td>10.063567</td>\n",
       "      <td>10.481050</td>\n",
       "      <td>5.970233</td>\n",
       "      <td>5.970233</td>\n",
       "      <td>5.970233</td>\n",
       "      <td>5.897909</td>\n",
       "      <td>5.922754</td>\n",
       "      <td>6.012195</td>\n",
       "      <td>6.022112</td>\n",
       "      <td>...</td>\n",
       "      <td>9.605143</td>\n",
       "      <td>5.260343</td>\n",
       "      <td>9.106287</td>\n",
       "      <td>5.419328</td>\n",
       "      <td>5.477552</td>\n",
       "      <td>9.108200</td>\n",
       "      <td>5.731314</td>\n",
       "      <td>9.166130</td>\n",
       "      <td>9.477614</td>\n",
       "      <td>8.815059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.242170</td>\n",
       "      <td>12.742173</td>\n",
       "      <td>11.854801</td>\n",
       "      <td>6.946651</td>\n",
       "      <td>6.946651</td>\n",
       "      <td>6.946651</td>\n",
       "      <td>6.951292</td>\n",
       "      <td>7.118220</td>\n",
       "      <td>6.998113</td>\n",
       "      <td>6.997842</td>\n",
       "      <td>...</td>\n",
       "      <td>9.631916</td>\n",
       "      <td>7.250736</td>\n",
       "      <td>4.211897</td>\n",
       "      <td>3.483494</td>\n",
       "      <td>8.690977</td>\n",
       "      <td>7.022602</td>\n",
       "      <td>5.709376</td>\n",
       "      <td>7.072189</td>\n",
       "      <td>7.799533</td>\n",
       "      <td>5.276364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.973942</td>\n",
       "      <td>10.998484</td>\n",
       "      <td>10.673437</td>\n",
       "      <td>7.474725</td>\n",
       "      <td>7.474725</td>\n",
       "      <td>7.474725</td>\n",
       "      <td>7.573672</td>\n",
       "      <td>7.546614</td>\n",
       "      <td>7.459363</td>\n",
       "      <td>7.459363</td>\n",
       "      <td>...</td>\n",
       "      <td>7.495007</td>\n",
       "      <td>7.576668</td>\n",
       "      <td>8.202629</td>\n",
       "      <td>5.410357</td>\n",
       "      <td>7.181006</td>\n",
       "      <td>8.593084</td>\n",
       "      <td>7.400333</td>\n",
       "      <td>7.987558</td>\n",
       "      <td>9.080473</td>\n",
       "      <td>10.133011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.269675</td>\n",
       "      <td>9.154137</td>\n",
       "      <td>9.308825</td>\n",
       "      <td>6.172396</td>\n",
       "      <td>6.172396</td>\n",
       "      <td>6.172396</td>\n",
       "      <td>6.950052</td>\n",
       "      <td>6.031546</td>\n",
       "      <td>6.521904</td>\n",
       "      <td>5.989501</td>\n",
       "      <td>...</td>\n",
       "      <td>7.845787</td>\n",
       "      <td>5.868503</td>\n",
       "      <td>8.546341</td>\n",
       "      <td>6.176498</td>\n",
       "      <td>6.311522</td>\n",
       "      <td>8.500408</td>\n",
       "      <td>5.672679</td>\n",
       "      <td>8.560579</td>\n",
       "      <td>9.097906</td>\n",
       "      <td>8.174707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.689345</td>\n",
       "      <td>11.436663</td>\n",
       "      <td>11.601030</td>\n",
       "      <td>7.344453</td>\n",
       "      <td>7.344453</td>\n",
       "      <td>7.344453</td>\n",
       "      <td>7.399371</td>\n",
       "      <td>7.521214</td>\n",
       "      <td>7.598316</td>\n",
       "      <td>7.615616</td>\n",
       "      <td>...</td>\n",
       "      <td>8.084208</td>\n",
       "      <td>6.066680</td>\n",
       "      <td>7.264904</td>\n",
       "      <td>5.262511</td>\n",
       "      <td>9.256780</td>\n",
       "      <td>8.031136</td>\n",
       "      <td>6.213771</td>\n",
       "      <td>7.963258</td>\n",
       "      <td>7.764239</td>\n",
       "      <td>8.322953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  180581 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0          1          2         3         4         5         6       \\\n",
       "0  8.489922  10.063567  10.481050  5.970233  5.970233  5.970233  5.897909   \n",
       "1  9.242170  12.742173  11.854801  6.946651  6.946651  6.946651  6.951292   \n",
       "2  8.973942  10.998484  10.673437  7.474725  7.474725  7.474725  7.573672   \n",
       "3  6.269675   9.154137   9.308825  6.172396  6.172396  6.172396  6.950052   \n",
       "4  7.689345  11.436663  11.601030  7.344453  7.344453  7.344453  7.399371   \n",
       "\n",
       "     7         8         9       ...    180571    180572    180573    180574  \\\n",
       "0  5.922754  6.012195  6.022112  ...  9.605143  5.260343  9.106287  5.419328   \n",
       "1  7.118220  6.998113  6.997842  ...  9.631916  7.250736  4.211897  3.483494   \n",
       "2  7.546614  7.459363  7.459363  ...  7.495007  7.576668  8.202629  5.410357   \n",
       "3  6.031546  6.521904  5.989501  ...  7.845787  5.868503  8.546341  6.176498   \n",
       "4  7.521214  7.598316  7.615616  ...  8.084208  6.066680  7.264904  5.262511   \n",
       "\n",
       "     180575    180576    180577    180578    180579     180580  \n",
       "0  5.477552  9.108200  5.731314  9.166130  9.477614   8.815059  \n",
       "1  8.690977  7.022602  5.709376  7.072189  7.799533   5.276364  \n",
       "2  7.181006  8.593084  7.400333  7.987558  9.080473  10.133011  \n",
       "3  6.311522  8.500408  5.672679  8.560579  9.097906   8.174707  \n",
       "4  9.256780  8.031136  6.213771  7.963258  7.764239   8.322953  \n",
       "\n",
       "[5 rows x 180581 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_proba_df = pd.DataFrame(submit_proba)\n",
    "submit_proba_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "635721f7-3898-453e-94ac-67b71141a0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>180571</th>\n",
       "      <th>180572</th>\n",
       "      <th>180573</th>\n",
       "      <th>180574</th>\n",
       "      <th>180575</th>\n",
       "      <th>180576</th>\n",
       "      <th>180577</th>\n",
       "      <th>180578</th>\n",
       "      <th>180579</th>\n",
       "      <th>180580</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.735038</td>\n",
       "      <td>11.138809</td>\n",
       "      <td>11.652602</td>\n",
       "      <td>6.768065</td>\n",
       "      <td>6.768065</td>\n",
       "      <td>6.768065</td>\n",
       "      <td>6.765581</td>\n",
       "      <td>6.587955</td>\n",
       "      <td>6.426687</td>\n",
       "      <td>6.693336</td>\n",
       "      <td>...</td>\n",
       "      <td>3.283219</td>\n",
       "      <td>1.826190</td>\n",
       "      <td>2.635335</td>\n",
       "      <td>5.520903</td>\n",
       "      <td>6.485078</td>\n",
       "      <td>6.168549</td>\n",
       "      <td>4.931555</td>\n",
       "      <td>6.509181</td>\n",
       "      <td>5.962986</td>\n",
       "      <td>3.899653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.328691</td>\n",
       "      <td>12.250772</td>\n",
       "      <td>11.780432</td>\n",
       "      <td>7.161756</td>\n",
       "      <td>7.161756</td>\n",
       "      <td>7.161756</td>\n",
       "      <td>7.007789</td>\n",
       "      <td>7.063338</td>\n",
       "      <td>6.860058</td>\n",
       "      <td>7.010128</td>\n",
       "      <td>...</td>\n",
       "      <td>7.014502</td>\n",
       "      <td>6.575831</td>\n",
       "      <td>5.914214</td>\n",
       "      <td>4.218393</td>\n",
       "      <td>8.248645</td>\n",
       "      <td>5.947807</td>\n",
       "      <td>6.504625</td>\n",
       "      <td>6.071412</td>\n",
       "      <td>6.409212</td>\n",
       "      <td>7.973583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.110669</td>\n",
       "      <td>11.153367</td>\n",
       "      <td>10.340147</td>\n",
       "      <td>6.090604</td>\n",
       "      <td>6.090604</td>\n",
       "      <td>6.090604</td>\n",
       "      <td>6.255491</td>\n",
       "      <td>6.101233</td>\n",
       "      <td>6.266757</td>\n",
       "      <td>6.502548</td>\n",
       "      <td>...</td>\n",
       "      <td>4.023444</td>\n",
       "      <td>1.867095</td>\n",
       "      <td>2.946946</td>\n",
       "      <td>6.652179</td>\n",
       "      <td>8.638971</td>\n",
       "      <td>5.207249</td>\n",
       "      <td>5.992674</td>\n",
       "      <td>6.460787</td>\n",
       "      <td>6.175128</td>\n",
       "      <td>3.487654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.604742</td>\n",
       "      <td>11.016546</td>\n",
       "      <td>13.976315</td>\n",
       "      <td>6.619453</td>\n",
       "      <td>6.619453</td>\n",
       "      <td>6.619453</td>\n",
       "      <td>6.699306</td>\n",
       "      <td>6.616934</td>\n",
       "      <td>6.614456</td>\n",
       "      <td>6.626598</td>\n",
       "      <td>...</td>\n",
       "      <td>5.642315</td>\n",
       "      <td>3.230766</td>\n",
       "      <td>3.285082</td>\n",
       "      <td>5.520288</td>\n",
       "      <td>11.582042</td>\n",
       "      <td>5.935596</td>\n",
       "      <td>8.202895</td>\n",
       "      <td>4.673194</td>\n",
       "      <td>5.541387</td>\n",
       "      <td>3.573328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.404734</td>\n",
       "      <td>9.483094</td>\n",
       "      <td>8.322335</td>\n",
       "      <td>6.653327</td>\n",
       "      <td>6.653327</td>\n",
       "      <td>6.653327</td>\n",
       "      <td>6.845078</td>\n",
       "      <td>6.719975</td>\n",
       "      <td>6.579948</td>\n",
       "      <td>6.675615</td>\n",
       "      <td>...</td>\n",
       "      <td>7.835574</td>\n",
       "      <td>7.401578</td>\n",
       "      <td>6.627490</td>\n",
       "      <td>5.588782</td>\n",
       "      <td>10.793053</td>\n",
       "      <td>5.367823</td>\n",
       "      <td>8.223528</td>\n",
       "      <td>6.927838</td>\n",
       "      <td>5.097910</td>\n",
       "      <td>7.506988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  180581 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0          1          2         3         4         5         6       \\\n",
       "0  11.735038  11.138809  11.652602  6.768065  6.768065  6.768065  6.765581   \n",
       "1   9.328691  12.250772  11.780432  7.161756  7.161756  7.161756  7.007789   \n",
       "2   6.110669  11.153367  10.340147  6.090604  6.090604  6.090604  6.255491   \n",
       "3   6.604742  11.016546  13.976315  6.619453  6.619453  6.619453  6.699306   \n",
       "4  11.404734   9.483094   8.322335  6.653327  6.653327  6.653327  6.845078   \n",
       "\n",
       "     7         8         9       ...    180571    180572    180573    180574  \\\n",
       "0  6.587955  6.426687  6.693336  ...  3.283219  1.826190  2.635335  5.520903   \n",
       "1  7.063338  6.860058  7.010128  ...  7.014502  6.575831  5.914214  4.218393   \n",
       "2  6.101233  6.266757  6.502548  ...  4.023444  1.867095  2.946946  6.652179   \n",
       "3  6.616934  6.614456  6.626598  ...  5.642315  3.230766  3.285082  5.520288   \n",
       "4  6.719975  6.579948  6.675615  ...  7.835574  7.401578  6.627490  5.588782   \n",
       "\n",
       "      180575    180576    180577    180578    180579    180580  \n",
       "0   6.485078  6.168549  4.931555  6.509181  5.962986  3.899653  \n",
       "1   8.248645  5.947807  6.504625  6.071412  6.409212  7.973583  \n",
       "2   8.638971  5.207249  5.992674  6.460787  6.175128  3.487654  \n",
       "3  11.582042  5.935596  8.202895  4.673194  5.541387  3.573328  \n",
       "4  10.793053  5.367823  8.223528  6.927838  5.097910  7.506988  \n",
       "\n",
       "[5 rows x 180581 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_proba_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d17416a-2769-4092-b4aa-e3986352fcb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import f1_score, mean_absolute_error, mean_squared_error\n",
    "r0 = np.array(submit_proba_df.loc[0])\n",
    "r1 = np.array(submit_proba_df.loc[1])\n",
    "r2 = np.array(submit_proba_df.loc[2])\n",
    "r3 = np.array(submit_proba_df.loc[3])\n",
    "r4 = np.array(submit_proba_df.loc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bbb5088-1133-476a-ab36-4483acc98edc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(r1).to_csv('./TabNet1.csv',header=True, index=None)\n",
    "pd.DataFrame(r2).to_csv('./TabNet2.csv',header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ced495-0788-48a6-896f-da7fdce05347",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae12 1.4619255 src12 0.5971175538911347\n",
      "mae13 1.4140396 src13 0.5066433724223843\n",
      "mae23 1.5974623 src23 0.3176376698819749\n"
     ]
    }
   ],
   "source": [
    "## r1,r2\n",
    "mae12 = mean_absolute_error(r1, r2)\n",
    "src12 = stats.spearmanr(r1, r2)[0]\n",
    "print('mae12', mae12, 'src12',src12)\n",
    "mae13 = mean_absolute_error(r1, r3)\n",
    "src13 = stats.spearmanr(r1, r3)[0]\n",
    "print('mae13', mae13, 'src13',src13)\n",
    "mae23 = mean_absolute_error(r2, r3)\n",
    "src23 = stats.spearmanr(r2, r3)[0]\n",
    "print('mae23', mae23, 'src23',src23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd96164-3fe5-4393-af5f-6b5440f39e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0968fced-76b1-49a9-b2ef-6006e3fc6413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          6.604742\n",
       "1         11.016546\n",
       "2         13.976315\n",
       "3          6.619453\n",
       "4          6.619453\n",
       "            ...    \n",
       "180576     5.935596\n",
       "180577     8.202895\n",
       "180578     4.673194\n",
       "180579     5.541387\n",
       "180580     3.573328\n",
       "Name: 3, Length: 180581, dtype: float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_proba_df.loc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29b7fa51-cc30-4863-bada-d4dee4d733cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          9.536386\n",
       "1         10.941848\n",
       "2         11.910572\n",
       "3          6.955659\n",
       "4          6.955659\n",
       "            ...    \n",
       "180576     6.200135\n",
       "180577     6.908788\n",
       "180578     6.703999\n",
       "180579     5.915728\n",
       "180580     5.314737\n",
       "Length: 180581, dtype: float32"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_ans = np.mean(submit_proba_df, axis=0)\n",
    "submit_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a76be008-d64f-480d-a921-5ccc2a74b440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(submit_ans).to_csv('./TabNet.csv',header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279fe694-89fe-4e53-8ce8-ee15ad8a371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result['post_id'] = submit_label_df['Pid'].apply(lambda x: 'post' + str(x))\n",
    "result['popularity_score'] = submit_ans.round(decimals=4)\n",
    "\n",
    "out_json = dict()\n",
    "out_json[\"version\"] = \"VERSION 1.0\"\n",
    "out_json[\"result\"] = result.to_dict(orient='records')\n",
    "out_json[\"external_data\"] = {\"used\": \"true\", \"details\": \"catboost\"}\n",
    "f = open('KFold_catboost.json', \"w\")\n",
    "json.dump(out_json, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10645dc3-1c6d-42f9-a2ec-62439c8313e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf65e3d-23bf-4cb8-b3f4-1bd3cb3cffb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b32d5-07fb-44b7-8e39-7fa2644b9b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ca0b57c-b846-4d4a-8f88-ff716378dd13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss = []\n",
    "test_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4662a074-8fa6-4dce-b58e-d508ab630774",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.0599, MAE: 0.5958, SRC: 0.9169\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, mean_absolute_error, mean_squared_error\n",
    "from scipy import stats\n",
    "valid_pred = model.predict(X_valid).reshape(-1)\n",
    "valid_mse = mean_squared_error(y_valid, valid_pred)\n",
    "valid_mae = mean_absolute_error(y_valid, valid_pred)\n",
    "valid_src = stats.spearmanr(y_valid, valid_pred)[0]\n",
    "    \n",
    "print(\"MSE: %.4f, MAE: %.4f, SRC: %.4f\"%(valid_mse, valid_mae, valid_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea8577ef-03de-4ec1-bfcc-1660d02cea37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.622696 , 12.716267 , 11.696694 , ...,  6.3496876, 10.743238 ,\n",
       "        2.0040984], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "822f7acd-a34e-48ad-a81a-8a3490a6e085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 8.622696 , 12.716267 , 11.696694 , ...,  6.3496876, 10.743238 ,\n",
       "         2.0040984], dtype=float32),\n",
       " array([ 8.622696 , 12.716267 , 11.696694 , ...,  6.3496876, 10.743238 ,\n",
       "         2.0040984], dtype=float32)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = []\n",
    "ss.append(test_preds.reshape(-1))\n",
    "ss.append(test_preds.reshape(-1))\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31bbf7c7-9307-4450-a96c-67c23d0ea9f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180581, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb81427-2f2e-401c-8243-fe572fa1d541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
